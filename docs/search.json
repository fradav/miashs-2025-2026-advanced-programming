[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced programming and parallel computing",
    "section": "",
    "text": "Preface\nThis is the course and materials for the lecture on “Advanced programming and parallel computing” at the Paul Valery University of Montpellier, France.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Software Prerequisites",
    "section": "",
    "text": "1.1 Software required\nYou will need to install the following software on your computer: Visual Studio Code (VSCode), a free and open-source code editor. You’ll have to install the following extensions:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#software-required",
    "href": "intro.html#software-required",
    "title": "1  Software Prerequisites",
    "section": "",
    "text": "Python extension to have everything you need to work with Python.\n Live Share to enable collaborative editing.\n Continue.dev to have the AI Code Assistant we will use as example in this course.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#ai-assistant-getting-api-key.",
    "href": "intro.html#ai-assistant-getting-api-key.",
    "title": "1  Software Prerequisites",
    "section": "1.2 AI Assistant, getting API Key.",
    "text": "1.2 AI Assistant, getting API Key.\nFor the practical work, you’ll need to get API Keys from Mistral.ai’s “La Plateforme” (it’s completely free). You will need a valid cell phone number for the registration to work. Contact me if this is a problem.\n\n\nMistral Login\n\n\n\nThe first time, you’ll have to create an account. Then, you’ll be able to get your API Key.\nOnce there, Click on “API Keys” tab.\n\n\nMistral API Keys Tab\n\n\n\nClick on “Choose a plan”.\n\n\nNo plan\n\n\n\nChoose “Experiment” plan.\n\n\nExperiment plan\n\n\n\nAccept the conditions.\n\n\nAccept\n\n\n\nGive a phone number for the final check.\n\n\nPhone number check\n\n\n\nConfirm the code.\n\n\nCode received\n\n\n\nIf successful, return on the “AI Keys” Tab and choose “Create a new key”\n\n\nCreate a new key\n\n\n\nChoose a name and an expiration date for your key (could be never if not set).\n\n\nKey name and expiration date\n\n\n\nCopy the key and save it somewhere.\n\n\nKey to copy\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou may also create a specific key for Codestral model, which could be used for auto-completion role. Auto-completion role needs specifically tailored models for this task, and the models you can access with the “generic” mistral key aren’t.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#vscode-setup",
    "href": "intro.html#vscode-setup",
    "title": "1  Software Prerequisites",
    "section": "1.3 VSCode setup",
    "text": "1.3 VSCode setup\nWhen you installed your continue vscode extension, it created a .continue folder in your home directory, which is ~/.continue on Linux and Mac, and %USERPROFILE%\\.continue on Windows. Create a .env file there and put your mistral key in it, like this:\nMISTRAL_API_KEY=your_key_here\nOptionally if you got a codestral key put it too:\nCODESTRAL_API_KEY=your_codestral_key_here\nOpen/Create config.yaml file in the same folder and put this in it:\nname: miashs/mistral\nversion: 1.0.0\nschema: v1\nmodels:\n## Uncomment this block if you want to use Codestral for auto-completion, needs a specific key\n#   - name: Codestral\n#     provider: mistral\n#     model: codestral-2508\n#     apiBase: https://codestral.mistral.ai/v1\n#     apiKey: ${{ secrets.CODESTRAL_API_KEY }}\n#     roles:\n#       - autocomplete\n#     defaultCompletionOptions:\n#       contextLength: 256000\n  - name: Devstral\n    provider: mistral\n    model: devstral-medium-latest\n    apiKey: ${{ secrets.MISTRAL_API_KEY }}\n    roles:\n      - chat\n      - edit\n      - apply\n    defaultCompletionOptions:\n      contextLength: 131072\n    capabilities:\n      - tool_use\n# You may choose mistral over devstral if you need image input\n  - name: Mistral\n    provider: mistral\n    model: mistral-medium-latest\n    apiKey: ${{ secrets.MISTRAL_API_KEY }}\n    roles:\n      - chat\n      - edit\n      - apply\n    defaultCompletionOptions:\n      contextLength: 131072\n    capabilities:\n      - tool_use\n      - image_input\n  - name: Codestral Embed\n    provider: mistral\n    model: codestral-embed\n    apiKey: ${{ secrets.MISTRAL_API_KEY }}\n    apiBase: https://api.mistral.ai/v1\n    roles:\n      - embed\ncontext:\n  - provider: code\n  - provider: docs\n    params:\n      maxdepth: 5\n  - provider: diff\n  - provider: terminal\n  - provider: problems\n  - provider: codebase\n    params:\n      nRetrieve: 60\n      nFinal: 25\n  - provider: folder\n    params:\n      nRetrieve: 60\n      nFinal: 25\n  - provider: open\n  - provider: web\n  - provider: tree\n  - provider: clipboard\n  - provider: debugger\n  - provider: repo-map\n  - provider: os\n  - provider: search\n  - provider: url\nCongratulations, you are now set for use of continue AI Code Assistant. You can open the chat panel either by clicking on the Continue in the right bottom status of vscode window or with Cmd + L (Mac) or Ctrl + L (Windows/Linux).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#collaborative-editing",
    "href": "intro.html#collaborative-editing",
    "title": "1  Software Prerequisites",
    "section": "1.4 Collaborative editing",
    "text": "1.4 Collaborative editing\n\nIn the discord channel, I’ll provide you a link to join a collaborative editing session. Don’t click on it, just copy it: \nThen open a new “blank” window in VSCode, which will be exclusively for collaborative session. \nThen, click on the “Live Share” button in the bottom left corner of the window \nClick on the “Join” button \nEither choose anonymous or sign in with your github/microsoft account \n\n\n\n\n\n\n\nAnonymous Guest Name\n\n\n\nIf you choose to sign in, you’ll have to authorize VSCode to access your github/microsoft account. If you choose anonymous, you’ll have to choose a username. Please choose a username that is easily identifiable as yours.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#installing-uv",
    "href": "intro.html#installing-uv",
    "title": "1  Software Prerequisites",
    "section": "2.1 Installing uv",
    "text": "2.1 Installing uv\nSee uv installation for your platform.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#creating-a-new-environment",
    "href": "intro.html#creating-a-new-environment",
    "title": "1  Software Prerequisites",
    "section": "2.2 Creating a new environment",
    "text": "2.2 Creating a new environment\nTo create a new environment, use the following command:\nuv init\nThis will create a new environment within the current directory.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#adding-packages",
    "href": "intro.html#adding-packages",
    "title": "1  Software Prerequisites",
    "section": "2.3 Adding packages",
    "text": "2.3 Adding packages\nTo add packages to your environment, use the following command:\nuv add package_name\nReplace package_name with the name of the package you want to install. This will install the package in your current environment.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#running-a-script-online-with-just-a-dependency",
    "href": "intro.html#running-a-script-online-with-just-a-dependency",
    "title": "1  Software Prerequisites",
    "section": "2.4 running a script online with just a dependency",
    "text": "2.4 running a script online with just a dependency\nuv run --with dependency script.py\nThis will execute the script.py file using just the specified dependency as the environment.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#running-script-with-a-directory",
    "href": "intro.html#running-script-with-a-directory",
    "title": "1  Software Prerequisites",
    "section": "2.5 running script with a directory",
    "text": "2.5 running script with a directory\nuv run --directory dir_env script.py\nThis will execute the script.py file using the specified directory as the working directory.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#macos",
    "href": "intro.html#macos",
    "title": "1  Software Prerequisites",
    "section": "3.1 MacOS",
    "text": "3.1 MacOS\nAdd this to you vscode settings.json\n    \"jupyter.runStartupCommands\": [\n        \"%colors nocolor\",\n        \"from multiprocessing import spawn\",\n        \"from multiprocessing.spawn import get_preparation_data as __get_preparation_data\",\n        \"def __patched_get_preparation_data(name):\",\n        \"    import sys\",\n        \"    main_mod = sys.modules['__main__']\",\n        \"    main_path = getattr(main_mod, '__file__', None)\",\n        \"    setattr(main_mod, '__file__', None)\",\n        \"    data = __get_preparation_data(name)\",\n        \"    setattr(main_mod, '__file__', main_path)\",\n        \"    return data\",\n        \"spawn.get_preparation_data = __patched_get_preparation_data\",\n        \"del spawn\",\n        \"import sys\",\n        \"from multiprocessing.reduction import ForkingPickler\",\n        \"from types import FunctionType\",\n        \"import cloudpickle\",\n        \"\",\n        \"def reducer_override(obj):\",\n        \"    if type(obj) is FunctionType:\",\n        \"        return (cloudpickle.loads, (cloudpickle.dumps(obj),))\",\n        \"    else:\",\n        \"        return NotImplemented\",\n        \"\",\n        \"ForkingPickler.reducer_override = staticmethod(reducer_override)\"\n    ],",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Prerequisites</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html",
    "href": "Courses/01_Code-Assistant.html",
    "title": "2  Code Assistants",
    "section": "",
    "text": "3 History of code editors/assistants\nHistory of code editor features, with a focus on the last three years (2022–2025) and the transformative impact of Large Language Models (LLMs):",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#early-days-text-editors",
    "href": "Courses/01_Code-Assistant.html#early-days-text-editors",
    "title": "2  Code Assistants",
    "section": "3.1 Early Days: Text Editors",
    "text": "3.1 Early Days: Text Editors\n\n1960s–1970s: vi (1976), Emacs (1976)\n\nBasic text manipulation, macros, and syntax highlighting.\n\n\n\n\n\nVi\n\n\n\n\n\n\n\nEmacs",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#the-rise-of-ides",
    "href": "Courses/01_Code-Assistant.html#the-rise-of-ides",
    "title": "2  Code Assistants",
    "section": "3.2 The Rise of IDEs",
    "text": "3.2 The Rise of IDEs\n\n\n\n1980s–1990s: Turbo Pascal (1983), Visual Basic (1991)\n\nIntegrated debugging\nproject management\nbasic autocompletion.\n\n\n\n\n\nTurbo Pascal\n\n\n\n\n\nVisual Basic",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#modern-era-powerful-extensible-ides",
    "href": "Courses/01_Code-Assistant.html#modern-era-powerful-extensible-ides",
    "title": "2  Code Assistants",
    "section": "3.3 Modern Era: Powerful, Extensible IDEs",
    "text": "3.3 Modern Era: Powerful, Extensible IDEs\n\n\n\n2000s: Visual Studio, Eclipse, IntelliJ IDEA\n\\Rightarrow Advanced autocompletion, refactoring, static analysis, and plugin ecosystems.\n\n\n\n\nVisual Studio\n\n\n\n\n\n\n\n\n\n2015: VSCode (based on Electron/Node.js)\n\\Rightarrow Lightweight, open-source, and extensible via marketplace.\n\n\n\n\nVSCode",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#the-language-server-protocol-lsp",
    "href": "Courses/01_Code-Assistant.html#the-language-server-protocol-lsp",
    "title": "2  Code Assistants",
    "section": "3.4 The Language Server Protocol (LSP)",
    "text": "3.4 The Language Server Protocol (LSP)\n2016: Microsoft introduces the Language Server Protocol (LSP)\n\nStandardizes communication between editors/IDEs and language-specific servers.\nEnables features like autocompletion, go-to-definition, linting, and refactoring across many languages.\nDecouples editor development from language tooling.\n\n\n\nLSP Architecture",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#impact-of-lsp-on-developer-experience",
    "href": "Courses/01_Code-Assistant.html#impact-of-lsp-on-developer-experience",
    "title": "2  Code Assistants",
    "section": "3.5 Impact of LSP on Developer Experience",
    "text": "3.5 Impact of LSP on Developer Experience\n\nUnified experience: VSCode, Vim, Emacs, Sublime Text, and more support LSP.\nRapid adoption: Hundreds of languages now have LSP servers.\nConsistent, high-quality tooling regardless of editor.\nPaved the way for advanced features and easier integration of AI assistants.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#the-breakthrough-year",
    "href": "Courses/01_Code-Assistant.html#the-breakthrough-year",
    "title": "2  Code Assistants",
    "section": "4.1 2022: The Breakthrough Year",
    "text": "4.1 2022: The Breakthrough Year\n\n\n\nGitHub Copilot (June 2022)\n\nFirst mainstream LLM-powered code assistant (OpenAI Codex).\nKey features:\n\nCode generation from comments or snippets.\nMulti-language support (Python, JavaScript, Java, etc.).\n\n\n\n\n\n\nGitHub Copilot Autocomplete Demo",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#ai-becomes-ubiquitous",
    "href": "Courses/01_Code-Assistant.html#ai-becomes-ubiquitous",
    "title": "2  Code Assistants",
    "section": "4.2 2023: AI Becomes Ubiquitous",
    "text": "4.2 2023: AI Becomes Ubiquitous\n\nGitHub Copilot X (March 2023)\n\nIntegrated ChatGPT-4 for explanations, test generation, and PR reviews.\nNew features:\n\nNatural language explanations of complex code.\nAutomatic test generation.\nAI-assisted debugging.\n\n\n\n\nJetBrains AI Assistant\n\nNative integration in IntelliJ, PyCharm, etc.\n\nCollaboration tools:\n\nCopilot for Pull Requests, Amazon Q.\n\nVSCode forks:\n\nCursor, Windsurf (by Codeium).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#the-rise-of-autonomous-agents",
    "href": "Courses/01_Code-Assistant.html#the-rise-of-autonomous-agents",
    "title": "2  Code Assistants",
    "section": "4.3 2024: The Rise of Autonomous Agents",
    "text": "4.3 2024: The Rise of Autonomous Agents\n\n\n\nClaude Code (Anthropic, 2024)\n\nAgentic capabilities: Executes tasks (file creation, commits, tests, PRs).\nTerminal integration: Works directly in the terminal.\nHolistic understanding: Cross-file refactors and dependency analysis.\nSecurity: Restrictions for risky commands:refs[1-6,9].\n\n\n\n\n\nClaude Code Demo\n\n\n\n\n\n\n\nGitHub Copilot Enterprise\n\nCustomization for company codebases.\nExtended context (internal docs, Jira tickets).\n\nAdvanced features:\n\nMulti-step agents.\nReal-time visualization (e.g., Artifacts in Claude).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#maturation-and-specialization",
    "href": "Courses/01_Code-Assistant.html#maturation-and-specialization",
    "title": "2  Code Assistants",
    "section": "4.4 2025: Maturation and Specialization",
    "text": "4.4 2025: Maturation and Specialization\n\nDeep integration:\n\nVSCode: Native support for AI agents (Cline, Augment).\nJetBrains: Claude 3.5 and Mellum models:refs[3-4].\nCursor/Windsurf: Popular AI-driven alternatives.\n\nNew features:\n\nMulti-modal editing (code from diagrams, screenshots).\nSpecialized agents for DevOps and security.\nExtreme customization and collaboration.\n\nChallenges:\n\nTechnical debt from “black box” AI-generated code.\nIP concerns and performance issues.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#summary-evolution-of-code-editor-features-20222025",
    "href": "Courses/01_Code-Assistant.html#summary-evolution-of-code-editor-features-20222025",
    "title": "2  Code Assistants",
    "section": "4.5 Summary: Evolution of Code Editor Features (2022–2025)",
    "text": "4.5 Summary: Evolution of Code Editor Features (2022–2025)\n\n\n\n\n\n\n\n\n2022\nLLM-powered code generation\nGitHub Copilot, TabNine\n\n\n\n\n2023\nExplanations, tests, PR reviews\nCopilot X, Amazon Q, JetBrains AI\n\n\n2024\nAutonomous agents, task execution\nClaude Code, Copilot Enterprise\n\n\n2025\nSpecialization, multi-modality\nCursor, Windsurf, Qodo, Continue",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#future-trends",
    "href": "Courses/01_Code-Assistant.html#future-trends",
    "title": "2  Code Assistants",
    "section": "4.6 Future Trends",
    "text": "4.6 Future Trends\n\n\n\nTrend\nBenefits\nChallenges\n\n\n\n\nAI as Co-Pilot\nFaster development, skill augmentation\nOver-reliance, quality control\n\n\nSelf-Healing Editors\nFewer bugs, improved code quality\nFalse positives, transparency\n\n\nLow-Code/No-Code\nAccessibility, rapid prototyping\nLimited customization, maintenance\n\n\nRegulation and Ethics\nSafer, more transparent tools\nCompliance complexity, global fragmentation",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#a-paradigm-shift",
    "href": "Courses/01_Code-Assistant.html#a-paradigm-shift",
    "title": "2  Code Assistants",
    "section": "4.7 A Paradigm Shift",
    "text": "4.7 A Paradigm Shift\n\nFrom manual editing → contextual assistance → AI co-creation.\nChallenge: Mastering tools without compromising quality or security.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#ai-domains",
    "href": "Courses/01_Code-Assistant.html#ai-domains",
    "title": "2  Code Assistants",
    "section": "5.1 AI domains",
    "text": "5.1 AI domains",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#train-a-neural-network",
    "href": "Courses/01_Code-Assistant.html#train-a-neural-network",
    "title": "2  Code Assistants",
    "section": "5.2 Train a neural network",
    "text": "5.2 Train a neural network\n\n\nTanguy Lefort, 2023",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#training-a-supervised-machine-learning-model",
    "href": "Courses/01_Code-Assistant.html#training-a-supervised-machine-learning-model",
    "title": "2  Code Assistants",
    "section": "5.3 Training a supervised Machine learning model",
    "text": "5.3 Training a supervised Machine learning model\n\n\nClass of prediction functions f_\\theta: linear, quadratic, trees\nLoss \\mathcal{L}: L^2 norm, CrossEntropy, purity score\nOptimizer: SGD, Adam, …\n\nlearning rate \\eta: \\theta_{k+1} \\gets \\theta_k - \\eta \\nabla_\\theta \\mathcal{L}\nother hyperparameters\n\nDataset:\n\ntraining: \\{(x_i, y_i)\\}_{i} to compute loss between prediction f_{\\theta}(x_i) and label y_i to update \\theta\ntest: only compute performance scores (no more updates !)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#foreword-beware-the-alchemy",
    "href": "Courses/01_Code-Assistant.html#foreword-beware-the-alchemy",
    "title": "2  Code Assistants",
    "section": "6.1 Foreword, beware the Alchemy",
    "text": "6.1 Foreword, beware the Alchemy\n\n\n\n\n\n\n\nMore or less theoretical guarantees\n\nfield of research\ntype of network\nfrom theory to applications: a gap\n\nMyriad of ad-hoc choices, engeenering tricks and empirical observations\nCurrent choices are critical for success: what are their pros and cons?\nTry \\rightarrow Fail \\rightarrow Try again is the current pipeline",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#tensor-algebra",
    "href": "Courses/01_Code-Assistant.html#tensor-algebra",
    "title": "2  Code Assistants",
    "section": "7.1 Tensor algebra",
    "text": "7.1 Tensor algebra\n\nLinear algebra operations on tensors\nMultiLayerPerceptron = sequence of linear operations and non-linear activations\n\n\\Rightarrow input can be anything: images, videos, text, sound, …",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#automatic-differentiation",
    "href": "Courses/01_Code-Assistant.html#automatic-differentiation",
    "title": "2  Code Assistants",
    "section": "7.2 Automatic differentiation",
    "text": "7.2 Automatic differentiation\n\n\n\nchain rule to compute gradient with respect to \\theta\nkey tool: backpropagation\n\ndon’t need to store the computation graph entirely\ngradient is fast to compute (a single pass)\nbut memory intensive\n\n\n\nf(x)=\\nabla\\frac{x_{1}x_{2} sin(x_3) +e^{x_{1}x_{2}}}{x_3}\n\n\n\\begin{darray}{rcl}\nx_4 & = & x_{1}x_{2}, \\\\\nx_5 & = & sin(x_3), \\\\\nx_6 & = & e^{x_4}, \\\\\nx_7 & = & x_{4}x_{5}, \\\\\nx_8 & = & x_{6}+x_7, \\\\\nx_9 & = & x_{8}/x_3.\n\\end{darray}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#gradient-descent",
    "href": "Courses/01_Code-Assistant.html#gradient-descent",
    "title": "2  Code Assistants",
    "section": "7.3 Gradient descent",
    "text": "7.3 Gradient descent\nExample with a non-convex function\nf(x_1, x_2) = (x_1^2 + x_2 - 11)^2 + (x_1 + x_2^2 - 7)^2\n\n\n\n\nCode\nPlotly = require(\"plotly.js@2.35.2/dist/plotly.min.js\");\n\nminX = -5;\nmaxX = 5;\n\nf = ([x1, x2]) =&gt; (x1**2 + x2 - 11)**2 + (x1 + x2**2 - 7)**2;\n\n{\n  const linspace = d3.scaleLinear().domain([0, 49]).range([minX, maxX]);\n  const X1 = Array.from({length: 50}, (_, i) =&gt; linspace(i));\n  const X2 = Array.from({length: 50}, (_, i) =&gt; linspace(i));\n\n  // Define your function f here\n  const f = ([x1, x2]) =&gt; (x1**2 + x2 - 11)**2 + (x1 + x2**2 - 7)**2;\n\n  const Z = X1.map((x1,i) =&gt; X2.map((x2,j) =&gt; f([x1,x2])));\n\n  const data = [{\n    x: X1.flat(),\n    y: X2.flat(),\n    z: Z,\n    type: 'surface'\n  }];\n\n  const layout = {\n    // title: '',\n    autosize: false,\n    width: 400,\n    height: 400,\n    paper_bgcolor: \"rgba(0,0,0,0)\",\n    plot_bgcolor: \"rgba(0,0,0,0)\",\n    template: 'plotly_dark',\n    margin: {\n      l: 0,\n      r: 0,\n      b: 0,\n      t: 0\n    }\n  };\n\n  const div = document.createElement('div');\n  Plotly.newPlot(div, data, layout,{displayModeBar: false});\n  return div;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nfunction grad_descent(x1,x2,step,max_iter) {\n  let grad = f_grad(x1, x2);\n  let iterations = [[x1, x2]];\n  function f_grad(x1, x2) {\n    let df_x1 = 2 * (-7 + x1 + x2**2 + 2 * x1 * (-11 + x1**2 + x2));\n    let df_x2 = 2 * (-11 + x1**2 + x2 + 2 * x2 * (-7 + x1 + x2**2));\n    return [df_x1, df_x2];\n  }\n  var count = 0;\n  while (count &lt; max_iter) {\n    x1 -= step * grad[0];\n    x2 -= step * grad[1];\n    grad = f_grad(x1, x2);\n    if (isFinite(x1) && isFinite(x2) &&\n      (minX &lt; x1) && (x1 &lt; maxX) &&\n      (minX &lt; x2) && (x2 &lt; maxX))\n        iterations.push([x1, x2]);\n    else iterations.push(iterations[count])\n    count += 1\n  }\n  return iterations;\n}\n\nviewof descent_params = Inputs.form({\n  x1: Inputs.range([minX, maxX], {step: 0.1, value: 0, label: 'x1 initial'}),\n  x2: Inputs.range([minX, maxX], {step: 0.1, value: 0, label: 'x2 initial'}),\n  step: Inputs.range([0.001, 0.04], {step: 0.001, value: 0.01, label: 'Step size'})\n})\n\nprefix = Inputs.text().classList[0];\n\n{\n  d3.selectAll(`.${prefix}`).style(\"font-size\", \"16px\");\n  var iterations = grad_descent(descent_params.x1,descent_params.x2,descent_params.step,20)\n  return Plot.plot({\n    aspectRatio: 1,\n    x: {tickSpacing: 50, label: \"x1 →\"},\n    y: {tickSpacing: 50, label: \"x2 →\"},\n    width: 600,\n    style: {\n      backgroundColor: 'rgba(0,0,0,0)'\n    },\n    marks: [\n      Plot.contour({\n        fill: (x1, x2) =&gt; Math.sqrt((x1**2 + x2 - 11)**2 + (x1 + x2**2 - 7)**2),\n        x1: minX,\n        y1: minX,\n        x2: maxX,\n        y2: maxX,\n        showlegend: false,\n        colorscale: 'RdBu',\n        ncontours: 30\n      }),\n      Plot.line(iterations,{marker: true})\n    ]\n  })\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSensitivity to initial point and step size",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#stochastic-gradient-descent",
    "href": "Courses/01_Code-Assistant.html#stochastic-gradient-descent",
    "title": "2  Code Assistants",
    "section": "7.4 (Stochastic) Gradient descent",
    "text": "7.4 (Stochastic) Gradient descent\n\n\n\n\nnot use all the data at once to compute the gradient\n\nnot feasible in practice (memory wise)\n\nUse mini-batch of data (boostrap samples)\n\none more hyperparameter…\n\n\n\n\n\n\\theta_{k+1} \\leftarrow \\theta_k - \\frac{\\eta}{n}\\sum_{i\\in\\text{batch}}\\nabla_\\theta \\mathcal{L}(f_\\theta(x_i), y_i)\n\n\n\n\n\\Rightarrow No general guarantees of convergence in DL setting",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#optimizers",
    "href": "Courses/01_Code-Assistant.html#optimizers",
    "title": "2  Code Assistants",
    "section": "7.5 Optimizers",
    "text": "7.5 Optimizers\nSGD, Adam, RMSProp\n\nNon-convex optimization research on the subject is still very active, and there is no clear consensus on what is the best optimizer to use in a given situation.\nNo guarantee of global minimum, only local minimum\nNo guarantee of convergence, only convergence in probability",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#more-than-a-pinch-of-non-linearities",
    "href": "Courses/01_Code-Assistant.html#more-than-a-pinch-of-non-linearities",
    "title": "2  Code Assistants",
    "section": "7.6 (More than) a pinch of non-linearities",
    "text": "7.6 (More than) a pinch of non-linearities\n\n\n\n\nLinear Transformations + Non-linear activation functions\nradically enhance the expressive power of the model\nability to explore the space of functions in gradient descent.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#from-text-to-numbers",
    "href": "Courses/01_Code-Assistant.html#from-text-to-numbers",
    "title": "2  Code Assistants",
    "section": "8.1 From text to numbers",
    "text": "8.1 From text to numbers\n\nMain problem: we can’t multiply or do convolutions with words\nSecond problem: many words (for a single language)\nThird problem: how to capture semantics?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#embeddings-2",
    "href": "Courses/01_Code-Assistant.html#embeddings-2",
    "title": "2  Code Assistants",
    "section": "8.2 Embeddings",
    "text": "8.2 Embeddings\n\nDistance between words should not be character based\n\n\n\n\n\n\n\n\\Rightarrow\n\n\n\n\n\n\n\nTanguy Lefort, 2023",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#multi-scale-learning-from-text",
    "href": "Courses/01_Code-Assistant.html#multi-scale-learning-from-text",
    "title": "2  Code Assistants",
    "section": "8.3 Multi-scale learning from text",
    "text": "8.3 Multi-scale learning from text\n\nDL layers = capture different levels of dependencies in the data\nattention mechansim applies “multi-scale learning” to data sequences \\Rightarrow e.g. not only words in sentences, but sentences in paragraphs, paragraphs in documents and so on.\n\n\\Rightarrow transformers capture dependencies in the “whole”",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#multi-facets-learning-from-text",
    "href": "Courses/01_Code-Assistant.html#multi-facets-learning-from-text",
    "title": "2  Code Assistants",
    "section": "8.4 Multi-facets learning from text",
    "text": "8.4 Multi-facets learning from text\nMulti-head attention mechanism extends the attention mechanism to multifaceted dependencies of the same text components.\nIn the sentence “the cat sat on the rug, and after a few hours, it moved to the mat.” :\n\ncat/rug/mat\nrug/mat\ncat/he\nsat/moved to\n\n\\Rightarrow All those groups of words/tokens are multiple facets of the same text and its meaning.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#transformers",
    "href": "Courses/01_Code-Assistant.html#transformers",
    "title": "2  Code Assistants",
    "section": "8.5 Transformers",
    "text": "8.5 Transformers\n\n\nVaswani et al. (2017)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#heart-of-transformers-attention-mechanism",
    "href": "Courses/01_Code-Assistant.html#heart-of-transformers-attention-mechanism",
    "title": "2  Code Assistants",
    "section": "8.6 Heart of Transformers: Attention mechanism",
    "text": "8.6 Heart of Transformers: Attention mechanism\n\n\n\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\n\n\n\n\nThree matrices: Query, Key, Value, derived from the input sequence\nd_k: dimension of the key matrix, typically 64 or 128\nwe want to compute a weighted sum of the values V with weights given by the compatibility between the query and the keys\nsoftmax to get a probability distribution\nmulti-head attention: several attention mechanisms in parallel\n\n\n\n\n\n\n\n\nVaswani et al. (2017)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#head-view-of-attention",
    "href": "Courses/01_Code-Assistant.html#head-view-of-attention",
    "title": "2  Code Assistants",
    "section": "8.7 Head view of attention",
    "text": "8.7 Head view of attention\n\n\n\n\n\n\nFigure 8.1: The model view visualizes attention across all heads in a single Transformer layer.\n\n\n\n\n\n\n\n\n\n\n\nEach line shows the attention from one token (left) to another (right).\nLine weight reflects the attention value (ranges from 0 to 1),\nline color identifies the attention head",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#bert-bidirectional-encoder-representations-from-transformers",
    "href": "Courses/01_Code-Assistant.html#bert-bidirectional-encoder-representations-from-transformers",
    "title": "2  Code Assistants",
    "section": "8.8 BERT: Bidirectional Encoder Representations from Transformers",
    "text": "8.8 BERT: Bidirectional Encoder Representations from Transformers\n\nembeddings: represent words as vectors in high dimensions",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#gpt-generative-pre-trained-transformer",
    "href": "Courses/01_Code-Assistant.html#gpt-generative-pre-trained-transformer",
    "title": "2  Code Assistants",
    "section": "8.9 GPT : Generative Pre-trained Transformer",
    "text": "8.9 GPT : Generative Pre-trained Transformer\n\nautoregressive model\ngenerates text by predicting the next token\npre-trained on large corpora of text",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#bert-vs-gpt",
    "href": "Courses/01_Code-Assistant.html#bert-vs-gpt",
    "title": "2  Code Assistants",
    "section": "8.10 BERT vs GPT",
    "text": "8.10 BERT vs GPT\n\n\n\n\n\nBERT\n\n\n\n\n\n\n\nGPT",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#summary-of-llm-types",
    "href": "Courses/01_Code-Assistant.html#summary-of-llm-types",
    "title": "2  Code Assistants",
    "section": "8.11 Summary of LLM types",
    "text": "8.11 Summary of LLM types\n\n\n\nType\nArchitecture\nTraining Objective\nAttention\nUse Cases\n\n\n\n\nBERT (Encoder-Only)\nEncoder stack only\nMasked Language Modeling (MLM)\nBidirectional (sees left and right context)\nClassification, QA, NER, sentiment analysis\n\n\nGPT (Decoder-Only)\nDecoder stack only\nAutoregressive Language Modeling (next token prediction)\nUnidirectional (left-to-right, autoregressive)\nText generation, chatbots, open-ended tasks\n\n\nSeq2Seq (Encoder-Decoder)\nEncoder + Decoder stacks\nSequence-to-sequence (e.g., translation, summarization)\nEncoder: Bidirectional; Decoder: Unidirectional (autoregressive)\nTranslation, summarization, speech recognition, data-to-text\n\n\n\n\n\n\nType\nStrengths\nWeaknesses\nExample Models\nTraining Data\nInference Speed\n\n\n\n\nBERT (Encoder-Only)\nDeep understanding of input; strong for discriminative tasks\nNot designed for generation\nBERT, RoBERTa, DistilBERT\nLarge corpus (masked tokens)\nFast (parallelizable)\n\n\nGPT (Decoder-Only)\nCoherent, fluent generation; open-ended creativity\nNo bidirectional context; limited to left-to-right generation\nGPT-3, GPT-4, Llama\nLarge corpus (autoregressive)\nSlower (autoregressive)\n\n\nSeq2Seq (Encoder-Decoder)\nExplicit input-output mapping; handles sequence transformation\nMore complex; requires aligned input-output pairs\nT5, BART, Transformer (original), Whisper\nParallel corpora (input-output pairs)\nModerate (depends on sequence length)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#generative-llms-base-vs-instruct",
    "href": "Courses/01_Code-Assistant.html#generative-llms-base-vs-instruct",
    "title": "2  Code Assistants",
    "section": "8.12 Generative LLMs, Base vs Instruct",
    "text": "8.12 Generative LLMs, Base vs Instruct\n\nBase models are just predicting the next word (pre-training phase, no task-specific fine-tuning)\nInstruct models are fine-tuned on specific tasks and follow user instructions more effectively.\n\n\n\n\n\n\n\nImportant\n\n\n\nNever use the base model for specific tasks without fine-tuning.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#generative-llms-reasoning-vs-non-reasoning",
    "href": "Courses/01_Code-Assistant.html#generative-llms-reasoning-vs-non-reasoning",
    "title": "2  Code Assistants",
    "section": "8.13 Generative LLMs, Reasoning vs non-Reasoning",
    "text": "8.13 Generative LLMs, Reasoning vs non-Reasoning\n\n(Non-reasoning) models focus on generating coherent text without explicit reasoning capabilities\nReasoning models are designed to perform complex reasoning tasks and can handle multi-step problems, at the cost of increased computational requirements (and budget)\n\n\n\n\n\n\n\nTip\n\n\n\nReasoning addition to LLM have been a breakthrough in the field since end of 2024.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#the-importance-of-the-context-window",
    "href": "Courses/01_Code-Assistant.html#the-importance-of-the-context-window",
    "title": "2  Code Assistants",
    "section": "8.14 The importance of the context window",
    "text": "8.14 The importance of the context window\n\nThe context window is crucial for understanding and generating text.\nIt determines how much information the model can consider at once.\nLarger context windows allow for better understanding of complex queries and generation of more coherent responses\nTypical max context window are in a 16k tokens, latest open-weights local llms are 128/256/512k tokens, frontier llms are 1M+ tokens\nlong context window are computationally expensive and require more memory/gpu ressources.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#what-happens-when-the-context-window-is-exceeded",
    "href": "Courses/01_Code-Assistant.html#what-happens-when-the-context-window-is-exceeded",
    "title": "2  Code Assistants",
    "section": "8.15 What happens when the context window is exceeded?",
    "text": "8.15 What happens when the context window is exceeded?\n\nWhen the context window is exceeded, the model may lose track of important information, leading to less coherent responses.\nStrategies to handle this include:\n\nSummarizing previous context\nUsing external memory stores\nChunking input data\n\n\n\n\n\n\n\n\nCaution\n\n\n\nVery large context (when permitted by the model) isn’t always a good thing: there is chances that the model may become overwhelmed with information, leading to decreased performance AND quality.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#rag-retrieval-augmented-generation",
    "href": "Courses/01_Code-Assistant.html#rag-retrieval-augmented-generation",
    "title": "2  Code Assistants",
    "section": "8.16 RAG (Retrieval-Augmented Generation)",
    "text": "8.16 RAG (Retrieval-Augmented Generation)\n\n\n\n\nRAG combines retrieval-based and generation-based approaches.\nIt retrieves relevant documents from a knowledge base and uses them to inform the generation process.\nThis allows for more accurate and contextually relevant responses.\n\n\n\n\n\n\n\n\nTurtlecrown, Wikipedia",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#reactive-agents",
    "href": "Courses/01_Code-Assistant.html#reactive-agents",
    "title": "2  Code Assistants",
    "section": "9.1 Reactive agents",
    "text": "9.1 Reactive agents\n\n\n\nExamples:\n\nChatbots that respond to user queries with pre-defined answers.\nSimple automation scripts that trigger actions based on specific events, like web search.\nAgentic mode in Code Assistants\n\n\n\n\n\n\n\nWarning\n\n\n\n\\Rightarrow Control is done by the LLM itself with all risks : infinite loops, unsupervised and potentially dangerous actions etc.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#pipeline-agents",
    "href": "Courses/01_Code-Assistant.html#pipeline-agents",
    "title": "2  Code Assistants",
    "section": "9.2 Pipeline Agents",
    "text": "9.2 Pipeline Agents\n\n\n\nExamples:\n\nRAG queries\nSummarizing documents\nCommunicating with other agents\n\n\n\n\n\n\n\nNote\n\n\n\n\\Rightarrow Control is done by normal, program logic",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#reactive-agent-2025-mcp",
    "href": "Courses/01_Code-Assistant.html#reactive-agent-2025-mcp",
    "title": "2  Code Assistants",
    "section": "9.3 Reactive agent 2025 : MCP",
    "text": "9.3 Reactive agent 2025 : MCP\n\n\nUjjwal Khadka Source",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#ai-for-pipelinesgraphs",
    "href": "Courses/01_Code-Assistant.html#ai-for-pipelinesgraphs",
    "title": "2  Code Assistants",
    "section": "9.4 AI for pipelines/graphs",
    "text": "9.4 AI for pipelines/graphs\n\nLow level (API, almost request-level)\n\nOpenAI API (ubiquitous)\nHuggingface\n\nHigh level (Framework)\n\nLangchain (most popular)\nSemantic Kernel (Microsoft)\nHaystack\nand many many others…\n\n\nSimple request with OpenAI API :\nfrom openai import OpenAI\nclient = OpenAI()\n\nchat_response = client.chat.completions.create(\n    model= \"gpt-4o\",\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": \"What is the best French cheese?\",\n        },\n    ]\n)\nprint(chat_response.choices[0].message.content)\nSimple request with LangChain :\nfrom langchain.chat_models import init_chat_model\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\nmodel = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n\nmessages = [\n    SystemMessage(\"Translate the following from English into Italian\"),\n    HumanMessage(\"hi!\"),\n]\n\nmodel.invoke(messages)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#mcp-server-example",
    "href": "Courses/01_Code-Assistant.html#mcp-server-example",
    "title": "2  Code Assistants",
    "section": "9.5 MCP Server example",
    "text": "9.5 MCP Server example\nfrom datetime import datetime\nfrom mcp.server.fastmcp import FastMCP\n\n# Create an MCP server\nmcp = FastMCP(\"GetTime\")\n\n@mcp.tool()\ndef get_date() -&gt; str:\n    \"\"\"Returns the current date in YYYY-MM-DD format.\"\"\"\n    return datetime.today().strftime('%Y-%m-%d')",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#the-real-question",
    "href": "Courses/01_Code-Assistant.html#the-real-question",
    "title": "2  Code Assistants",
    "section": "10.1 The real question",
    "text": "10.1 The real question\n\n\n\n\nAre the benefits of using generative AI worth the cost of extra supervision and the additional engineering effort?\n\n\n\nThe general answer is:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#on-the-other-hand",
    "href": "Courses/01_Code-Assistant.html#on-the-other-hand",
    "title": "2  Code Assistants",
    "section": "10.2 On the other hand…",
    "text": "10.2 On the other hand…\n\n\n\n\n\n\n\n\n\nBeware of vibe-coding",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html",
    "href": "Courses/02_Parallel-intro.html",
    "title": "3  Introduction to parallel computing",
    "section": "",
    "text": "4 Parallel computing: the intuition",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#computing",
    "href": "Courses/02_Parallel-intro.html#computing",
    "title": "3  Introduction to parallel computing",
    "section": "4.1 Computing ?",
    "text": "4.1 Computing ?\n\na computation = a succession of tasks to complete\na task \\approx a single command/action or a group of commands/actions\n\n\n\n\nExample 1:\n\n\nExample 2:\n\n\n\n\n# task i:\n# sum of elements at index i\n# from two vectors\nfor i in range(10):\n    res[i] = a[i] + b[i]\n\n\n# task 1: matrix product\nC = A @ B\n# task 2: colwise sum over matrix C\nnp.sum(C,axis=0)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#why-parallel-computing",
    "href": "Courses/02_Parallel-intro.html#why-parallel-computing",
    "title": "3  Introduction to parallel computing",
    "section": "4.2 Why parallel computing?",
    "text": "4.2 Why parallel computing?\n\nObjective: accelerate computations &lt;=&gt; reduce computation time\nIdea: run multiple tasks in parallel instead of sequentially",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#context-level-1",
    "href": "Courses/02_Parallel-intro.html#context-level-1",
    "title": "3  Introduction to parallel computing",
    "section": "4.3 Context (level 1)",
    "text": "4.3 Context (level 1)\n\ndifferent tasks to complete\none or more workers to complete the tasks",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#sequential-computing",
    "href": "Courses/02_Parallel-intro.html#sequential-computing",
    "title": "3  Introduction to parallel computing",
    "section": "4.4 Sequential computing",
    "text": "4.4 Sequential computing\n\n\n\n\nn tasks to complete (n&gt;1)\n1 worker\n\nTotal time (exercise)\n\n\\sum_{i=1}^n t_i \\sim O(n)\\ with t_i time to complete task i}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#parallel-computing-the-most-simple-case",
    "href": "Courses/02_Parallel-intro.html#parallel-computing-the-most-simple-case",
    "title": "3  Introduction to parallel computing",
    "section": "4.5 Parallel computing (the most simple case)",
    "text": "4.5 Parallel computing (the most simple case)\n\n\n\n\n\n\nn tasks to complete (n&gt;1)\np workers (p&gt;=n)\n\n\n\n\nTotal time (exercise)\n\n\\underset{i=1,\\dots,n}{\\text{max}}\\{t_i\\}\\sim O(1)\\ with t_i time to complete task i\n\n\n\nPotential bottleneck? (exercise)\n\nnot enough workers to complete all tasks",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#task-scheduling",
    "href": "Courses/02_Parallel-intro.html#task-scheduling",
    "title": "3  Introduction to parallel computing",
    "section": "4.6 Task scheduling",
    "text": "4.6 Task scheduling\n\n\n\n\nn tasks to complete (n&gt;1)\np workers (p&lt;n)\n\nNeed: assign multiple tasks to each worker (and manage this assignment)\n\n\n\n\n\n\n\n\n\n\nTotal time (exercise)\n\n\\underset{k=1,\\dots,p}{\\text{max}}\\{T_k\\}\\sim O(n/p)\\ with T_k = \\sum_{i\\in I_k} t_i, total time to complete all tasks assigned to worker k (where I_k is the set of indexes of tasks assigned to worker k)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#illustration-parallel-computing-simple-case",
    "href": "Courses/02_Parallel-intro.html#illustration-parallel-computing-simple-case",
    "title": "3  Introduction to parallel computing",
    "section": "4.7 Illustration: parallel computing (simple case)",
    "text": "4.7 Illustration: parallel computing (simple case)\n\n\n\na task = “wait 1 \\mus”\nObjective: run 100 tasks\nNumber of workers: 1, 2, 4, 6, 8\n\nWhy is the time gain not linear?\n\n\n\n10 repetitions in each configurations",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#context-level-2",
    "href": "Courses/02_Parallel-intro.html#context-level-2",
    "title": "3  Introduction to parallel computing",
    "section": "4.8 Context (level 2)",
    "text": "4.8 Context (level 2)\n\ndifferent tasks to complete\nmultiple workers to complete the tasks\none or more working resources1\n\nPotential bottleneck? (exercise)\n\nnot enough resources for all workers",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#resource-management",
    "href": "Courses/02_Parallel-intro.html#resource-management",
    "title": "3  Introduction to parallel computing",
    "section": "4.9 Resource management",
    "text": "4.9 Resource management\n\n\n\n\nn tasks to complete (n&gt;1)\np workers (p&lt;n)\nq working resources (q&lt;p)\n\nNeed:\n\nassign workers to each resource (and manage this assignment)\n\nTotal time = ? (exercise)\nPotential issues? (exercise)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#resource-management-1",
    "href": "Courses/02_Parallel-intro.html#resource-management-1",
    "title": "3  Introduction to parallel computing",
    "section": "4.10 Resource management",
    "text": "4.10 Resource management\nTotal time = \\text{max}_{\\ell=1,\\dots,q}\\{\\tau_\\ell\\}\\sim O(n/q)\nwith \\tau_\\ell = \\sum_{i\\in J_\\ell} t_i = total time to complete all tasks done on resource \\ell (where J_\\ell is the set of indexes of tasks assigned done on resource \\ell)\nPotential issues? multiple workers want to use the same working resources\n\nthey have to wait for their turn (workers are not working all the time)\nrisk to jam2 resource access (organizing resource access takes time)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#illustration-overhead-for-resource-access",
    "href": "Courses/02_Parallel-intro.html#illustration-overhead-for-resource-access",
    "title": "3  Introduction to parallel computing",
    "section": "4.11 Illustration: overhead for resource access",
    "text": "4.11 Illustration: overhead for resource access\n\n\n\na task = “wait 1 \\mus”\nObjective: run 100 tasks\n8 computing units\nNumber of workers: 1, 2, 4, 8, 16, 32\n\n\n\n\n10 repetitions in each configurations",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#context-level-3-realistic",
    "href": "Courses/02_Parallel-intro.html#context-level-3-realistic",
    "title": "3  Introduction to parallel computing",
    "section": "4.12 Context (level 3: realistic)",
    "text": "4.12 Context (level 3: realistic)\n\ndifferent tasks to complete\nmultiple workers to complete the tasks\none or more working resources\n\nInput/Output (I/O)\n\nInput: each task requires some materials (data) to be completed, these materials are stored in a storage area (memory)\nOutput: each task returns a result that need to be put in the storage area (memory)\n\nExamples: vector/matrix/array operations, process the content of multiple files",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#inputoutput-management",
    "href": "Courses/02_Parallel-intro.html#inputoutput-management",
    "title": "3  Introduction to parallel computing",
    "section": "4.13 Input/Output management",
    "text": "4.13 Input/Output management\n\nn tasks to complete (n&gt;1)\np workers (p&lt;n)\nq working resources (q&lt;p)\ntasks need input (data) and produce output (results)\n\nNeed:\n\nload input (data) from storage when needed by a worker to complete a task\nwrite output (result) to storage when a task is completed\n\nTotal time = ? (exercise)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#parallel-computing-realistic-model",
    "href": "Courses/02_Parallel-intro.html#parallel-computing-realistic-model",
    "title": "3  Introduction to parallel computing",
    "section": "4.14 Parallel computing: realistic model",
    "text": "4.14 Parallel computing: realistic model",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#computing-time-and-potential-bottleneck",
    "href": "Courses/02_Parallel-intro.html#computing-time-and-potential-bottleneck",
    "title": "3  Introduction to parallel computing",
    "section": "4.15 Computing time and potential bottleneck",
    "text": "4.15 Computing time and potential bottleneck\nTotal time = \\text{max}_{\\ell=1,\\dots,q}\\{\\tau_\\ell\\}\nwith \\tau_\\ell = \\sum_{i\\in J_\\ell} t_{i,\\text{in}} + t_i + t_{i,\\text{out}} = total time to complete all tasks done on resource \\ell (where J_\\ell is the set of indexes of tasks done on resource \\ell)\nPotential bottlenecks:\n\ninput (data) are not ready/available when a worker need them to complete a task (the worker have to wait)\noutput (results) cannot be written when a worker complete a task (the worker have to wait)\n\nOverhead on memory access\n\nconcurrent access to a memory space when reading input and/or when writing output\nconcurrent data transfer from or to memory (the “pipe” are jammed)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#illustration-1-overhead-for-io-access",
    "href": "Courses/02_Parallel-intro.html#illustration-1-overhead-for-io-access",
    "title": "3  Introduction to parallel computing",
    "section": "4.16 Illustration 1: overhead for I/O access",
    "text": "4.16 Illustration 1: overhead for I/O access\n\n\n\na task\n\nsimulate a vector of 10 values\ncompute the mean\n\nObjective: run 10000 tasks\nResources: 8 computing units\nNumber of workers: 1, 2, 4, 6, 8\n\n\n\n\n10 repetitions in each configurations",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#illustration-2-overhead-for-io-access",
    "href": "Courses/02_Parallel-intro.html#illustration-2-overhead-for-io-access",
    "title": "3  Introduction to parallel computing",
    "section": "4.17 Illustration 2: overhead for I/O access",
    "text": "4.17 Illustration 2: overhead for I/O access\n\n\n\na task = “compute the sum of a given row in a matrix”\nObjective: compute all row-wise sums for a 10000 \\times 1000 matrix (i.e. 10000 tasks)\nResources: 8 computing units\nNumber of workers: 1, 2, 4, 6, 8\n\n\n\n\n20 repetitions in each configurations",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#the-vocabulary-of-parallel-computing",
    "href": "Courses/02_Parallel-intro.html#the-vocabulary-of-parallel-computing",
    "title": "3  Introduction to parallel computing",
    "section": "4.18 The vocabulary of parallel computing",
    "text": "4.18 The vocabulary of parallel computing\n\ntasks = a command or a group of commands\nworker = a program or a sub-program (like a thread or a sub-process) → Software\nworking resources = processing units → Hardware\ninput = data\noutput = result\nstorage = memory\n\nAttention: “worker” may sometimes refer to a working resource in the literature",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#task-synchronization",
    "href": "Courses/02_Parallel-intro.html#task-synchronization",
    "title": "3  Introduction to parallel computing",
    "section": "4.19 Task synchronization",
    "text": "4.19 Task synchronization\n\nSometimes tasks cannot be done in parallel\n\nSpecific case: output of task i_1 is input of task i_2\nNeed: wait for task i_1 before task i_2 starts\n\n\n\n\nExample 1:\n# task 1: matrix product\nC = A @ B\n# task 2: colwise sum over matrix C\nnp.sum(C,axis=0)\n\nExample 2:\n\ntask 1: train a predictive model\ntask 2: use the trained model to predict new labels",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#trend-over-50years",
    "href": "Courses/02_Parallel-intro.html#trend-over-50years",
    "title": "3  Introduction to parallel computing",
    "section": "5.1 Trend over ~50years",
    "text": "5.1 Trend over ~50years\n\n\nMoore’s Law (doubling the transistor counts every two years) is live\nSingle thread performance hit a wall in 2000s\nAlong with typical power usage and frequency\nNumber of logical cores is doubling every ~3 years since mid-2000\n\n\n\n\nOriginal data up to the year 2010 collected and plotted by M. Horowitz, F. Labonte, O. Shacham, K. Olukotun, L. Hammond, and C. Batten New plot and data collected for 2010-2021 by K. Rupp\n\n\n\n\nGithub repo for data",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#computing-units",
    "href": "Courses/02_Parallel-intro.html#computing-units",
    "title": "3  Introduction to parallel computing",
    "section": "5.2 Computing units",
    "text": "5.2 Computing units\n\n\nCPU :\n\n4/8/16+ execution cores (depending on context, laptop, desktop, server)\nHyperthreading (Intel) or SMT (AMD), x2\nVector units (multiple instructions processed on a vector of data)\n\nGPU computing : 100/1000 “simple” cores per card",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#the-reality",
    "href": "Courses/02_Parallel-intro.html#the-reality",
    "title": "3  Introduction to parallel computing",
    "section": "5.3 The reality",
    "text": "5.3 The reality\n\n\nA serial application only accesses 0.8% of the processing power of a 16-core CPU.\n\n\n\n0.08\\% = \\frac{1}{16 * 2 (cores + hyperthreading) * \\frac{256 (bitwide vector unit}{64(bit double)} = 128}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#faster-for-less-development",
    "href": "Courses/02_Parallel-intro.html#faster-for-less-development",
    "title": "3  Introduction to parallel computing",
    "section": "6.1 Faster for less development",
    "text": "6.1 Faster for less development\n\\frac{S_{up}}{T_{par}} \\gg \\frac{S_{up}}{T_{seq}}\nRatio of speedup improvment S_{up} over time of development (T_{seq|par}) comparison.\nFrom a development time perspective, return on investment (speedup) is often several magnitudes of order better than pure “serial/sequential” improvment.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#scaling",
    "href": "Courses/02_Parallel-intro.html#scaling",
    "title": "3  Introduction to parallel computing",
    "section": "6.2 Scaling",
    "text": "6.2 Scaling\nSimple “divide and conquer” strategies in parallel programming allow to handle data with previously almost untractable sizes and scale before.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#energy-efficiency",
    "href": "Courses/02_Parallel-intro.html#energy-efficiency",
    "title": "3  Introduction to parallel computing",
    "section": "6.3 Energy efficiency",
    "text": "6.3 Energy efficiency\n\n\n\n\n\n\nNote\n\n\n\nThis is a huge one, in the present context 😬\n\n\nDifficult to estimate but the Thermal Design Power (TDP), given by hardware manufacturers, is a good rule of thumb. Just factor the number of units, and usual proportionality rules.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#energy-efficiency-a-bunch-of-cpus",
    "href": "Courses/02_Parallel-intro.html#energy-efficiency-a-bunch-of-cpus",
    "title": "3  Introduction to parallel computing",
    "section": "6.4 Energy efficiency, a bunch of CPUs",
    "text": "6.4 Energy efficiency, a bunch of CPUs\nExample of “standard” use : 20 16-core Intel Xeon E5-4660 which is 120~W of TDP\nP = (20~Processors) * (120~W/~Processors) * (24~hours) = 57.60~kWhrs",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#energy-efficiency-just-a-few-big-gpus",
    "href": "Courses/02_Parallel-intro.html#energy-efficiency-just-a-few-big-gpus",
    "title": "3  Introduction to parallel computing",
    "section": "6.5 Energy efficiency, just a few (big) GPUs",
    "text": "6.5 Energy efficiency, just a few (big) GPUs\nA Tesla V100 GPU is of 300~W of TDP. Let’s use 4 of them.\nP = (4~GPUs) * (300~W/~GPUs) * (24~hours) = 28.80~kWhrs\n\\Longrightarrow half of the power use",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#terms-and-definitions",
    "href": "Courses/02_Parallel-intro.html#terms-and-definitions",
    "title": "3  Introduction to parallel computing",
    "section": "7.1 Terms and definitions",
    "text": "7.1 Terms and definitions\n\nSpeedup S_{up}(N): ratio of the time of execution in serial and parallel mode\nNumber of computing units N\nP (resp. S) is the parallel (resp. serial) fraction of the time spent in the parallel (resp. serial) part of the program (P+S=1).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#asymptote-of-parallel-computing-amdahls-law",
    "href": "Courses/02_Parallel-intro.html#asymptote-of-parallel-computing-amdahls-law",
    "title": "3  Introduction to parallel computing",
    "section": "7.2 Asymptote of parallel computing : Amdahl’s Law",
    "text": "7.2 Asymptote of parallel computing : Amdahl’s Law\nThere P is the fraction of the time spent in the parallel part of the program in a sequential execution.\nS_{up}(N) \\le \\frac{1}{S+\\frac{P}{N}}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#asymptote-of-parallel-computing-amdahls-law-graphic",
    "href": "Courses/02_Parallel-intro.html#asymptote-of-parallel-computing-amdahls-law-graphic",
    "title": "3  Introduction to parallel computing",
    "section": "7.3 Asymptote of parallel computing : Amdahl’s Law, Graphic",
    "text": "7.3 Asymptote of parallel computing : Amdahl’s Law, Graphic\n\n\nIdeal speedup : 100% of the code parallelized; 90%, 75%, and 50% : limited by the fractions of code that remain serial. (Robey and Zamora 2021)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#more-with-almost-less-the-pump-it-up-approach",
    "href": "Courses/02_Parallel-intro.html#more-with-almost-less-the-pump-it-up-approach",
    "title": "3  Introduction to parallel computing",
    "section": "7.4 More with (almost) less : the pump it up approach",
    "text": "7.4 More with (almost) less : the pump it up approach\nGustafson’s law\nThere now, P is the fraction of the time spent in the parallel part of the program in a parallel execution.\n\n\n\n\nWhen the size of the problem grows up proportionnaly to the number of computing units.\nS_{up}(N) \\le N - S*(N-1)\nwhere N is the number of computing units and S the serial fraction as before.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#more-with-almost-less-graphic",
    "href": "Courses/02_Parallel-intro.html#more-with-almost-less-graphic",
    "title": "3  Introduction to parallel computing",
    "section": "7.5 More with (almost) less : graphic",
    "text": "7.5 More with (almost) less : graphic\n\n\nLinear growth with the number of processor (and data size too)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#strong-vs-weak-scaling-definitions",
    "href": "Courses/02_Parallel-intro.html#strong-vs-weak-scaling-definitions",
    "title": "3  Introduction to parallel computing",
    "section": "7.6 Strong vs Weak Scaling, definitions",
    "text": "7.6 Strong vs Weak Scaling, definitions\n\n\nStrong Scaling\n\nStrong scaling represents the time to solution with respect to the number of processors for a fixed total size.\n\n\n\\Rightarrow Amdahl’s law\n\nWeak Scaling\n\nWeak scaling represents the time to solution with respect to the number of processors for a fixed-sized problem per processor.\n\n\n\\Rightarrow Gustafson’s law",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#strong-vs-weak-scaling-schemas",
    "href": "Courses/02_Parallel-intro.html#strong-vs-weak-scaling-schemas",
    "title": "3  Introduction to parallel computing",
    "section": "7.7 Strong vs Weak Scaling, schemas",
    "text": "7.7 Strong vs Weak Scaling, schemas",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#flynns-taxonomy",
    "href": "Courses/02_Parallel-intro.html#flynns-taxonomy",
    "title": "3  Introduction to parallel computing",
    "section": "8.1 Flynn’s taxonomy",
    "text": "8.1 Flynn’s taxonomy\n\n\n\n\nSimple Instruction\nMultiple Instructions\n\n\n\n\nSimple Data\n\n\n\n\nMultiple Data",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#a-different-approach",
    "href": "Courses/02_Parallel-intro.html#a-different-approach",
    "title": "3  Introduction to parallel computing",
    "section": "8.2 A different approach",
    "text": "8.2 A different approach\n\n\n\nParallelism level\nHardware\nSoftware\nParallelism extraction\n\n\n\n\nInstruction\nSIMD (or VLIW)\nIntrinsics\nCompiler\n\n\nThread\nMulti-core RTOS\nLibrary or language extension\nPartitioning/Scheduling (dependency control)\n\n\nTask\nMulti-core (w/o RTOS)\nProcesses (OS level)\nPartitioning/Scheduling",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#multi-processing-vs-multi-threading",
    "href": "Courses/02_Parallel-intro.html#multi-processing-vs-multi-threading",
    "title": "3  Introduction to parallel computing",
    "section": "8.3 Multi-processing vs Multi-threading",
    "text": "8.3 Multi-processing vs Multi-threading\n\n\n\n\n\nMulti-Processing\n\n\n\n\n\n\n\nMulti-Threading\n\n\n\n\n\n\n\n\n\n\nMulti-processing\nMulti-threading\n\n\n\n\nMemory\nExclusive\nShared\n\n\nCommunication\nInter-process\nAt caller site\n\n\nCreation overhead\nHeavy\nMinimal\n\n\nConcurrency\nAt OS level\nLibrary/language",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#footnotes",
    "href": "Courses/02_Parallel-intro.html#footnotes",
    "title": "3  Introduction to parallel computing",
    "section": "",
    "text": "i.e. a set of tools/machines used by a worker to complete a task↩︎\noverhead on resource access↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html",
    "href": "Courses/03_Asynchronous.html",
    "title": "4  Asynchronous Programming with Python",
    "section": "",
    "text": "5 Asynchronous, Basics",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#what-is-asynchronous-programming",
    "href": "Courses/03_Asynchronous.html#what-is-asynchronous-programming",
    "title": "4  Asynchronous Programming with Python",
    "section": "5.1 What is Asynchronous Programming?",
    "text": "5.1 What is Asynchronous Programming?\n\nAsynchronous programming is a programming paradigm that allows the program to continue executing other tasks before the current task is finished.\nIt is a way to achieve concurrency in a program.\n\n\\Rightarrow it is an abstraction over concurrency, it does not necessarily mean that the program is executed in parallel.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#io-bound-vs.-cpu-bound",
    "href": "Courses/03_Asynchronous.html#io-bound-vs.-cpu-bound",
    "title": "4  Asynchronous Programming with Python",
    "section": "5.2 I/O Bound vs. CPU Bound",
    "text": "5.2 I/O Bound vs. CPU Bound\nimport requests\n \n1response = requests.get('https://www.example.com')\n \nitems = response.headers.items()\n \n2headers = [f'{key}: {header}' for key, header in items]\n \n3formatted_headers = '\\n'.join(headers)\n \nwith open('headers.txt', 'w') as file:\n4    file.write(formatted_headers)\n\n1\n\nI/O-bound web request\n\n2\n\nCPU-bound response processing\n\n3\n\nCPU-bound string concatenation\n\n4\n\nI/O-bound write to disk",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#concurrency-vs.-parallelism",
    "href": "Courses/03_Asynchronous.html#concurrency-vs.-parallelism",
    "title": "4  Asynchronous Programming with Python",
    "section": "6.1 Concurrency vs. Parallelism",
    "text": "6.1 Concurrency vs. Parallelism\n\n\nOne baker and two cakes to prepare.\n\nCan preheat the oven while preparing the first cake.\nCan start the second cake while the first one is in the oven.\n\n\\Rightarrow Switching between tasks is concurrency (or concurrent behavior).\n\nTwo bakers and two cakes to prepare.\n\nCan prepare both cakes at the same time.\n\n\\Rightarrow Doing multiple tasks in parallel is parallelism (or parallel behavior).\n\n\n\n\nWith concurrency, we have multiple tasks happening at the same time, but only one we’re actively doing at a given point in time. With parallelism, we have multiple tasks happening and are actively doing more than one simultaneously.\n\n\n\n\nFrom Fowler (2022)\n\n\n\nWith concurrency, we switch between running two applications. With parallelism, we actively run two applications simultaneously.\n\n\n\n\nFrom Fowler (2022)\n\n\nConcurrency is about multiple independent tasks that can happen.\nParallelism is concurrency AND simultaneous execution.\n\nWhile parallelism implies concurrency, concurrency does not always imply parallelism.\n\\Rightarrow Concurrency is a broader concept than parallelism.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#multitasking",
    "href": "Courses/03_Asynchronous.html#multitasking",
    "title": "4  Asynchronous Programming with Python",
    "section": "6.2 Multitasking",
    "text": "6.2 Multitasking\n\n\n\n6.2.1 Preemptive multitasking\n\nThe operating system decides when to switch between tasks.\nThe tasks are not aware of each other.\n\n\n\n\n6.2.2 Cooperative multitasking\n\nIn this model we have to explicitly to decide when to switch between tasks.\nThe tasks are aware of each other.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#benefits-of-cooperative-multitasking",
    "href": "Courses/03_Asynchronous.html#benefits-of-cooperative-multitasking",
    "title": "4  Asynchronous Programming with Python",
    "section": "6.3 Benefits of cooperative multitasking",
    "text": "6.3 Benefits of cooperative multitasking\n\nLess overhead than preemptive multitasking.\nGranular/optimal control over when to switch between tasks.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#multi-processing-vs-multi-threading",
    "href": "Courses/03_Asynchronous.html#multi-processing-vs-multi-threading",
    "title": "4  Asynchronous Programming with Python",
    "section": "7.1 Multi-processing vs Multi-threading",
    "text": "7.1 Multi-processing vs Multi-threading\n\n\n\n\n\nMulti-Processing\n\n\n\n\n\n\n\nMulti-Threading",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#processes-and-threads",
    "href": "Courses/03_Asynchronous.html#processes-and-threads",
    "title": "4  Asynchronous Programming with Python",
    "section": "7.2 Processes and threads",
    "text": "7.2 Processes and threads\n\nimport os\nimport threading\n \nprint(f'Python process running with process id: {os.getpid()}')\ntotal_threads = threading.active_count()\nthread_name = threading.current_thread().name\n \nprint(f'Python is currently running {total_threads} thread(s)')\nprint(f'The current thread is {thread_name}')\n\nPython process running with process id: 89280\nPython is currently running 8 thread(s)\nThe current thread is MainThread",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#creating-processes",
    "href": "Courses/03_Asynchronous.html#creating-processes",
    "title": "4  Asynchronous Programming with Python",
    "section": "7.3 Creating processes",
    "text": "7.3 Creating processes\n\nimport multiprocessing\nimport os\n \n \ndef hello_from_process():\n    print(f'Hello from child process {os.getpid()}!')\nif __name__ == '__main__':\n    hello_process = multiprocessing.Process(target=hello_from_process)\n    hello_process.start()\n \n    print(f'Hello from parent process {os.getpid()}')\n \n    hello_process.join()\n\nHello from parent process 89280\nHello from child process 89329!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#creating-threads",
    "href": "Courses/03_Asynchronous.html#creating-threads",
    "title": "4  Asynchronous Programming with Python",
    "section": "7.4 Creating threads",
    "text": "7.4 Creating threads\n\nimport threading\n \n \ndef hello_from_thread():\n    print(f'Hello from thread {threading.current_thread()}!')\n \n \nhello_thread = threading.Thread(target=hello_from_thread)\nhello_thread.start()\n \ntotal_threads = threading.active_count()\nthread_name = threading.current_thread().name\n \nprint(f'Python is currently running {total_threads} thread(s)')\nprint(f'The current thread is {thread_name}')\n \nhello_thread.join()\n\nHello from thread &lt;Thread(Thread-6 (hello_from_thread), started 6226817024)&gt;!Python is currently running 9 thread(s)\nThe current thread is MainThread",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#what-about-python",
    "href": "Courses/03_Asynchronous.html#what-about-python",
    "title": "4  Asynchronous Programming with Python",
    "section": "8.1 What about Python?",
    "text": "8.1 What about Python?\n\n\nDesigned for sequential and single-core architecture from the beginning\nEverything is interpreted\nAll dynamic (no static types)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#the-gil",
    "href": "Courses/03_Asynchronous.html#the-gil",
    "title": "4  Asynchronous Programming with Python",
    "section": "8.2 The GIL",
    "text": "8.2 The GIL\nAka Global Interpreter Lock\n. . .\n\nThe GIL allows thread usage, you can create threads and launch them: YES!\n\n. . .\n\nbut…\n\n. . .\n\nOnly ONE thread can actually execute code at python level..",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#multi-threaded-parallel-execution",
    "href": "Courses/03_Asynchronous.html#multi-threaded-parallel-execution",
    "title": "4  Asynchronous Programming with Python",
    "section": "8.3 Multi-threaded != Parallel execution",
    "text": "8.3 Multi-threaded != Parallel execution\nMulti-threading doesn’t guarantee parallel execution…\n\n\n\n\n\n\\Longrightarrow Python seems to have started off with the wrong foot by a long way…",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#high-performance-python",
    "href": "Courses/03_Asynchronous.html#high-performance-python",
    "title": "4  Asynchronous Programming with Python",
    "section": "8.4 High performance Python 😬",
    "text": "8.4 High performance Python 😬\n\n\n\n\n\n\nBut wait!\n\n\nActually we can run (real) parallel programs with the multiprocessing package.\n\\Rightarrow But this is an “OS level” multiprocessing, with associated huge overhead (relatively)\nPython actually releases the GIL when executing everything that is not Python code (e.g. C/C++ extensions and libraries)\n\\Rightarrow It means we can parallelize our code by using I/O bound and CPU bound libraries that release the GIL (this is the case for most of them)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#socket",
    "href": "Courses/03_Asynchronous.html#socket",
    "title": "4  Asynchronous Programming with Python",
    "section": "9.1 Socket",
    "text": "9.1 Socket\n\n\nWriting bytes to a socket and reading bytes from a socket\n\n\n\n\nFrom Fowler (2022)\n\n\nThis a mailbox metaphor\nBy default, the socket is blocking, i.e. the program will wait until the socket is ready to be read or written.\nWe can make the socket non-blocking, i.e. the program will not wait for the socket to be ready to be read or written. \\Rightarrow Later on, the OS will tell us we received byte and we deal with it.\n\n\n\n\n\n\n\nMaking a non-blocking I/O request returns immediately\ntells the O/S to watch sockets for data \\Rightarrow This allows execute_other_code() to run right away instead of waiting for the I/O requests to finish\nLater, we can be alerted when I/O is complete and process the response.\n\n\n\n\n\nFrom Fowler (2022)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#event-loop",
    "href": "Courses/03_Asynchronous.html#event-loop",
    "title": "4  Asynchronous Programming with Python",
    "section": "9.2 Event Loop",
    "text": "9.2 Event Loop\n\n\nfrom collections import deque\n \nmessages = deque()\n \nwhile True:\n    if messages:\n        message = messages.pop()\n        process_message(message)\n\n\n\nThe event loop is a loop that runs forever.\nIt checks if there are any messages to process.\nIf there are, it processes them.\nIf there are not, it waits for messages to arrive.\n\n\n\n\n\\Rightarrow In asyncio, the event loop is queue of tasks instead of messages, Tasks are wrapped coroutines.\n\ndef make_request():\n    cpu_bound_setup()\n    io_bound_web_request()\n    cpu_bound_postprocess()\n \ntask_one = make_request()\ntask_two = make_request()\ntask_three = make_request()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#what-is-it",
    "href": "Courses/03_Asynchronous.html#what-is-it",
    "title": "4  Asynchronous Programming with Python",
    "section": "10.1 What is it?",
    "text": "10.1 What is it?\n\nasync def coroutine_add_one(number: int) -&gt; int:\n    return number + 1\n \ndef add_one(number: int) -&gt; int:\n    return number + 1\n \n1function_result = add_one(1)\n2coroutine_result = coroutine_add_one(1)\n \nprint(f'Function result is {function_result}\\n\\\n    and the type is {type(function_result)}')\nprint(f'Coroutine result is {coroutine_result}\\n\\\n    and the type is {type(coroutine_result)}')\n\n\n1\n\nfunction call, is executed immediately.\n\n2\n\ncoroutine call, is not executed at all, but returns a coroutine object.\n\n\n\n\nFunction result is 2\n    and the type is &lt;class 'int'&gt;\nCoroutine result is &lt;coroutine object coroutine_add_one at 0x121356440&gt;\n    and the type is &lt;class 'coroutine'&gt;\n\n\n\nFrom Fowler (2022)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#how-to-execute-a-coroutine",
    "href": "Courses/03_Asynchronous.html#how-to-execute-a-coroutine",
    "title": "4  Asynchronous Programming with Python",
    "section": "10.2 How to execute a coroutine?",
    "text": "10.2 How to execute a coroutine?\nYou need an event loop.\nimport asyncio\n \nasync def coroutine_add_one(number: int) -&gt; int:\n    return number + 1\n \n1result = asyncio.run(coroutine_add_one(1))\n\nprint(result)\n\n1\n\nThis launches the event loop, executes the coroutine, and returns the result.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis code will not work in a Jupyter notebook, because the event loop is already running (by Jupyter itself). So you just have to replace the line 4 by:\nresult = await coroutine_add_one(1)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#await-keyword",
    "href": "Courses/03_Asynchronous.html#await-keyword",
    "title": "4  Asynchronous Programming with Python",
    "section": "10.3 await keyword",
    "text": "10.3 await keyword\n\nimport asyncio\n \nasync def add_one(number: int) -&gt; int:\n    return number + 1\n \n \nasync def main() -&gt; None:\n1    one_plus_one = await add_one(1)\n2    two_plus_one = await add_one(2)\n    print(one_plus_one)\n    print(two_plus_one)\n \n3await main()\n\n\n1\n\nPause, and wait for the result of add_one(1).\n\n2\n\nPause, and wait for the result of add_one(2).\n\n3\n\nPause, and wait for the result of main(). (outside of a Jupyter notebook, you have to launch the event loop somewhere, like asyncio.run(main()) instead of await main())\n\n\n\n\n2\n3\n\n\n\n\nFrom Fowler (2022)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#simulating-the-real-thing-with-asyncio.sleep",
    "href": "Courses/03_Asynchronous.html#simulating-the-real-thing-with-asyncio.sleep",
    "title": "4  Asynchronous Programming with Python",
    "section": "10.4 Simulating the real thing with asyncio.sleep",
    "text": "10.4 Simulating the real thing with asyncio.sleep\n\nimport asyncio\n \nasync def hello_world_message() -&gt; str:\n1    await asyncio.sleep(1)\n    return 'Hello World!'\n \nasync def main() -&gt; None:\n2    message = await hello_world_message()\n    print(message)\n \nawait main()\n\n\n1\n\nPause hello_world_message for 1 second.\n\n2\n\nPause main until hello_world_message is finished.\n\n\n\n\nHello World!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#utility-function-delayseconds",
    "href": "Courses/03_Asynchronous.html#utility-function-delayseconds",
    "title": "4  Asynchronous Programming with Python",
    "section": "10.5 Utility function delay(seconds)",
    "text": "10.5 Utility function delay(seconds)\n\nimport asyncio\n \n \n1async def delay(delay_seconds: int) -&gt; int:\n2    print(f'sleeping for {delay_seconds} second(s)')\n    await asyncio.sleep(delay_seconds)\n    print(f'finished sleeping for {delay_seconds} second(s)')\n3    return delay_seconds\n\n\n1\n\nTakes an integer of the duration in seconds that we’d like the function to sleep.\n\n2\n\nPrints when sleep begins and ends.\n\n3\n\nReturns that integer to the caller once it has finished sleeping.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#running-two-coroutines",
    "href": "Courses/03_Asynchronous.html#running-two-coroutines",
    "title": "4  Asynchronous Programming with Python",
    "section": "10.6 Running two coroutines",
    "text": "10.6 Running two coroutines\n\nimport asyncio\n \nasync def add_one(number: int) -&gt; int:\n    return number + 1\n \nasync def hello_world_message() -&gt; str:\n    await delay(1)\n    return 'Hello World!'\n \nasync def main() -&gt; None:\n1    message = await hello_world_message()\n2    one_plus_one = await add_one(1)\n    print(one_plus_one)\n    print(message)\n \nawait main()\n\n\n1\n\nPause main until hello_world_message is finished.\n\n2\n\nPause main until add_one is finished.\n\n\n\n\nsleeping for 1 second(s)\nfinished sleeping for 1 second(s)\n2\nHello World!\n\n\n\n\nFrom Fowler (2022)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#what-to-do-next",
    "href": "Courses/03_Asynchronous.html#what-to-do-next",
    "title": "4  Asynchronous Programming with Python",
    "section": "10.7 What to do next?",
    "text": "10.7 What to do next?\nMoving away from sequential execution and run add_one and hello_world_message concurrently.\nFor that we need…",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#creating-tasks",
    "href": "Courses/03_Asynchronous.html#creating-tasks",
    "title": "4  Asynchronous Programming with Python",
    "section": "11.1 Creating tasks",
    "text": "11.1 Creating tasks\n\nimport asyncio\n\nasync def main():\n    sleep_for_three = asyncio.create_task(delay(3))\n    print(type(sleep_for_three))\n    result = await sleep_for_three\n    print(result)\n \nawait main()\n\n&lt;class '_asyncio.Task'&gt;\nsleeping for 3 second(s)\nfinished sleeping for 3 second(s)\n3\n\n\n\nthe coroutine is scheduled to run in the event loop as soon as possible.\nbefore, it was just run at the await statement (pausing the caller).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#running-tasks-concurrently",
    "href": "Courses/03_Asynchronous.html#running-tasks-concurrently",
    "title": "4  Asynchronous Programming with Python",
    "section": "11.2 Running tasks concurrently",
    "text": "11.2 Running tasks concurrently\n\nimport asyncio\n \nasync def main():\n    sleep_for_three = \\\n        asyncio.create_task(delay(3))\n    sleep_again = \\\n        asyncio.create_task(delay(3))\n    sleep_once_more = \\\n        asyncio.create_task(delay(3))\n \n    await sleep_for_three\n    await sleep_again\n    await sleep_once_more\n\nawait main()\n\nsleeping for 3 second(s)\nsleeping for 3 second(s)\nsleeping for 3 second(s)\nfinished sleeping for 3 second(s)\nfinished sleeping for 3 second(s)\nfinished sleeping for 3 second(s)\n\n\n\n\nFrom Fowler (2022)\n\n\nimport asyncio\n \nasync def hello_every_second():\n    for i in range(2):\n        await asyncio.sleep(1)\n        print(\"I'm running other code while I'm waiting!\")\n \nasync def main():\n    first_delay = asyncio.create_task(delay(3))\n    second_delay = asyncio.create_task(delay(3))\n    await hello_every_second()\n    await first_delay\n    await second_delay\n\nawait main()\n\nsleeping for 3 second(s)\nsleeping for 3 second(s)\nI'm running other code while I'm waiting!\nI'm running other code while I'm waiting!\nfinished sleeping for 3 second(s)\nfinished sleeping for 3 second(s)\n\n\n\n\nFrom Fowler (2022)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#canceling-tasks",
    "href": "Courses/03_Asynchronous.html#canceling-tasks",
    "title": "4  Asynchronous Programming with Python",
    "section": "11.3 Canceling tasks",
    "text": "11.3 Canceling tasks\n\nimport asyncio\nfrom asyncio import CancelledError\n\nasync def main():\n    long_task = asyncio.create_task(delay(10))\n \n    seconds_elapsed = 0\n \n    while not long_task.done():\n        print('Task not finished, checking again in a second.')\n        await asyncio.sleep(1)\n        seconds_elapsed = seconds_elapsed + 1\n        if seconds_elapsed == 5:\n            long_task.cancel()\n \n    try:\n        await long_task\n    except CancelledError:\n        print('Our task was cancelled')\n \nawait main()\n\nTask not finished, checking again in a second.\nsleeping for 10 second(s)\nTask not finished, checking again in a second.\nTask not finished, checking again in a second.\nTask not finished, checking again in a second.\nTask not finished, checking again in a second.\nTask not finished, checking again in a second.\nOur task was cancelled",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#setting-a-timeout",
    "href": "Courses/03_Asynchronous.html#setting-a-timeout",
    "title": "4  Asynchronous Programming with Python",
    "section": "11.4 Setting a timeout",
    "text": "11.4 Setting a timeout\n\nimport asyncio\n\nasync def main():\n    delay_task = asyncio.create_task(delay(2))\n    try:\n        result = await asyncio.wait_for(delay_task, timeout=1)\n        print(result)\n    except asyncio.exceptions.TimeoutError:\n        print('Got a timeout!')\n        print(f'Was the task cancelled? {delay_task.cancelled()}')\n \nawait main()\n\nsleeping for 2 second(s)\nGot a timeout!\nWas the task cancelled? True",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#introducing-futures",
    "href": "Courses/03_Asynchronous.html#introducing-futures",
    "title": "4  Asynchronous Programming with Python",
    "section": "12.1 Introducing futures",
    "text": "12.1 Introducing futures\n\nfrom asyncio import Future\n \nmy_future = Future()\n \nprint(f'Is my_future done? {my_future.done()}')\n \nmy_future.set_result(42)\n \nprint(f'Is my_future done? {my_future.done()}')\nprint(f'What is the result of my_future? {my_future.result()}')\n\nIs my_future done? False\nIs my_future done? True\nWhat is the result of my_future? 42",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#awaiting-futures",
    "href": "Courses/03_Asynchronous.html#awaiting-futures",
    "title": "4  Asynchronous Programming with Python",
    "section": "12.2 Awaiting futures",
    "text": "12.2 Awaiting futures\n\nfrom asyncio import Future\nimport asyncio\n \n \ndef make_request() -&gt; Future:\n    future = Future()\n1    asyncio.create_task(set_future_value(future))\n    return future\n \n \nasync def set_future_value(future) -&gt; None:\n2    await asyncio.sleep(1)\n    future.set_result(42)\n \n \nasync def main():\n    future = make_request()\n    print(f'Is the future done? {future.done()}')\n3    value = await future\n    print(f'Is the future done? {future.done()}')\n    print(value)\n \nawait main()\n\n\n1\n\nCreate a task to asynchronously set the value of the future.\n\n2\n\nWait 1 second before setting the value of the future.\n\n3\n\nPause main until the future’s value is set.\n\n\n\n\nIs the future done? False\nIs the future done? True\n42",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#comparing-tasks-coroutines-futures-and-awaitables",
    "href": "Courses/03_Asynchronous.html#comparing-tasks-coroutines-futures-and-awaitables",
    "title": "4  Asynchronous Programming with Python",
    "section": "12.3 Comparing tasks, coroutines, futures, and awaitables",
    "text": "12.3 Comparing tasks, coroutines, futures, and awaitables\n\n\n\n\n\n\n\nAwaitables\n\nObjects that can be awaited in an async function, including coroutines, tasks, and futures.\n\nCoroutines\n\nSpecial functions that can be paused and resumed later, defined using async def, and can be awaited to allow other coroutines to run.\n\nFutures\n\nRepresent the result of an asynchronous operation, manage its state, and can be awaited to get the result.\n\nTasks\n\nSchedule and run coroutines concurrently, and can be used to cancel or check their status.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#with-a-decorator",
    "href": "Courses/03_Asynchronous.html#with-a-decorator",
    "title": "4  Asynchronous Programming with Python",
    "section": "13.1 With a decorator",
    "text": "13.1 With a decorator\n\n\n\nimport functools\nimport time\nfrom typing import Callable, Any\n \ndef async_timed():\n    def wrapper(func: Callable) -&gt; Callable:\n        @functools.wraps(func)\n        async def wrapped(*args, **kwargs) -&gt; Any:\n            print(f'starting {func} with args {args} {kwargs}')\n            start = time.time()\n            try:\n                return await func(*args, **kwargs)\n            finally:\n                end = time.time()\n                total = end - start\n                print(f'finished {func} in {total:.4f} second(s)')\n \n        return wrapped\n \n    return wrapper\n\n\nOfficial Python documentation for decorators\n\nadd functionality to an existing function\nwithout modifying the function itself\nit intercepts the function call and runs “decorated” code before and after it",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#using-it",
    "href": "Courses/03_Asynchronous.html#using-it",
    "title": "4  Asynchronous Programming with Python",
    "section": "13.2 Using it",
    "text": "13.2 Using it\n\nimport asyncio\n \n@async_timed()\nasync def delay(delay_seconds: int) -&gt; int:\n    print(f'sleeping for {delay_seconds} second(s)')\n    await asyncio.sleep(delay_seconds)\n    print(f'finished sleeping for {delay_seconds} second(s)')\n    return delay_seconds\n \n \n@async_timed()\nasync def main():\n    task_one = asyncio.create_task(delay(2))\n    task_two = asyncio.create_task(delay(3))\n \n    await task_one\n    await task_two\n\nawait main()\n\nstarting &lt;function main at 0x1213b5120&gt; with args () {}\nstarting &lt;function delay at 0x1213b47c0&gt; with args (2,) {}\nsleeping for 2 second(s)\nstarting &lt;function delay at 0x1213b47c0&gt; with args (3,) {}\nsleeping for 3 second(s)\nfinished sleeping for 2 second(s)\nfinished &lt;function delay at 0x1213b47c0&gt; in 2.0011 second(s)\nfinished sleeping for 3 second(s)\nfinished &lt;function delay at 0x1213b47c0&gt; in 3.0013 second(s)\nfinished &lt;function main at 0x1213b5120&gt; in 3.0016 second(s)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#asyncio.gather",
    "href": "Courses/03_Asynchronous.html#asyncio.gather",
    "title": "4  Asynchronous Programming with Python",
    "section": "13.3 asyncio.gather",
    "text": "13.3 asyncio.gather\nasyncio.gather() runs multiple asynchronous operations, wraps a coroutine as a task, and returns a list of results in the same order of awaitables.\n\nimport asyncio\n\n\nasync def call_api(message, result, delay=3):\n    print(message)\n    await asyncio.sleep(delay)\n    return result\n\n\nasync def main():\n    return await asyncio.gather(\n        call_api('Calling API 1 ...', 1),\n        call_api('Calling API 2 ...', 2)\n    )\n\nawait main()\n\nCalling API 1 ...\nCalling API 2 ...\n\n\n[1, 2]\n\n\n\n\n\n\n\n\nCaution\n\n\n\nasyncio.gather takes a tuple of awaitables, not a list of awaitables, but returns a list of results in the same order of awaitables.\nIf you want to pass a list, use the * operator to unpack it as a tuple.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#running-cpu-bound-code",
    "href": "Courses/03_Asynchronous.html#running-cpu-bound-code",
    "title": "4  Asynchronous Programming with Python",
    "section": "14.1 Running CPU-bound code",
    "text": "14.1 Running CPU-bound code\n\nimport asyncio\n\n@async_timed()\nasync def cpu_bound_work() -&gt; int:\n    counter = 0\n    for i in range(100000000):\n        counter = counter + 1\n    return counter\n \n \n@async_timed()\nasync def main():\n    task_one = asyncio.create_task(cpu_bound_work())\n    task_two = asyncio.create_task(cpu_bound_work())\n    await task_one\n    await task_two\n \nawait main()\n\nstarting &lt;function main at 0x1213b5940&gt; with args () {}\nstarting &lt;function cpu_bound_work at 0x1213b5760&gt; with args () {}\nfinished &lt;function cpu_bound_work at 0x1213b5760&gt; in 1.2570 second(s)\nstarting &lt;function cpu_bound_work at 0x1213b5760&gt; with args () {}\nfinished &lt;function cpu_bound_work at 0x1213b5760&gt; in 1.2420 second(s)\nfinished &lt;function main at 0x1213b5940&gt; in 2.4994 second(s)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#running-blocking-apis",
    "href": "Courses/03_Asynchronous.html#running-blocking-apis",
    "title": "4  Asynchronous Programming with Python",
    "section": "14.2 Running blocking APIs",
    "text": "14.2 Running blocking APIs\n\nimport asyncio\nimport requests\n \n@async_timed()\nasync def get_example_status() -&gt; int:\n    return requests.get('http://www.example.com').status_code\n \n \n@async_timed()\nasync def main():\n    task_1 = asyncio.create_task(get_example_status())\n    task_2 = asyncio.create_task(get_example_status())\n    task_3 = asyncio.create_task(get_example_status())\n    await task_1\n    await task_2\n    await task_3\n \nawait main()\n\nstarting &lt;function main at 0x121a04400&gt; with args () {}\nstarting &lt;function get_example_status at 0x1213b56c0&gt; with args () {}\nfinished &lt;function get_example_status at 0x1213b56c0&gt; in 0.1187 second(s)\nstarting &lt;function get_example_status at 0x1213b56c0&gt; with args () {}\nfinished &lt;function get_example_status at 0x1213b56c0&gt; in 0.0649 second(s)\nstarting &lt;function get_example_status at 0x1213b56c0&gt; with args () {}\nfinished &lt;function get_example_status at 0x1213b56c0&gt; in 0.0680 second(s)\nfinished &lt;function main at 0x121a04400&gt; in 0.2522 second(s)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#example-of-blocking-code",
    "href": "Courses/03_Asynchronous.html#example-of-blocking-code",
    "title": "4  Asynchronous Programming with Python",
    "section": "15.1 Example of blocking code",
    "text": "15.1 Example of blocking code\n\nimport requests\n \n \ndef get_status_code(url: str) -&gt; int:\n    response = requests.get(url)\n    return response.status_code\n \n \nurl = 'https://www.example.com'\nprint(get_status_code(url))\nprint(get_status_code(url))\n\n200\n200",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#thread-pool",
    "href": "Courses/03_Asynchronous.html#thread-pool",
    "title": "4  Asynchronous Programming with Python",
    "section": "15.2 Thread Pool",
    "text": "15.2 Thread Pool\n\nimport time\nimport requests\nfrom concurrent.futures import ThreadPoolExecutor\n \n \ndef get_status_code(url: str) -&gt; int:\n    response = requests.get(url)\n    return response.status_code\n \n \nstart = time.time()\n \nwith ThreadPoolExecutor() as pool:\n    urls = ['https://www.example.com' for _ in range(10)]\n    results = pool.map(get_status_code, urls)\n    for result in results:\n        # print(result)\n        pass\n\n \nend = time.time()\n \nprint(f'finished requests in {end - start:.4f} second(s)')\n\nfinished requests in 0.8679 second(s)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#compare-with-sequential-code",
    "href": "Courses/03_Asynchronous.html#compare-with-sequential-code",
    "title": "4  Asynchronous Programming with Python",
    "section": "15.3 Compare with sequential code",
    "text": "15.3 Compare with sequential code\n\nstart = time.time()\n \nurls = ['https://www.example.com' for _ in range(10)]\n \nfor url in urls:\n    result = get_status_code(url)\n    # print(result)\n \nend = time.time()\n \nprint(f'finished requests in {end - start:.4f} second(s)')\n\nfinished requests in 1.4278 second(s)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#thread-pool-with-asyncio",
    "href": "Courses/03_Asynchronous.html#thread-pool-with-asyncio",
    "title": "4  Asynchronous Programming with Python",
    "section": "15.4 Thread pool with asyncio",
    "text": "15.4 Thread pool with asyncio\n\nimport functools\nimport requests\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor\n \ndef get_status_code(url: str) -&gt; int:\n    response = requests.get(url)\n    return response.status_code\n \n \n@async_timed()\nasync def main():\n    loop = asyncio.get_running_loop()\n    with ThreadPoolExecutor() as pool:\n        urls = ['https://www.example.com' for _ in range(10)]\n        tasks = [loop.run_in_executor(pool, functools.partial(get_status_code, url)) for url in urls]\n        results = await asyncio.gather(*tasks)\n        print(results)\n \nawait main()\n\nstarting &lt;function main at 0x1213b5940&gt; with args () {}\n[200, 200, 200, 200, 200, 200, 200, 200, 200, 200]\nfinished &lt;function main at 0x1213b5940&gt; in 0.8599 second(s)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#multithreading-with-numpy",
    "href": "Courses/03_Asynchronous.html#multithreading-with-numpy",
    "title": "4  Asynchronous Programming with Python",
    "section": "15.5 Multithreading with numpy",
    "text": "15.5 Multithreading with numpy\nLet’s define a big matrix on which we will compute the mean of each row.\nNow process the matrix sequentially.\n\ns = time.time()\n \nres_seq = np.mean(matrix, axis=1)\n \ne = time.time()\nprint(e - s)\n\n0.30220913887023926\n\n\nAnd then the same with multithreading (we check that the results are exactly the same).\n\nimport functools\nfrom concurrent.futures.thread import ThreadPoolExecutor\nimport asyncio\n \ndef mean_for_row(arr, row):\n    return np.mean(arr[row])\n \n@async_timed()\nasync def main():\n    loop = asyncio.get_running_loop()\n    with ThreadPoolExecutor() as pool:\n        tasks = []\n        for i in range(rows):\n            mean = functools.partial(mean_for_row, matrix, i)\n            tasks.append(loop.run_in_executor(pool, mean))\n \n        return await asyncio.gather(*tasks)\n\nres_threads = np.array(await main())\nnp.testing.assert_array_equal(res_seq, res_threads)\n\nstarting &lt;function main at 0x121a05580&gt; with args () {}\nfinished &lt;function main at 0x121a05580&gt; in 0.0163 second(s)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html",
    "href": "Courses/04_IPC-and-Locking.html",
    "title": "5  IPC and locking",
    "section": "",
    "text": "6 Inter-Process Communication",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#remainder-on-process-level-parallelization",
    "href": "Courses/04_IPC-and-Locking.html#remainder-on-process-level-parallelization",
    "title": "5  IPC and locking",
    "section": "6.1 Remainder on Process-level parallelization",
    "text": "6.1 Remainder on Process-level parallelization\n\n\n\n\n\nMulti-Processing\n\n\n\n\n\n\n\nMulti-Threading",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#inter-process-is-easy",
    "href": "Courses/04_IPC-and-Locking.html#inter-process-is-easy",
    "title": "5  IPC and locking",
    "section": "6.2 Inter-process is easy…",
    "text": "6.2 Inter-process is easy…\n\n\nBut if my algorithm is not “embarrassingly parallel”, what if we want to share data between processes ?\nlet’s go for Shared Memory",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#shared-memory-model",
    "href": "Courses/04_IPC-and-Locking.html#shared-memory-model",
    "title": "5  IPC and locking",
    "section": "6.3 Shared Memory Model",
    "text": "6.3 Shared Memory Model\n\n\nShared Memory Model",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#aside-memory-models",
    "href": "Courses/04_IPC-and-Locking.html#aside-memory-models",
    "title": "5  IPC and locking",
    "section": "6.4 Aside : memory models",
    "text": "6.4 Aside : memory models\n\n\n\n\n\nUMA\n\n\n\n\n\n \n\n\n\n\nNUMA\n\n\n\n\n\n\n\nThere are differents models",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#shared-fifos-queues",
    "href": "Courses/04_IPC-and-Locking.html#shared-fifos-queues",
    "title": "5  IPC and locking",
    "section": "6.5 Shared FIFOs : Queues",
    "text": "6.5 Shared FIFOs : Queues\nAn ubiquitous tool in multiprocessing (and distributed computing) is shared memory FIFO list, aka Queues.\nA FIFO is a :\n\nLinked list\nwith FIFO (First In First Out) semantics, with enqueue(x) et dequeue() function (or push(x)/pop())\n\n\n\n\n\n\nIn the context of multi-processing (or multi-threading) :\nShared Memory + FIFO list = Queue\nQueues are the basis of the consumer/producer model, which is widely used in concurrent and distributed applications.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#when-to-use-queues",
    "href": "Courses/04_IPC-and-Locking.html#when-to-use-queues",
    "title": "5  IPC and locking",
    "section": "6.6 When to use queues?",
    "text": "6.6 When to use queues?\nAn algorithm with two computations A and B where :\n\nB depends on the result of A\nA is independent of B\n\n. . .\nA could be a producer for B, and B a consumer for A.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#how-to-use-queues",
    "href": "Courses/04_IPC-and-Locking.html#how-to-use-queues",
    "title": "5  IPC and locking",
    "section": "6.7 How to use queues?",
    "text": "6.7 How to use queues?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#producerconsumer-examples",
    "href": "Courses/04_IPC-and-Locking.html#producerconsumer-examples",
    "title": "5  IPC and locking",
    "section": "6.8 Producer/consumer, Examples",
    "text": "6.8 Producer/consumer, Examples\n\nA finds primes in a list of number, B formats and prints them every 10 numbers found.\nA fetches a bunch of images on the web, B downloads them and saves them to disk.\nA takes the orders in the restaurant, B cooks them.\n\n. . .",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#the-main-gotcha",
    "href": "Courses/04_IPC-and-Locking.html#the-main-gotcha",
    "title": "5  IPC and locking",
    "section": "7.1 The main gotcha",
    "text": "7.1 The main gotcha\nwhat if several processes want to write/read the same shared memory portions at the same time?\n. . .\nEnter the realm of the dreaded race condition",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#simple-example",
    "href": "Courses/04_IPC-and-Locking.html#simple-example",
    "title": "5  IPC and locking",
    "section": "7.2 Simple example",
    "text": "7.2 Simple example\nPrinting from several processes a string with 10 times the same char.\n\n\nfrom multiprocessing.pool import Pool\nfrom itertools import repeat\n# print \"AAAAAAAAA\", \"BBBBBBBBBBB\" etc.\ndef repeat10Cap(c): \n    print(\"\".join(repeat(chr(c+65),10))) \nwith Pool(8) as pool:\n    pool.map(repeat10Cap, range(10))\n\n\nOutput:\nAAAAAAAAAACCCCCCCCCCBBBBBBBBBBDDDDDDDDDDEEEEEEEEEE\n\n\nFFFFFFFFFFGGGGGGGGGG\nIIIIIIIIII\n\nHHHHHHHHHH\nJJJJJJJJJJ",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#critical-section-workflow",
    "href": "Courses/04_IPC-and-Locking.html#critical-section-workflow",
    "title": "5  IPC and locking",
    "section": "8.1 Critical section workflow",
    "text": "8.1 Critical section workflow\n\n\nThree processes with critical section",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#a-simple-implementation-in-python-lock",
    "href": "Courses/04_IPC-and-Locking.html#a-simple-implementation-in-python-lock",
    "title": "5  IPC and locking",
    "section": "8.2 A simple implementation in Python : Lock",
    "text": "8.2 A simple implementation in Python : Lock\n\n\nfrom multiprocessing.pool import Pool\nfrom multiprocessing import Lock\nfrom itertools import repeat\nlock = Lock()\ndef safe_repeat10Cap(c):\n    with lock: \n        # Beginning of critical section\n        print(\"\".join(repeat(chr(c+65),10)))\n        # End of critical section\nwith Pool(8) as pool:\n    pool.map(safe_repeat10Cap, range(10))\n\n\nOutput:\nAAAAAAAAAA\nBBBBBBBBBB\nCCCCCCCCCC\nDDDDDDDDDD\nEEEEEEEEEE\nFFFFFFFFFF\nGGGGGGGGGG\nHHHHHHHHHH\nIIIIIIIIII\nJJJJJJJJJJ",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#consistency-problems-with-fifo-example-i",
    "href": "Courses/04_IPC-and-Locking.html#consistency-problems-with-fifo-example-i",
    "title": "5  IPC and locking",
    "section": "9.1 Consistency problems with FIFO example I",
    "text": "9.1 Consistency problems with FIFO example I\nProcess A (resp. B) wants to push x (resp. y) on the list.\n\n\n\\Longrightarrow Consistency problem if they both create a new linked node to node 3.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#consistency-problems-with-fifo-example-2",
    "href": "Courses/04_IPC-and-Locking.html#consistency-problems-with-fifo-example-2",
    "title": "5  IPC and locking",
    "section": "9.2 Consistency problems with FIFO example 2",
    "text": "9.2 Consistency problems with FIFO example 2\nProcess A and B both want to pop the list.\n\n\n\\Longrightarrow Consistency problem if they both pop the same node.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#no-consistency-problems-with-fifo-example-3",
    "href": "Courses/04_IPC-and-Locking.html#no-consistency-problems-with-fifo-example-3",
    "title": "5  IPC and locking",
    "section": "9.3 (No) Consistency problems with FIFO example 3",
    "text": "9.3 (No) Consistency problems with FIFO example 3\n\n\nNo problem there.\n\n\n\n. . .\n\n\n\n\n\n\nWarning\n\n\n\n⚠ ⚠ As long the list is not empty ⚠ ⚠",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#deadlock-example",
    "href": "Courses/04_IPC-and-Locking.html#deadlock-example",
    "title": "5  IPC and locking",
    "section": "10.1 Deadlock example",
    "text": "10.1 Deadlock example",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#deadlock-serious-example",
    "href": "Courses/04_IPC-and-Locking.html#deadlock-serious-example",
    "title": "5  IPC and locking",
    "section": "10.2 Deadlock (serious) example",
    "text": "10.2 Deadlock (serious) example\n\n\nDeadlock illustration\n\n\n\n\nProcess A acquires lock L1. Process B acquires lock L2. Process A tries to acquire lock L2, but it is already held by B. Process B tries to acquire lock L1, but it is already held by A. Both processes are blocked.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#avoiding-deadlocks",
    "href": "Courses/04_IPC-and-Locking.html#avoiding-deadlocks",
    "title": "5  IPC and locking",
    "section": "10.3 Avoiding Deadlocks",
    "text": "10.3 Avoiding Deadlocks\nThere is several ways to avoid deadlocks. One of them is the Dijkstra’s Resource Hiearchy Solution.\n. . .\nIn the previous example, processes should try the lowest numbered locks first. Instead of B acquiring L2 first, it should tries to acquire L1 instead and L2 after.\n. . .\nThis solution isn’t universal but is pretty usable in general case.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/05_Distributed.html",
    "href": "Courses/05_Distributed.html",
    "title": "6  Distributed Computing models",
    "section": "",
    "text": "7 Map-Reduce",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/05_Distributed.html#the-real-beating-heart-of-big-data",
    "href": "Courses/05_Distributed.html#the-real-beating-heart-of-big-data",
    "title": "6  Distributed Computing models",
    "section": "7.1 The (real) beating Heart of Big Data",
    "text": "7.1 The (real) beating Heart of Big Data\nMap\\rightarrow{}Reduce pattern is the most common pattern to process data in (real) Big Data.\n\nIt is heavily used by Google, Facebook, and IBM.\n\n\nHadoop from Apache is a popular Map-Reduce framework (also called MapReduce in the Hadoop framework, not to be confused with the more general Map\\rightarrow{}Reduce Pattern).\n\n\nHadoop is backed by a HDFS (Hadoop Distributed File System) and a YARN (Yet Another Resource Manager)\n\n\n\nHDFS is a distributed file system (a file system that is distributed across a cluster of computers)\nYARN is a resource manager (a program that manages the resources of a cluster)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/05_Distributed.html#split-apply-combine-pattern",
    "href": "Courses/05_Distributed.html#split-apply-combine-pattern",
    "title": "6  Distributed Computing models",
    "section": "7.2 Split-Apply-Combine pattern",
    "text": "7.2 Split-Apply-Combine pattern\n\n\n\n\nSplit:\n\nSplit the data into smaller pieces\n\nApply:\n\nProcess the data in the pieces\n\nCombine:\n\nMerge the results",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/05_Distributed.html#map",
    "href": "Courses/05_Distributed.html#map",
    "title": "6  Distributed Computing models",
    "section": "7.3 Map",
    "text": "7.3 Map\nMap takes one pair of data with a type in one data domain, and returns a list of pairs in a different domain:\nMap(k1,v1) → list(k2,v2)\n\\Longrightarrow heavily parallelized",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/05_Distributed.html#reduce",
    "href": "Courses/05_Distributed.html#reduce",
    "title": "6  Distributed Computing models",
    "section": "7.4 Reduce",
    "text": "7.4 Reduce\nThe values associated from the same key are combined.\nThe Reduce function is then applied in parallel to each group, which in turn produces a collection of values in the same domain:\nReduce(k2, list (v2)) → list((k3, v3))",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/05_Distributed.html#schema",
    "href": "Courses/05_Distributed.html#schema",
    "title": "6  Distributed Computing models",
    "section": "7.5 Schema",
    "text": "7.5 Schema",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/05_Distributed.html#canonical-example-word-count-i",
    "href": "Courses/05_Distributed.html#canonical-example-word-count-i",
    "title": "6  Distributed Computing models",
    "section": "7.6 Canonical example : Word Count, I",
    "text": "7.6 Canonical example : Word Count, I\nThe canonical MapReduce example counts the appearance of each word in a set of documents\ndef map(name, document):\n  // name: document name\n  // document: document contents (list of words)\n  for word in document:\n    emit (word, 1)\n\ndef reduce(word, partialCounts):\n  // word: a word\n  // partialCounts: a list of aggregated partial counts\n  sum = 0\n  for pc in partialCounts:\n    sum += pc\n  emit (word, sum)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/05_Distributed.html#canonical-example-word-count-ii",
    "href": "Courses/05_Distributed.html#canonical-example-word-count-ii",
    "title": "6  Distributed Computing models",
    "section": "7.7 Canonical example : Word Count, II",
    "text": "7.7 Canonical example : Word Count, II",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/05_Distributed.html#spark-spiritual-son-of-mapreduce",
    "href": "Courses/05_Distributed.html#spark-spiritual-son-of-mapreduce",
    "title": "6  Distributed Computing models",
    "section": "7.8 Spark, spiritual son of MapReduce",
    "text": "7.8 Spark, spiritual son of MapReduce\nSpark is widely used for machine learning on scalable data sets (faster than MapReduce by an order of magnitude).\n\nSpark is largely inspired by the MapReduce pattern but extends it by using a distributed graph rather than a “linear” data flow like Map\\rightarrow{}Reduce.\n\n\n\\Longrightarrow Complex distributed computing.\n\n\nSpark emphasizes ease of use of the cluster resources in a simple and functional way",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/05_Distributed.html#spark-code-example-word-count",
    "href": "Courses/05_Distributed.html#spark-code-example-word-count",
    "title": "6  Distributed Computing models",
    "section": "7.9 Spark, code example : Word Count",
    "text": "7.9 Spark, code example : Word Count\ntext_file = sc.textFile(\"hdfs://...\")\ncounts = text_file.flatMap(lambda line: line.split(\" \")) \\\n             .map(lambda word: (word, 1)) \\\n             .reduceByKey(lambda a, b: a + b)\ncounts.saveAsTextFile(\"hdfs://...\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/05_Distributed.html#spark-another-example-machine-learning",
    "href": "Courses/05_Distributed.html#spark-another-example-machine-learning",
    "title": "6  Distributed Computing models",
    "section": "7.10 Spark, another example : machine learning",
    "text": "7.10 Spark, another example : machine learning\n# Every record of this DataFrame contains the label and\n# features represented by a vector.\ndf = sqlContext.createDataFrame(data, [\"label\", \"features\"])\n\n# Set parameters for the algorithm.\n# Here, we limit the number of iterations to 10.\nlr = LogisticRegression(maxIter=10)\n\n# Fit the model to the data.\nmodel = lr.fit(df)\n\n# Given a dataset, predict each point's label, \n# and show the results.\nmodel.transform(df).show()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/05_Distributed.html#schema-1",
    "href": "Courses/05_Distributed.html#schema-1",
    "title": "6  Distributed Computing models",
    "section": "8.1 Schema",
    "text": "8.1 Schema",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/05_Distributed.html#main-message-passing-functions",
    "href": "Courses/05_Distributed.html#main-message-passing-functions",
    "title": "6  Distributed Computing models",
    "section": "8.2 Main message-passing functions",
    "text": "8.2 Main message-passing functions\n\n\nScatter\n\npartition the data into smaller pieces and send them to the different processes\n\nGather\n\ncollect the data from the different processes and merge them.\n\nBroadcast\n\nSend the same data to all the processes.\n\nReduce\n\nMerge the data from all the processes and produce a single result.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/06_Dask-Ray.html",
    "href": "Courses/06_Dask-Ray.html",
    "title": "7  Dask and Ray",
    "section": "",
    "text": "8 Dask vs. Ray: Programming Paradigm Comparison\nHere’s a structured outline for your lecture comparing Dask and Ray from a programming paradigm perspective",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask and Ray</span>"
    ]
  },
  {
    "objectID": "Courses/06_Dask-Ray.html#dask",
    "href": "Courses/06_Dask-Ray.html#dask",
    "title": "7  Dask and Ray",
    "section": "8.1  Dask",
    "text": "8.1  Dask\n\n\n\n\nDask is a flexible parallel computing library for analytics.\nIntegrates seamlessly with NumPy, Pandas, and Scikit-learn.\nUses a task graph paradigm to break down computations into smaller tasks.\nEnables parallel execution of tasks for scalable analytics.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask and Ray</span>"
    ]
  },
  {
    "objectID": "Courses/06_Dask-Ray.html#section",
    "href": "Courses/06_Dask-Ray.html#section",
    "title": "7  Dask and Ray",
    "section": "8.2 ",
    "text": "8.2 \n\n\n\n\nRay is a distributed execution framework for building and running distributed applications.\nSupports a wide range of workloads, including machine learning and reinforcement learning.\nUses an actor model paradigm for stateful computations.\nAllows dynamic task scheduling and fine-grained control over distributed execution.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask and Ray</span>"
    ]
  },
  {
    "objectID": "Courses/06_Dask-Ray.html#dask-task-graph-computing",
    "href": "Courses/06_Dask-Ray.html#dask-task-graph-computing",
    "title": "7  Dask and Ray",
    "section": "8.3 Dask: Task Graph Computing",
    "text": "8.3 Dask: Task Graph Computing\n\nimport numpy as np\nimport dask.array as da\n\ndata = np.arange(100_000).reshape(200, 500)\na = da.from_array(data, chunks=(100, 100))\na\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n781.25 kiB\n78.12 kiB\n\n\nShape\n(200, 500)\n(100, 100)\n\n\nDask graph\n10 chunks in 1 graph layer\n\n\nData type\nint64 numpy.ndarray\n\n\n\n\n              500 200\n\n\n\n\n\n\n\nmean_graph = a.mean()\nmean_graph.visualize()\n\n\n\n\n\n\n\n\n\nresult = mean_graph.compute()\nresult\n\nnp.float64(49999.5)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask and Ray</span>"
    ]
  },
  {
    "objectID": "Courses/06_Dask-Ray.html#rays-actor-model-example",
    "href": "Courses/06_Dask-Ray.html#rays-actor-model-example",
    "title": "7  Dask and Ray",
    "section": "8.4 Ray’s actor model example",
    "text": "8.4 Ray’s actor model example\n\nimport ray\n\n@ray.remote\nclass Counter:\n    def __init__(self):\n        self.value = 0\n\n    def increment(self):\n        self.value += 1\n        return self.value\n\n# Create actor instances\ncounters = [Counter.remote() for _ in range(4)]\n# Increment counters in parallel\nray.get([counter.increment.remote() for counter in counters])\n\n[1, 1, 1, 1]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask and Ray</span>"
    ]
  },
  {
    "objectID": "Courses/06_Dask-Ray.html#key-differences-dask-vs-ray",
    "href": "Courses/06_Dask-Ray.html#key-differences-dask-vs-ray",
    "title": "7  Dask and Ray",
    "section": "8.5 Key Differences Dask vs Ray",
    "text": "8.5 Key Differences Dask vs Ray\n\n\n\n\n\n\n\n\nFeature\nDask\nRay\n\n\n\n\nParadigm\nTask graph (functional)\nActor model (OOP)\n\n\nState\nStateless (pure functions)\nStateful (actor instances)\n\n\nScheduling\nStatic graph optimization\nDynamic task scheduling\n\n\nBest for\nArray/collection operations\nHeterogeneous workloads",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask and Ray</span>"
    ]
  },
  {
    "objectID": "Courses/06_Dask-Ray.html#practical-examples",
    "href": "Courses/06_Dask-Ray.html#practical-examples",
    "title": "7  Dask and Ray",
    "section": "8.6 Practical Examples",
    "text": "8.6 Practical Examples\n\n\n\n# Dask version (graph-based)\nimport dask.bag as db\n\nbag = db.read_text('large_file.txt')\nresult = bag.map(lambda x: x.upper()).compute()\n\n\n# Ray version (actor-based)\n@ray.remote\ndef process_line(line):\n    return line.upper()\n\nwith open('large_file.txt') as f:\n    lines = f.readlines()\n\nresults = ray.get([process_line.remote(line) for line in lines])",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask and Ray</span>"
    ]
  },
  {
    "objectID": "Courses/06_Dask-Ray.html#example-2-machine-learning",
    "href": "Courses/06_Dask-Ray.html#example-2-machine-learning",
    "title": "7  Dask and Ray",
    "section": "8.7 Example 2: Machine Learning",
    "text": "8.7 Example 2: Machine Learning\n\n\n\n# Dask-ML example\nfrom dask_ml.linear_model import LogisticRegression\nfrom dask_ml.datasets import make_classification\n\nX, y = make_classification(n_samples=100000, chunks=1000)\nclf = LogisticRegression()\nclf.fit(X, y)\n\n\n# Ray Train example\nfrom ray.train.xgboost import XGBoostTrainer\n\ntrainer = XGBoostTrainer(\n    label_column=\"target\",\n    params={\"objective\": \"binary:logistic\"},\n    datasets={\"train\": ray.data.from_pandas(df)},\n)\nresult = trainer.fit()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask and Ray</span>"
    ]
  },
  {
    "objectID": "Courses/06_Dask-Ray.html#performance-considerations",
    "href": "Courses/06_Dask-Ray.html#performance-considerations",
    "title": "7  Dask and Ray",
    "section": "8.8 Performance Considerations",
    "text": "8.8 Performance Considerations\n\n\n\nDask excels at:\n\n\nLarge array operations\nPredictable workloads\nNumpy/Pandas-like workflows\n\n\n\n\nRay excels at:\n\n\nHeterogeneous tasks\nStateful applications\nReinforcement learning",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask and Ray</span>"
    ]
  },
  {
    "objectID": "Courses/06_Dask-Ray.html#hybrid-approaches",
    "href": "Courses/06_Dask-Ray.html#hybrid-approaches",
    "title": "7  Dask and Ray",
    "section": "8.9 Hybrid Approaches",
    "text": "8.9 Hybrid Approaches\n# Using both together\nimport dask.dataframe as dd\nimport ray\n\n# Process data with Dask\nddf = dd.read_csv('large_dataset.csv')\nprocessed = ddf.groupby('category').mean()\n\n# Use Ray for model serving\n@ray.remote\nclass ModelServer:\n    def __init__(self, model):\n        self.model = model\n    def predict(self, data):\n        return self.model.predict(data)\n\n# Convert Dask results to Ray tasks\nmodel = train_model(processed.compute())\nservers = [ModelServer.remote(model) for _ in range(4)]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask and Ray</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html",
    "href": "Courses/07_Dask-delayed.html",
    "title": "8  Dask delayed",
    "section": "",
    "text": "9 Dask Delayed\nSometimes problems don't fit into one of the collections like dask.array or dask.dataframe. In these cases, users can parallelize custom algorithms using the simpler dask.delayed interface. This allows you to create graphs directly with a light annotation of normal python code:",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#dask-delayed-1",
    "href": "Courses/07_Dask-delayed.html#dask-delayed-1",
    "title": "8  Dask delayed",
    "section": "Dask Delayed",
    "text": "Dask Delayed\n\n\nA Dask Delayed task graph with two \"inc\" functions combined using an \"add\" function resulting in an output node.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#example",
    "href": "Courses/07_Dask-delayed.html#example",
    "title": "8  Dask delayed",
    "section": "9.1 Example",
    "text": "9.1 Example\nVisit https://examples.dask.org/delayed.html to see and run examples using Dask Delayed.\nSometimes we face problems that are parallelizable, but don't fit into high-level abstractions like Dask Array or Dask DataFrame. Consider the following example:\ndef inc(x):\n    return x + 1\n\ndef double(x):\n    return x * 2\n\ndef add(x, y):\n    return x + y\n\ndata = [1, 2, 3, 4, 5]\n\noutput = []\nfor x in data:\n    a = inc(x)\n    b = double(x)\n    c = add(a, b)\n    output.append(c)\n\ntotal = sum(output)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#example-1",
    "href": "Courses/07_Dask-delayed.html#example-1",
    "title": "8  Dask delayed",
    "section": "Example",
    "text": "Example\nThere is clearly parallelism in this problem (many of the inc, double, and add functions can be evaluated independently), but it's not clear how to convert this to an array or DataFrame computation. As written, this code runs sequentially in a single thread. However, we see that a lot of this could be executed in parallel.\nThe Dask delayed function decorates your functions so that they operate lazily. Rather than executing your function immediately, it will defer execution, placing the function and its arguments into a task graph.\n\ndask.delayed\n\n\ndelayed",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#example-2",
    "href": "Courses/07_Dask-delayed.html#example-2",
    "title": "8  Dask delayed",
    "section": "Example",
    "text": "Example\nWe slightly modify our code by wrapping functions in delayed. This delays the execution of the function and generates a Dask graph instead:\nimport dask\n\noutput = []\nfor x in data:\n    a = dask.delayed(inc)(x)\n    b = dask.delayed(double)(x)\n    c = dask.delayed(add)(a, b)\n    output.append(c)\n\ntotal = dask.delayed(sum)(output)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#example-3",
    "href": "Courses/07_Dask-delayed.html#example-3",
    "title": "8  Dask delayed",
    "section": "Example",
    "text": "Example\nWe used the dask.delayed function to wrap the function calls that we want to turn into tasks. None of the inc, double, add, or sum calls have happened yet. Instead, the object total is a Delayed result that contains a task graph of the entire computation. Looking at the graph we see clear opportunities for parallel execution. The Dask schedulers &lt;scheduling&gt; will exploit this parallelism, generally improving performance (although not in this example, because these functions are already very small and fast.)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#example-4",
    "href": "Courses/07_Dask-delayed.html#example-4",
    "title": "8  Dask delayed",
    "section": "Example",
    "text": "Example\ntotal.visualize()  # see image to the right\n\n\nA task graph with many nodes for \"inc\" and \"double\" that combine with \"add\" nodes. The output of the \"add\" nodes finally aggregate with a \"sum\" node.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#example-5",
    "href": "Courses/07_Dask-delayed.html#example-5",
    "title": "8  Dask delayed",
    "section": "Example",
    "text": "Example\nWe can now compute this lazy result to execute the graph in parallel:\n&gt;&gt;&gt; total.compute()\n45",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#decorator",
    "href": "Courses/07_Dask-delayed.html#decorator",
    "title": "8  Dask delayed",
    "section": "9.2 Decorator",
    "text": "9.2 Decorator\nIt is also common to see the delayed function used as a decorator. Here is a reproduction of our original problem as a parallel code:\nimport dask\n\n@dask.delayed\ndef inc(x):\n    return x + 1\n\n@dask.delayed\ndef double(x):\n    return x * 2\n\n@dask.delayed\ndef add(x, y):\n    return x + y\n\ndata = [1, 2, 3, 4, 5]\n\noutput = []\nfor x in data:\n    a = inc(x)\n    b = double(x)\n    c = add(a, b)\n    output.append(c)\n\ntotal = dask.delayed(sum)(output)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#real-time",
    "href": "Courses/07_Dask-delayed.html#real-time",
    "title": "8  Dask delayed",
    "section": "9.3 Real time",
    "text": "9.3 Real time\nSometimes you want to create and destroy work during execution, launch tasks from other tasks, etc. For this, see the Futures &lt;futures&gt; interface.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#best-practices",
    "href": "Courses/07_Dask-delayed.html#best-practices",
    "title": "8  Dask delayed",
    "section": "9.4 Best Practices",
    "text": "9.4 Best Practices\nFor a list of common problems and recommendations see Delayed Best Practices &lt;delayed-best-practices&gt;.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#indirect-dependencies",
    "href": "Courses/07_Dask-delayed.html#indirect-dependencies",
    "title": "8  Dask delayed",
    "section": "9.5 Indirect Dependencies",
    "text": "9.5 Indirect Dependencies\nSometimes you might find yourself wanting to add a dependency to a task that does not take the result of that dependency as an input. For example when a task depends on the side-effect of another task. In these cases you can use dask.graph_manipulation.bind.\nimport dask\nfrom dask.graph_manipulation import bind\n\nDATA = []\n\n@dask.delayed\ndef inc(x):\n    return x + 1\n\n@dask.delayed\ndef add_data(x):\n    DATA.append(x)\n\n@dask.delayed\ndef sum_data(x):\n    return sum(DATA) + x\n\na = inc(1)\nb = add_data(a)\nc = inc(3)\nd = add_data(c)\ne = inc(5)\nf = bind(sum_data, [b, d])(e)\nf.compute()\nsum_data will operate on DATA only after both the expected items have been appended to it. bind can also be used along with direct dependencies passed through the function arguments.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#call-delayed-on-the-function-not-the-result",
    "href": "Courses/07_Dask-delayed.html#call-delayed-on-the-function-not-the-result",
    "title": "8  Dask delayed",
    "section": "10.1 Call delayed on the function, not the result",
    "text": "10.1 Call delayed on the function, not the result\nDask delayed operates on functions like dask.delayed(f)(x, y), not on their results like dask.delayed(f(x, y)). When you do the latter, Python first calculates f(x, y) before Dask has a chance to step in.\n\n\n\n\n\n\n\nDon't\nDo\n\n\n# This executes immediately\n\ndask.delayed(f(x, y))\n# This ma\nkes a delayed function, acting lazily\n\ndask.delayed(f)(x, y)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#compute-on-lots-of-computation-at-once",
    "href": "Courses/07_Dask-delayed.html#compute-on-lots-of-computation-at-once",
    "title": "8  Dask delayed",
    "section": "10.2 Compute on lots of computation at once",
    "text": "10.2 Compute on lots of computation at once\nTo improve parallelism, you want to include lots of computation in each compute call. Ideally, you want to make many dask.delayed calls to define your computation and then call dask.compute only at the end. It is ok to call dask.compute in the middle of your computation as well, but everything will stop there as Dask computes those results before moving forward with your code.\n\n\n\n\n\n\n\nDon't\nDo\n\n\n# Avoid calling compute repeatedly\n\nresults = []\nfor x in L:\n    y = dask.delayed(f)(x)\n    results.append(y.compute())\n\nresults\n# Collec\nt many calls for one compute\n\nresults = []\nfor x in L:\n    y = dask.delayed(f)(x)\n    results.append(y)\n\nresu\nlts = dask.compute(*results)\n\n\n\nCalling y.compute() within the loop would await the result of the computation every time, and so inhibit parallelism.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#dont-mutate-inputs",
    "href": "Courses/07_Dask-delayed.html#dont-mutate-inputs",
    "title": "8  Dask delayed",
    "section": "10.3 Don't mutate inputs",
    "text": "10.3 Don't mutate inputs\nYour functions should not change the inputs directly.\n\n\n\n\n\n\n\nDon't\nDo\n\n\n# Mutate inputs in functions\n\n@dask.delayed\ndef f(x):\n    x += 1\n    return x\n# Return new values or copies\n\n@dask.delayed\ndef f(x):\n    x = x + 1\n    return x\n\n\n\nIf you need to use a mutable operation, then make a copy within your function first:\n@dask.delayed\ndef f(x):\n    x = copy(x)\n    x += 1\n    return x",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#avoid-global-state",
    "href": "Courses/07_Dask-delayed.html#avoid-global-state",
    "title": "8  Dask delayed",
    "section": "10.4 Avoid global state",
    "text": "10.4 Avoid global state\nIdeally, your operations shouldn't rely on global state. Using global state might work if you only use threads, but when you move to multiprocessing or distributed computing then you will likely encounter confusing errors.\n\n\n\n\n\n\nDon't\n\n\nL = []\n\n# This references global variable L\n\n@dask.delayed\ndef f(x):\n    L.append(x)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#dont-rely-on-side-effects",
    "href": "Courses/07_Dask-delayed.html#dont-rely-on-side-effects",
    "title": "8  Dask delayed",
    "section": "10.5 Don't rely on side effects",
    "text": "10.5 Don't rely on side effects\nDelayed functions only do something if they are computed. You will always need to pass the output to something that eventually calls compute.\n\n\n\n\n\n\n\nDon't\nDo\n\n\n# Forget to call compute\n\ndask.delayed(f)(1, 2, 3)\n\n...\n# Ensure delayed tasks are computed\n\nx = dask.delayed(f)(1, 2, 3)\n...\ndask.compute(x, ...)\n\n\n\nIn the first case here, nothing happens, because compute() is never called.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#break-up-computations-into-many-pieces",
    "href": "Courses/07_Dask-delayed.html#break-up-computations-into-many-pieces",
    "title": "8  Dask delayed",
    "section": "10.6 Break up computations into many pieces",
    "text": "10.6 Break up computations into many pieces\nEvery dask.delayed function call is a single operation from Dask's perspective. You achieve parallelism by having many delayed calls, not by using only a single one: Dask will not look inside a function decorated with @dask.delayed and parallelize that code internally. To accomplish that, it needs your help to find good places to break up a computation.\n\n\n\n\n\n\n\nDon't\nDo\n\n\n# One giant task\n\n\ndef load(filename):\n    ...\n\n\ndef process(data):\n    ...\n\n\ndef save(data):\n    ...\n\n@dask.delayed\ndef f(filenames):\n    results = []\n    for filename in filenames:\n        data = load(filename)\n        data = process(data)\n        result = save(data)\n        results.append(result)\n\n    return results\n\ndask.compute(f(filenames))\n# Break up into many tasks\n\n@dask.delayed\ndef load(filename):\n    ...\n\n@dask.delayed\ndef process(data):\n    ...\n\n@dask.delayed\ndef save(data):\n    ...\n\n\ndef f(filenames):\n    results = []\n    for filename in filenames:\n        data = load(filename)\n        data = process(data)\n        result = save(data)\n        results.append(result)\n\n    return results\n\ndask.compute(f(filenames))\n\n\n\nThe first version only has one delayed task, and so cannot parallelize.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#avoid-too-many-tasks",
    "href": "Courses/07_Dask-delayed.html#avoid-too-many-tasks",
    "title": "8  Dask delayed",
    "section": "10.7 Avoid too many tasks",
    "text": "10.7 Avoid too many tasks\nEvery delayed task has an overhead of a few hundred microseconds. Usually this is ok, but it can become a problem if you apply dask.delayed too finely. In this case, it's often best to break up your many tasks into batches or use one of the Dask collections to help you.\n\n\n\n\n\n\n\nDon't\nDo\n\n\n# Too many tasks\n\nresults = []\nfo\nr x in range(10000000):\n\n y = dask.delayed(f)(x)\n    results.append(y)\n# Use collections\n\nimport dask.bag as db\nb = db.from_s\nequence(range(10000000), npartitions=1000)\nb = b.map(f)\n...",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#avoid-too-many-tasks-1",
    "href": "Courses/07_Dask-delayed.html#avoid-too-many-tasks-1",
    "title": "8  Dask delayed",
    "section": "Avoid too many tasks",
    "text": "Avoid too many tasks\nHere we use dask.bag to automatically batch applying our function. We could also have constructed our own batching as follows\ndef batch(seq):\n    sub_results = []\n    for x in seq:\n        sub_results.append(f(x))\n    return sub_results\n\n batches = []\n for i in range(0, 10000000, 10000):\n     result_batch = dask.delayed(batch)(range(i, i + 10000))\n     batches.append(result_batch)\nHere we construct batches where each delayed function call computes for many data points from the original input.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#avoid-calling-delayed-within-delayed-functions",
    "href": "Courses/07_Dask-delayed.html#avoid-calling-delayed-within-delayed-functions",
    "title": "8  Dask delayed",
    "section": "10.8 Avoid calling delayed within delayed functions",
    "text": "10.8 Avoid calling delayed within delayed functions\nOften, if you are new to using Dask delayed, you place dask.delayed calls everywhere and hope for the best. While this may actually work, it's usually slow and results in hard-to-understand solutions.\nUsually you never call dask.delayed within dask.delayed functions.\n\n\n\n\n\n\n\nDon't\nDo\n\n\n# Delayed function calls delayed\n\n@dask.delayed\ndef process_all(L):\n    result = []\n    for x in L:\n        y = dask.delayed(f)(x)\n        result.append(y)\n    return result\n# Normal function calls delayed\n\n\ndef process_all(L):\n    result = []\n    for x in L:\n        y = dask.delayed(f)(x)\n        result.append(y)\n    return result\n\n\n\nBecause the normal function only does delayed work it is very fast and so there is no reason to delay it.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#dont-call-dask.delayed-on-other-dask-collections",
    "href": "Courses/07_Dask-delayed.html#dont-call-dask.delayed-on-other-dask-collections",
    "title": "8  Dask delayed",
    "section": "10.9 Don't call dask.delayed on other Dask collections",
    "text": "10.9 Don't call dask.delayed on other Dask collections\nWhen you place a Dask array or Dask DataFrame into a delayed call, that function will receive the NumPy or Pandas equivalent. Beware that if your array is large, then this might crash your workers.\nInstead, it's more common to use methods like da.map_blocks\n\n\n\n\n\n\n\nDon't\nDo\n\n\n# Call del\nayed functions on Dask collections\n\nimport dask.dataframe as dd\ndf = dd.read_csv('/path/to/*.csv')\n\ndask.delayed(train)(df)\n# Us\ne mapping methods if applicable\n\nimport dask.dataframe as dd\ndf\n= dd.read_csv('/path/to/*.csv')\n\ndf.map_partitions(train)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#dont-call-dask.delayed-on-other-dask-collections-1",
    "href": "Courses/07_Dask-delayed.html#dont-call-dask.delayed-on-other-dask-collections-1",
    "title": "8  Dask delayed",
    "section": "Don't call dask.delayed on other Dask collections",
    "text": "Don't call dask.delayed on other Dask collections\nAlternatively, if the procedure doesn't fit into a mapping, you can always turn your arrays or dataframes into many delayed objects, for example\npartitions = df.to_delayed()\ndelayed_values = [dask.delayed(train)(part)\n                  for part in partitions]\nHowever, if you don't mind turning your Dask array/DataFrame into a single chunk, then this is ok.\ndask.delayed(train)(..., y=df.sum())",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#avoid-repeatedly-putting-large-inputs-into-delayed-calls",
    "href": "Courses/07_Dask-delayed.html#avoid-repeatedly-putting-large-inputs-into-delayed-calls",
    "title": "8  Dask delayed",
    "section": "10.10 Avoid repeatedly putting large inputs into delayed calls",
    "text": "10.10 Avoid repeatedly putting large inputs into delayed calls\nEvery time you pass a concrete result (anything that isn't delayed) Dask will hash it by default to give it a name. This is fairly fast (around 500 MB/s) but can be slow if you do it over and over again. Instead, it is better to delay your data as well.\nThis is especially important when using a distributed cluster to avoid sending your data separately for each function call.\n\n\n\n\n\n\n\nDon't\nDo\n\n\nx = np.arr\nay(...)  # some large array\n\nresults =\n [dask.delayed(train)(x, i)\n\n      for i in range(1000)]\nx\n = np.array(...)    # some large array\nx =\ndask.delayed(x)  # delay the data once\nresults = [dask.delayed(train)(x, i)\n           for i in range(1000)]\n\n\n\nEvery call to dask.delayed(train)(x, ...) has to hash the NumPy array x, which slows things down.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#working-with-collections-1",
    "href": "Courses/07_Dask-delayed.html#working-with-collections-1",
    "title": "8  Dask delayed",
    "section": "Working with Collections",
    "text": "Working with Collections\nAs an example, consider the case where we store tabular data in a custom format not known by Dask DataFrame. This format is naturally broken apart into pieces and we have a function that reads one piece into a Pandas DataFrame. We use dask.delayed to lazily read these files into Pandas DataFrames, use dd.from_delayed to wrap these pieces up into a single Dask DataFrame, use the complex algorithms within the DataFrame (groupby, join, etc.), and then switch back to dask.delayed to save our results back to the custom format:",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#working-with-collections-2",
    "href": "Courses/07_Dask-delayed.html#working-with-collections-2",
    "title": "8  Dask delayed",
    "section": "Working with Collections",
    "text": "Working with Collections\nimport dask.dataframe as dd\nfrom dask.delayed import delayed\n\nfrom my_custom_library import load, save\n\nfilenames = ...\ndfs = [delayed(load)(fn) for fn in filenames]\n\ndf = dd.from_delayed(dfs)\ndf = ... # do work with dask.dataframe\n\ndfs = df.to_delayed()\nwrites = [delayed(save)(df, fn) for df, fn in zip(dfs, filenames)]\n\ndd.compute(*writes)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#working-with-collections-3",
    "href": "Courses/07_Dask-delayed.html#working-with-collections-3",
    "title": "8  Dask delayed",
    "section": "Working with Collections",
    "text": "Working with Collections\nData science is often complex, and dask.delayed provides a release valve for users to manage this complexity on their own, and solve the last mile problem for custom formats and complex situations.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/08_SIMD.html",
    "href": "Courses/08_SIMD.html",
    "title": "9  Hardware Vectorization with SIMD",
    "section": "",
    "text": "10 Generalities on SIMD",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/08_SIMD.html#why-vectorization",
    "href": "Courses/08_SIMD.html#why-vectorization",
    "title": "9  Hardware Vectorization with SIMD",
    "section": "10.1 Why vectorization ?",
    "text": "10.1 Why vectorization ?\n\n\nUbiquitousness\n\nAlmost every CPU made on the market since 2010 got a SIMD unit and it operates on multiple elements at the same time on a single instruction.\n\nNatural congruence with parallel progamming\n\nIn a good number of cases, parallelization of algorithm is often achieved by vectorization and the use of SIMD is then a free bonus.\n\nPerformance boost almost guaranteed\n\nWith a little effort it could boost your performance by a factor of two or more. It is even more energy efficient than raw CPU (non SIMD) computing(Inoue 2016).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/08_SIMD.html#scalar-vs-vector-operation",
    "href": "Courses/08_SIMD.html#scalar-vs-vector-operation",
    "title": "9  Hardware Vectorization with SIMD",
    "section": "10.2 Scalar vs Vector operation",
    "text": "10.2 Scalar vs Vector operation\n\n\n\n\n\nA scalar operation does a single double-precision addition in one cycle. It takes eight cycles to process a 64-byte cache line. In comparison, a vector operation on a 512-bit vector unit can process all eight double-precision values in one cycle.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/08_SIMD.html#terminology",
    "href": "Courses/08_SIMD.html#terminology",
    "title": "9  Hardware Vectorization with SIMD",
    "section": "10.3 Terminology",
    "text": "10.3 Terminology\n\nVector (SIMD) lane\n\nA pathway through a vector operation on vector registers for a single data element much like a lane on a multi-lane freeway.\n\nVector width\n\nThe width of the vector unit, usually expressed in bits.\n\nVector length\n\nThe number of data elements that can be processed by the vector in one operation.\n\nVector (SIMD) instruction sets\n\nThe set of instructions that extend the regular scalar processor instructions to utilize the vector processor.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/08_SIMD.html#hardwaresoftware-requirements",
    "href": "Courses/08_SIMD.html#hardwaresoftware-requirements",
    "title": "9  Hardware Vectorization with SIMD",
    "section": "10.4 Hardware/Software Requirements",
    "text": "10.4 Hardware/Software Requirements\n\nGeneration of instructions\n\nvector instructions are generated by the compiler OR manually by the programmer via the “intrisics” (explicit SIMD instructions)\n\nMatching of instructions\n\nmatching of the instructions and the hardware, because there is several units and instructions sets. (the compiler does the matching, most of the time).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/08_SIMD.html#evolution-of-the-vector-length",
    "href": "Courses/08_SIMD.html#evolution-of-the-vector-length",
    "title": "9  Hardware Vectorization with SIMD",
    "section": "11.1 Evolution of the vector length",
    "text": "11.1 Evolution of the vector length",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/08_SIMD.html#versions-i",
    "href": "Courses/08_SIMD.html#versions-i",
    "title": "9  Hardware Vectorization with SIMD",
    "section": "11.2 Versions, I",
    "text": "11.2 Versions, I\n\n\n\nRelease\nFunctionality\n\n\n\n\nMMX (trademark with no official meaning)\nTargeted towards the graphics market, but GPUs soon took over this function. Vector units shifted their focus to computation rather than graphics. AMD released its version under the name 3DNow! with single-precision support.\n\n\nSSE (Streaming SIMD Extensions)\nFirst Intel vector unit to offer floating-point operations with single-precision support\n\n\nSSE2\nDouble-precision support added",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/08_SIMD.html#versions-ii",
    "href": "Courses/08_SIMD.html#versions-ii",
    "title": "9  Hardware Vectorization with SIMD",
    "section": "11.3 Versions, II",
    "text": "11.3 Versions, II\n\n\n\nRelease\nFunctionality\n\n\n\n\nAVX (Advanced Vector Extensions)\nTwice the vector length. AMD added a fused multiply-add FMA vector instruction in its competing hardware, effectively doubling the performance for some loops.\n\n\nAVX2\nIntel added a fused multiply-add (FMA) to its vector processor.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/08_SIMD.html#versions-iii",
    "href": "Courses/08_SIMD.html#versions-iii",
    "title": "9  Hardware Vectorization with SIMD",
    "section": "11.4 Versions, III",
    "text": "11.4 Versions, III\n\n\n\nRelease\nFunctionality\n\n\n\n\nAVX512\nFirst offered on the Knights Landing processor; it came to the main-line multi-core processor hardware lineup in 2017.From the years 2018 and on, Intel and AMD (Advanced Micro Devices, Inc.) have created multiple variants of AVX512 as incremental improvements to vector hardware architectures.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/08_SIMD.html#vectors-types-avx",
    "href": "Courses/08_SIMD.html#vectors-types-avx",
    "title": "9  Hardware Vectorization with SIMD",
    "section": "12.1 Vectors types (AVX)",
    "text": "12.1 Vectors types (AVX)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/08_SIMD.html#encoding-scheme",
    "href": "Courses/08_SIMD.html#encoding-scheme",
    "title": "9  Hardware Vectorization with SIMD",
    "section": "12.2 Encoding scheme",
    "text": "12.2 Encoding scheme\n_mm256_{operation}{non-alignement}_{dataorganization}{datatype}",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/08_SIMD.html#add-example",
    "href": "Courses/08_SIMD.html#add-example",
    "title": "9  Hardware Vectorization with SIMD",
    "section": "12.3 Add example",
    "text": "12.3 Add example",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/08_SIMD.html#various-i",
    "href": "Courses/08_SIMD.html#various-i",
    "title": "9  Hardware Vectorization with SIMD",
    "section": "12.4 Various, I",
    "text": "12.4 Various, I\n\ns (single): single precision float (32bits)\nd (double): double precision float (64bits)\ni… (integer): integer\np (packed): contiguous, operates on the whole vector\ns (scalar): operates on a single element",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/08_SIMD.html#various-ii",
    "href": "Courses/08_SIMD.html#various-ii",
    "title": "9  Hardware Vectorization with SIMD",
    "section": "12.5 Various, II",
    "text": "12.5 Various, II\n\nu (unaligned): data non aligned in memory\nl (low): least significant bits\nh (high): most significant bits\nr (reversed): reversed order",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "10  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Fowler, M. 2022. Python Concurrency with Asyncio. Manning. https://www.manning.com/books/python-concurrency-with-asyncio.\n\n\nInoue, Hiroshi. 2016. “How SIMD Width Affects Energy Efficiency: A\nCase Study on Sorting.” In 2016 IEEE Symposium in Low-Power\nand High-Speed Chips (COOL CHIPS XIX), 1–3. IEEE.\n\n\nRobey, R., and Y. Zamora. 2021. Parallel and High Performance\nComputing. Manning. https://www.manning.com/books/parallel-and-high-performance-computing.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.\n“Attention Is All You Need.” Advances in Neural\nInformation Processing Systems 30.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "Projects/list.html",
    "href": "Projects/list.html",
    "title": "Appendix A — List of projects",
    "section": "",
    "text": "A.1 General Indications\nThis is a list of project ideas, some are of different difficulty levels, and some are cross-over projects that combine multiple subjects of the course.\nThose projects should be done alone or in pairs. The deliverables are also indicative, you can propose something else if you think it is more appropriate.\nAs one say “It’s not about the destination, it’s about the journey”, so the goal is to learn, and to showcase what you have learned.\nYou need to demonstrate intellectual curiosity, to dig into the subject, search for related work, and to understand what is already done in the field of the project you choose. Sometimes there is already prior work, you need to understand it, and to build on top of it if it is relevant.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>List of projects</span>"
    ]
  },
  {
    "objectID": "Projects/list.html#general-indications",
    "href": "Projects/list.html#general-indications",
    "title": "Appendix A — List of projects",
    "section": "",
    "text": "Important\n\n\n\nDon’t hesitate to propose your own project idea if you have something else in mind that is not listed here!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>List of projects</span>"
    ]
  },
  {
    "objectID": "Projects/list.html#terminology",
    "href": "Projects/list.html#terminology",
    "title": "Appendix A — List of projects",
    "section": "A.2 Terminology",
    "text": "A.2 Terminology\n\nA.2.1 ETL\nExtract, transform, load\n\n\n\n\n\n\n\nA.2.2 HPO\nHyperparameter optimization\n\n\n\n\n\n\n\nA.2.3 Deep Research\n\nDeep Research is composed of an LLM (which can be selected from the current list of LLMs provided by OpenAI, 4o, o1, o3, etc) and an internal “agentic framework” which guide the LLM to use tools like web search and organize its actions in steps.\n\nHere, we may broaden the “Deep Research” domain of application to any type of search like codebase extraction (instead of web search).\nSee Building Deep Research from scratch\n\n\n\n\n\n\n\nA.2.4 Work-Stealing\nWork-stealing is a scheduling strategy for parallel computing where idle processors dynamically “steal” tasks from busy processors to balance the workload and improve efficiency.\n\n\n\n\n\n\n\nA.2.5 OLAP (Online Analytical Processing)\nOnline Analytical Processing is a category of software technology that enables analysts, managers, and executives to gain insight into data through fast, consistent, interactive access in a variety of ways.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>List of projects</span>"
    ]
  },
  {
    "objectID": "Projects/list.html#code-assistants",
    "href": "Projects/list.html#code-assistants",
    "title": "Appendix A — List of projects",
    "section": "A.3 Code Assistants",
    "text": "A.3 Code Assistants\n\n(1) Deep research assistant for creating a README for a codebase (pipeline version)\n\n\n\n\nDescription\n\nBuild an assistant that analyzes a codebase, infers project structure, and generates a high‑quality README with a summary, usage and architecture details.\n\nDeliverables\n\nCLI/Notebook that ingests a repo and outputs a README.md file.\n\n\n\n\n\n\n\n\n\n\n(2) Deep research assistant for creating a README for a codebase (agentic/mcp version in continue.dev)\n\n\n\n\nDescription\n\nBuild an agentic/mcp assistant that analyzes a codebase, infers project structure, and generates a high‑quality README with a summary, usage and architecture details.\n\nDeliverables\n\na set of LLM setup with mcp servers and promopts in vscode continue extension that can generate a README.md file for a codebase.\n\n\n\n\n\n\n\n\n\n\n(3) Generating docstrings for Python code in a codebase\n\n\n\n\nDescription\n\nAuto-generate docstrings for functions and classes, then implement automated checks and tests that validate docstring correctness against function behavior.\n\nDeliverables\n\ntools that annotates Python code with docstrings, and a test suite that checks docstring claims (examples/expected behavior)\n\n\n\n\n\n\n\n\n\n\n(4) Code summarization/explanation and modularization suggestions for long Python scripts\n\n\n\n\nDescription\n\nCreate a tool that parses very long Python scripts and outputs modularization suggestions (module boundaries, function extraction) and a summary with refactor hints.\n\nDeliverables\n\nTool that takes a long Python script and outputs a summary and modularization suggestions (mcp server)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples of long scripts\n\n\n\n\nhttps://github.com/yt-dlp/yt-dlp/blob/master/yt_dlp/YoutubeDL.py\nhttps://github.com/aws/aws-cli/blob/develop/awscli/clidriver.py\nhttps://github.com/sqlalchemy/sqlalchemy/blob/main/lib/sqlalchemy/orm/session.py\nhttps://github.com/pallets/jinja/blob/main/src/jinja2/environment.py\nhttps://github.com/ipython/ipython/blob/main/IPython/core/interactiveshell.py\n\n\n\n\n\n(5) Pipeline for generating D2lang diagrams with a vision LLM\n\n\n\n\nDescription\n\nPipeline that takes a prompt, translates/templates it for a RAG query over the D2lang documentation, and generates a D2 diagram, compiles it to SVG/PNG, analyzes it for correctness and completeness, and iterates to improve it.\n\nDeliverables\n\nGennerate tool that takes a prompt and outputs a D2lang diagram over multiple iterations.\n\n\n\n\n\n\n\n\nD2 Documentation\n\n\n\n\n\n\nTip\n\n\n\nmistral is a vision LLM.\n\n\n\n\nA.3.1 Cross-over : Code Assistants + Numba or Dask\n\n(6) Refactoring bottleneck functions in a codebase to use Numba both with benchmarks, profiling, and tests.\n\n\n\n\nDescription\n\nIdentify hotspots, refactor to Numba, and quantify speedup/accuracy tradeoffs with tests and profiles.\n\nDeliverables\n\nOriginal vs optimized implementations, benchmark script, profiling report, tests verifying numerical parity. Clear reporting of reproducible use of LLMS/Deep Research for codebase understanding and refactoring and reproducibility.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples of codebases\n\n\n\n\nscikit-learn/scikit-learn — sklearn/manifold/_locally_linear.py — _locally_linear_embedding()\npandas-dev/pandas — pandas/core/window/common.py — flex_binary_moment()\nultralytics/yolov5 — train.py — train()\nPaddlePaddle/PaddleOCR\nultralytics/yolov5 — utils/plots.py, utils/segment/plots.py\nmdobook/resources - exercises/tenbartruss/truss.py\n\n\n\n\n\n\n\n\n\nExamples of numba uses\n\n\n\npandas/core/_numba/kernels/* — sliding_mean/var/sum/min_max\n\n\n\n\n(7) Translating PySpark pipelines to Dask — practical migration guide + AI Code Assistant.\n\n\n\n\nDescription\n\nPort small-to-medium PySpark ETL pipelines to Dask, preserve semantics, benchmark PySpark vs Dask, and provide an AI assistant that suggests rewrites, generates patches/tests, and explains tuning/semantic differences.\n\nDeliverables\n\nPySpark examples, Dask reimplementations, benchmark notebook, migration checklist, assistant (mapping rules + prompt templates + patch/test generator), and evaluation report.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>List of projects</span>"
    ]
  },
  {
    "objectID": "Projects/list.html#parallel-computing",
    "href": "Projects/list.html#parallel-computing",
    "title": "Appendix A — List of projects",
    "section": "A.4 Parallel Computing",
    "text": "A.4 Parallel Computing\n\n(8) Locking Strategy Lab: coarse vs fine vs optimistic vs RW locks\n\n\n\n\nDescription\n\nImplement and compare multiple locking strategies on a shared structure (e.g., graph or ledger).\n\nDeliverables\n\nImplementations, microbench harness, plots comparing throughput vs contention, write-up with recommendations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStrategies to consider\n\n\n\nCoarse grain: unique global Lock protecting whole structure Fine grain: sharded locks or per-node locks Reader‑Writer lock: shared readers, exclusive writers Optimistic read (versioning / retry): read w/o lock, validate version/cas (Optionnel) Lock‑free / CAS implementations in lower level language\n\n\n\n\n\n\n\n\nLinks\n\n\n\n\nGeneral background on locks and trade‑offs.\nTheory and common variants.\nClassical OCC discussion.\nPython threading locks.\nPython atomicts\n\n\n\n\n\n(9) Multi‑stage Image Pipeline with Backpressure & Flow Control\n\n\n\n\nDescription\n\nImplement a 3+ stage image processing pipeline with bounded queues, backpressure propagation, and dynamic scaling heuristics.\n\nDeliverables\n\nPipeline code, scaling heuristics, benchmark, short UI or logging dashboard.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTypical pipelines\n\n\n\n\nGeneric supervised CV pipeline (classification/recognition)\nOCR pipeline (detection + recognition)\n\n\n\n\n\n(10) Adaptive Concurrent Downloader with Politeness & Rate‑Limits (e.g., Google Scholar scraping or LLM API application)\n\n\n\n\nDescription\n\nImplement polite, adaptive downloader that enforces per-domain politeness, adapts concurrency to observed latency and failures, supports cancellation/priorities. Application to recursive scraping in google scholar or LLM API calls with rate limits.\n\nDeliverables\n\nCLI downloader, simulator of remote servers, policies and tests with real-world scraping or API calls example.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExemple of polite scraper\n\n\n\nhttps://github.com/dmi3kno/polite\n\n\n\n\n(11) Filesystem Work‑Stealing Crawler (toy distributed scheduler)\n\n\n\n\nDescription\n\nParallel filesystem walker and worker pool using work-stealing deques to deal with skewed directories.\n\nDeliverables\n\nCrawler implementation, skewed tree generator, benchmark and analysis.\n\n\n\n\n\n\n\n\n\n\nA.4.1 Dask/Ray based\n\n(12) Ray Tune: distributed HPO with comparison of strategies\n\n\n\n\nDescription\n\nImplement reproducible HPO experiments comparing grid/random/ASHA on a model and present best practices for local vs distributed runs.\n\nDeliverables\n\nExperiment suite, plots, reproducible configs with\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nRay Tune\n\n\n\n\n(13) ETL dask/ray pipeline + Front‑end (Streamlit/Panel/React)\n\n\n\n\nDescription\n\nBuild an ETL that scales locally with Dask or Ray and expose controls/visualization through a simple front-end.\n\nDeliverables\n\nETL code + front-end, README, sample dataset and demo video.\n\n\n\n\n\n\n\n\n\n\n(14) Synthetic Dask DAGs generator for evaluating dask task fusion and scheduling strategies.\n\n\n\n\nDescription\n\nCreate a synthetic DAG generator that can vary task sizes, dependencies, and fusion parameters to evaluate scheduler behavior.\n\nDeliverables\n\nGenerator, benchmark harness, plots and scheduler tuning recommendations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nOfficial Dask benchmarks\nDask Graphs\nDask Transformations\n\n\n\n\n\n(15) Streaming source + sliding window aggregation + dashboard (air quality / financial / river monitoring)\n\n\n\n\nDescription\n\nBuild a streaming pipeline using a local source (or Kafka emulator) performing sliding-window aggregations with a Streamlit/Dash dashboard. Apply to air quality, financial data, river monitoring, etc.\n\nDeliverables\n\nStreaming app, dashboard, reproducible synthetic generator, evaluation of latency/accuracy.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>List of projects</span>"
    ]
  },
  {
    "objectID": "Projects/list.html#webgpu",
    "href": "Projects/list.html#webgpu",
    "title": "Appendix A — List of projects",
    "section": "A.5 WebGPU",
    "text": "A.5 WebGPU\n\n(16) Proof of concept for using WebGPU for Olap queries in the browser\n\n\n\n\nDescription\n\nDemonstrate how WebGPU can accelerate in‑browser OLAP operations (filters/group-bys/aggregates) on columnar data.\n\nDeliverables\n\nIn-browser demo, short perf report.\n\n\n\n\n\n\n\n\n\n\n(17) WebGPU app for counting people in a video stream (browser)\n\n\n\n\nDescription\n\nImplement a browser-based (WebGPU-accelerated) pipeline that performs simple frame-level processing to count people (using a small model or heuristic).\n\nDeliverables\n\nWeb demo\n\n\n\n\n\n\n\n\n\n\n(18) WebGPU path-tracing\n\n\n\n\nDescription\n\nhttps://github.com/ferminLR/webgpu-path-tracing \\Rightarrow Implement one or more of the TODO items in the referenced path-tracing repo: advanced shader/task mapping and performance tuning etc.\n\nDeliverables\n\nPR to the repo with one or more completed TODO items.\n\n\n\n\n\n\n\n\n\n\nA.5.1 Crossover : Webgpu + LLMs\n\n(19) WebGPU/WebLLM RAG app for document/website Q&A in browser\n\n\n\n\nDescription\n\nImplement Retrieval-Augmented-Generation entirely in the browser: use WebGPU for vector similarity (ANN) and a small local WebLLM. Application to document or website Q&A.\n\nDeliverables\n\nWeb demo with interface to upload a document or URL and ask questions.\n\n\n\n\n\n\n\n\n\n\n\nA.5.2 Crossover : WebGPU + Dask\n\n(20) ETL + WebGPU visualization dashboard with Dask and Streamlit\n\n\n\n\nDescription\n\nRun ETL at scale locally with Dask and visualize large aggregated results in-browser using WebGPU for rendering (or via Streamlit embedding).\n\nDeliverables\n\nDask ETL pipeline, frontend WebGPU visualizer, fallback renderer, performance report.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>List of projects</span>"
    ]
  },
  {
    "objectID": "applications.html",
    "href": "applications.html",
    "title": "Appendix B — Applications",
    "section": "",
    "text": "Original (In Percent Format)\nOnline Html (Corrected)\nNotebook (Corrected)\n\n\n\n\nPrompt engineering exercises\nSolution\nNotebook\n\n\nAI Agent\nSolution\nNotebook\n\n\nNumpy Workout\nSolution\nNotebook\n\n\nDecorators Tutorial\nSolution\nNotebook\n\n\nA tutorial on Python generators\nSolution\nNotebook\n\n\nMultiprocessing in Python 3\nSolution\nNotebook\n\n\nScaling App with multiprocessing\n\n\n\n\nAn asyncio application\n\n\n\n\nIPC and Locking\n\n\n\n\nLocking with multiprocessing.Value\n\n\n\n\nDistributed models examples\n\n\n\n\nDask delayed App\n\n\n\n\nNumba Introduction\nSolution\nNotebook\n\n\nNumba first steps\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Applications</span>"
    ]
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Appendix C — Slides in reveal.js",
    "section": "",
    "text": "Code Assistants\nNumpy Workout\nIntroduction to parallel computing\nAsynchronous Programming with Python\nIPC and locking\nDistributed Computing models\nDask and Ray\nDask delayed\nHardware Vectorization with SIMD\n\nNo matching items",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Slides in reveal.js</span>"
    ]
  }
]