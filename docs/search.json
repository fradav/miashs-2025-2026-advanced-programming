[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced programming and parallel computing",
    "section": "",
    "text": "Preface\nThis is the course and materials for the lecture on “Advanced programming and parallel computing” at the Paul Valery University of Montpellier, France.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Software Prerequisites",
    "section": "",
    "text": "1.1 Software required\nYou will need to install the following software on your computer: Visual Studio Code (VSCode), a free and open-source code editor. You’ll have to install the following extensions:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#software-required",
    "href": "intro.html#software-required",
    "title": "1  Software Prerequisites",
    "section": "",
    "text": "Python extension to have everything you need to work with Python.\n Live Share to enable collaborative editing.\n Continue.dev to have the AI Code Assistant we will use as example in this course.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#ai-assistant-getting-api-key.",
    "href": "intro.html#ai-assistant-getting-api-key.",
    "title": "1  Software Prerequisites",
    "section": "1.2 AI Assistant, getting API Key.",
    "text": "1.2 AI Assistant, getting API Key.\nFor the practical work, you’ll need to get API Keys from Mistral.ai’s “La Plateforme” (it’s completely free). You will need a valid cell phone number for the registration to work. Contact me if this is a problem.\n\n\nMistral Login\n\n\n\nThe first time, you’ll have to create an account. Then, you’ll be able to get your API Key.\nOnce there, Click on “API Keys” tab.\n\n\nMistral API Keys Tab\n\n\n\nClick on “Choose a plan”.\n\n\nNo plan\n\n\n\nChoose “Experiment” plan.\n\n\nExperiment plan\n\n\n\nAccept the conditions.\n\n\nAccept\n\n\n\nGive a phone number for the final check.\n\n\nPhone number check\n\n\n\nConfirm the code.\n\n\nCode received\n\n\n\nIf successful, return on the “AI Keys” Tab and choose “Create a new key”\n\n\nCreate a new key\n\n\n\nChoose a name and an expiration date for your key (could be never if not set).\n\n\nKey name and expiration date\n\n\n\nCopy the key and save it somewhere.\n\n\nKey to copy\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou may also create a specific key for Codestral model, which could be used for auto-completion role. Auto-completion role needs specifically tailored models for this task, and the models you can access with the “generic” mistral key aren’t.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#vscode-setup",
    "href": "intro.html#vscode-setup",
    "title": "1  Software Prerequisites",
    "section": "1.3 VSCode setup",
    "text": "1.3 VSCode setup\nWhen you installed your continue vscode extension, it created a .continue folder in your home directory, which is ~/.continue on Linux and Mac, and %USERPROFILE%\\.continue on Windows. Create a .env file there and put your mistral key in it, like this:\nMISTRAL_API_KEY=your_key_here\nOptionally if you got a codestral key put it too:\nCODESTRAL_API_KEY=your_codestral_key_here\nOpen/Create config.yaml file in the same folder and put this in it:\nname: miashs/mistral\nversion: 1.0.0\nschema: v1\nmodels:\n## Uncomment this block if you want to use Codestral for auto-completion, needs a specific key\n#   - name: Codestral\n#     provider: mistral\n#     model: codestral-2508\n#     apiBase: https://codestral.mistral.ai/v1\n#     apiKey: ${{ secrets.CODESTRAL_API_KEY }}\n#     roles:\n#       - autocomplete\n#     defaultCompletionOptions:\n#       contextLength: 256000\n  - name: Devstral\n    provider: mistral\n    model: devstral-medium-latest\n    apiKey: ${{ secrets.MISTRAL_API_KEY }}\n    roles:\n      - chat\n      - edit\n      - apply\n    defaultCompletionOptions:\n      contextLength: 131072\n    capabilities:\n      - tool_use\n# You may choose mistral over devstral if you need image input\n  - name: Mistral\n    provider: mistral\n    model: mistral-medium-latest\n    apiKey: ${{ secrets.MISTRAL_API_KEY }}\n    roles:\n      - chat\n      - edit\n      - apply\n    defaultCompletionOptions:\n      contextLength: 131072\n    capabilities:\n      - tool_use\n      - image_input\n  - name: Codestral Embed\n    provider: mistral\n    model: codestral-embed\n    apiKey: ${{ secrets.MISTRAL_API_KEY }}\n    apiBase: https://api.mistral.ai/v1\n    roles:\n      - embed\ncontext:\n  - provider: code\n  - provider: docs\n    params:\n      maxdepth: 5\n  - provider: diff\n  - provider: terminal\n  - provider: problems\n  - provider: codebase\n    params:\n      nRetrieve: 60\n      nFinal: 25\n  - provider: folder\n    params:\n      nRetrieve: 60\n      nFinal: 25\n  - provider: open\n  - provider: web\n  - provider: tree\n  - provider: clipboard\n  - provider: debugger\n  - provider: repo-map\n  - provider: os\n  - provider: search\n  - provider: url\nCongratulations, you are now set for use of continue AI Code Assistant. You can open the chat panel either by clicking on the Continue in the right bottom status of vscode window or with Cmd + L (Mac) or Ctrl + L (Windows/Linux).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#collaborative-editing",
    "href": "intro.html#collaborative-editing",
    "title": "1  Software Prerequisites",
    "section": "1.4 Collaborative editing",
    "text": "1.4 Collaborative editing\n\nIn the discord channel, I’ll provide you a link to join a collaborative editing session. Don’t click on it, just copy it: \nThen open a new “blank” window in VSCode, which will be exclusively for collaborative session. \nThen, click on the “Live Share” button in the bottom left corner of the window \nClick on the “Join” button \nEither choose anonymous or sign in with your github/microsoft account \n\n\n\n\n\n\n\nAnonymous Guest Name\n\n\n\nIf you choose to sign in, you’ll have to authorize VSCode to access your github/microsoft account. If you choose anonymous, you’ll have to choose a username. Please choose a username that is easily identifiable as yours.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#installing-uv",
    "href": "intro.html#installing-uv",
    "title": "1  Software Prerequisites",
    "section": "2.1 Installing uv",
    "text": "2.1 Installing uv\nSee uv installation for your platform.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#creating-a-new-environment",
    "href": "intro.html#creating-a-new-environment",
    "title": "1  Software Prerequisites",
    "section": "2.2 Creating a new environment",
    "text": "2.2 Creating a new environment\nTo create a new environment, use the following command:\nuv init\nThis will create a new environment within the current directory.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#adding-packages",
    "href": "intro.html#adding-packages",
    "title": "1  Software Prerequisites",
    "section": "2.3 Adding packages",
    "text": "2.3 Adding packages\nTo add packages to your environment, use the following command:\nuv add package_name\nReplace package_name with the name of the package you want to install. This will install the package in your current environment.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#running-a-script-online-with-just-a-dependency",
    "href": "intro.html#running-a-script-online-with-just-a-dependency",
    "title": "1  Software Prerequisites",
    "section": "2.4 running a script online with just a dependency",
    "text": "2.4 running a script online with just a dependency\nuv run --with dependency script.py\nThis will execute the script.py file using just the specified dependency as the environment.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#running-script-with-a-directory",
    "href": "intro.html#running-script-with-a-directory",
    "title": "1  Software Prerequisites",
    "section": "2.5 running script with a directory",
    "text": "2.5 running script with a directory\nuv run --directory dir_env script.py\nThis will execute the script.py file using the specified directory as the working directory.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#macos",
    "href": "intro.html#macos",
    "title": "1  Software Prerequisites",
    "section": "3.1 MacOS",
    "text": "3.1 MacOS\nAdd this to you vscode settings.json\n    \"jupyter.runStartupCommands\": [\n        \"%colors nocolor\",\n        \"from multiprocessing import spawn\",\n        \"from multiprocessing.spawn import get_preparation_data as __get_preparation_data\",\n        \"def __patched_get_preparation_data(name):\",\n        \"    import sys\",\n        \"    main_mod = sys.modules['__main__']\",\n        \"    main_path = getattr(main_mod, '__file__', None)\",\n        \"    setattr(main_mod, '__file__', None)\",\n        \"    data = __get_preparation_data(name)\",\n        \"    setattr(main_mod, '__file__', main_path)\",\n        \"    return data\",\n        \"spawn.get_preparation_data = __patched_get_preparation_data\",\n        \"del spawn\",\n        \"import sys\",\n        \"from multiprocessing.reduction import ForkingPickler\",\n        \"from types import FunctionType\",\n        \"import cloudpickle\",\n        \"\",\n        \"def reducer_override(obj):\",\n        \"    if type(obj) is FunctionType:\",\n        \"        return (cloudpickle.loads, (cloudpickle.dumps(obj),))\",\n        \"    else:\",\n        \"        return NotImplemented\",\n        \"\",\n        \"ForkingPickler.reducer_override = staticmethod(reducer_override)\"\n    ],",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Prerequisites</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html",
    "href": "Courses/01_Code-Assistant.html",
    "title": "2  Code Assistants",
    "section": "",
    "text": "3 History of code editors/assistants\nHistory of code editor features, with a focus on the last three years (2022–2025) and the transformative impact of Large Language Models (LLMs):",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#early-days-text-editors",
    "href": "Courses/01_Code-Assistant.html#early-days-text-editors",
    "title": "2  Code Assistants",
    "section": "3.1 Early Days: Text Editors",
    "text": "3.1 Early Days: Text Editors\n\n1960s–1970s: vi (1976), Emacs (1976)\n\nBasic text manipulation, macros, and syntax highlighting.\n\n\n\n\n\nVi\n\n\n\n\n\n\n\nEmacs",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#the-rise-of-ides",
    "href": "Courses/01_Code-Assistant.html#the-rise-of-ides",
    "title": "2  Code Assistants",
    "section": "3.2 The Rise of IDEs",
    "text": "3.2 The Rise of IDEs\n\n\n\n1980s–1990s: Turbo Pascal (1983), Visual Basic (1991)\n\nIntegrated debugging\nproject management\nbasic autocompletion.\n\n\n\n\n\nTurbo Pascal\n\n\n\n\n\nVisual Basic",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#modern-era-powerful-extensible-ides",
    "href": "Courses/01_Code-Assistant.html#modern-era-powerful-extensible-ides",
    "title": "2  Code Assistants",
    "section": "3.3 Modern Era: Powerful, Extensible IDEs",
    "text": "3.3 Modern Era: Powerful, Extensible IDEs\n\n\n\n2000s: Visual Studio, Eclipse, IntelliJ IDEA\n\\Rightarrow Advanced autocompletion, refactoring, static analysis, and plugin ecosystems.\n\n\n\n\nVisual Studio\n\n\n\n\n\n\n\n\n\n2015: VSCode (based on Electron/Node.js)\n\\Rightarrow Lightweight, open-source, and extensible via marketplace.\n\n\n\n\nVSCode",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#the-language-server-protocol-lsp",
    "href": "Courses/01_Code-Assistant.html#the-language-server-protocol-lsp",
    "title": "2  Code Assistants",
    "section": "3.4 The Language Server Protocol (LSP)",
    "text": "3.4 The Language Server Protocol (LSP)\n2016: Microsoft introduces the Language Server Protocol (LSP)\n\nStandardizes communication between editors/IDEs and language-specific servers.\nEnables features like autocompletion, go-to-definition, linting, and refactoring across many languages.\nDecouples editor development from language tooling.\n\n\n\nLSP Architecture",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#impact-of-lsp-on-developer-experience",
    "href": "Courses/01_Code-Assistant.html#impact-of-lsp-on-developer-experience",
    "title": "2  Code Assistants",
    "section": "3.5 Impact of LSP on Developer Experience",
    "text": "3.5 Impact of LSP on Developer Experience\n\nUnified experience: VSCode, Vim, Emacs, Sublime Text, and more support LSP.\nRapid adoption: Hundreds of languages now have LSP servers.\nConsistent, high-quality tooling regardless of editor.\nPaved the way for advanced features and easier integration of AI assistants.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#the-breakthrough-year",
    "href": "Courses/01_Code-Assistant.html#the-breakthrough-year",
    "title": "2  Code Assistants",
    "section": "4.1 2022: The Breakthrough Year",
    "text": "4.1 2022: The Breakthrough Year\n\n\n\nGitHub Copilot (June 2022)\n\nFirst mainstream LLM-powered code assistant (OpenAI Codex).\nKey features:\n\nCode generation from comments or snippets.\nMulti-language support (Python, JavaScript, Java, etc.).\n\n\n\n\n\n\nGitHub Copilot Autocomplete Demo",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#ai-becomes-ubiquitous",
    "href": "Courses/01_Code-Assistant.html#ai-becomes-ubiquitous",
    "title": "2  Code Assistants",
    "section": "4.2 2023: AI Becomes Ubiquitous",
    "text": "4.2 2023: AI Becomes Ubiquitous\n\nGitHub Copilot X (March 2023)\n\nIntegrated ChatGPT-4 for explanations, test generation, and PR reviews.\nNew features:\n\nNatural language explanations of complex code.\nAutomatic test generation.\nAI-assisted debugging.\n\n\n\n\nJetBrains AI Assistant\n\nNative integration in IntelliJ, PyCharm, etc.\n\nCollaboration tools:\n\nCopilot for Pull Requests, Amazon Q.\n\nVSCode forks:\n\nCursor, Windsurf (by Codeium).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#the-rise-of-autonomous-agents",
    "href": "Courses/01_Code-Assistant.html#the-rise-of-autonomous-agents",
    "title": "2  Code Assistants",
    "section": "4.3 2024: The Rise of Autonomous Agents",
    "text": "4.3 2024: The Rise of Autonomous Agents\n\n\n\nClaude Code (Anthropic, 2024)\n\nAgentic capabilities: Executes tasks (file creation, commits, tests, PRs).\nTerminal integration: Works directly in the terminal.\nHolistic understanding: Cross-file refactors and dependency analysis.\nSecurity: Restrictions for risky commands:refs[1-6,9].\n\n\n\n\n\nClaude Code Demo\n\n\n\n\n\n\n\nGitHub Copilot Enterprise\n\nCustomization for company codebases.\nExtended context (internal docs, Jira tickets).\n\nAdvanced features:\n\nMulti-step agents.\nReal-time visualization (e.g., Artifacts in Claude).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#maturation-and-specialization",
    "href": "Courses/01_Code-Assistant.html#maturation-and-specialization",
    "title": "2  Code Assistants",
    "section": "4.4 2025: Maturation and Specialization",
    "text": "4.4 2025: Maturation and Specialization\n\nDeep integration:\n\nVSCode: Native support for AI agents (Cline, Augment).\nJetBrains: Claude 3.5 and Mellum models:refs[3-4].\nCursor/Windsurf: Popular AI-driven alternatives.\n\nNew features:\n\nMulti-modal editing (code from diagrams, screenshots).\nSpecialized agents for DevOps and security.\nExtreme customization and collaboration.\n\nChallenges:\n\nTechnical debt from “black box” AI-generated code.\nIP concerns and performance issues.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#summary-evolution-of-code-editor-features-20222025",
    "href": "Courses/01_Code-Assistant.html#summary-evolution-of-code-editor-features-20222025",
    "title": "2  Code Assistants",
    "section": "4.5 Summary: Evolution of Code Editor Features (2022–2025)",
    "text": "4.5 Summary: Evolution of Code Editor Features (2022–2025)\n\n\n\n\n\n\n\n\n2022\nLLM-powered code generation\nGitHub Copilot, TabNine\n\n\n\n\n2023\nExplanations, tests, PR reviews\nCopilot X, Amazon Q, JetBrains AI\n\n\n2024\nAutonomous agents, task execution\nClaude Code, Copilot Enterprise\n\n\n2025\nSpecialization, multi-modality\nCursor, Windsurf, Qodo, Continue",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#future-trends",
    "href": "Courses/01_Code-Assistant.html#future-trends",
    "title": "2  Code Assistants",
    "section": "4.6 Future Trends",
    "text": "4.6 Future Trends\n\n\n\nTrend\nBenefits\nChallenges\n\n\n\n\nAI as Co-Pilot\nFaster development, skill augmentation\nOver-reliance, quality control\n\n\nSelf-Healing Editors\nFewer bugs, improved code quality\nFalse positives, transparency\n\n\nLow-Code/No-Code\nAccessibility, rapid prototyping\nLimited customization, maintenance\n\n\nRegulation and Ethics\nSafer, more transparent tools\nCompliance complexity, global fragmentation",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#a-paradigm-shift",
    "href": "Courses/01_Code-Assistant.html#a-paradigm-shift",
    "title": "2  Code Assistants",
    "section": "4.7 A Paradigm Shift",
    "text": "4.7 A Paradigm Shift\n\nFrom manual editing → contextual assistance → AI co-creation.\nChallenge: Mastering tools without compromising quality or security.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#ai-domains",
    "href": "Courses/01_Code-Assistant.html#ai-domains",
    "title": "2  Code Assistants",
    "section": "5.1 AI domains",
    "text": "5.1 AI domains",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#train-a-neural-network",
    "href": "Courses/01_Code-Assistant.html#train-a-neural-network",
    "title": "2  Code Assistants",
    "section": "5.2 Train a neural network",
    "text": "5.2 Train a neural network\n\n\nTanguy Lefort, 2023",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#training-a-supervised-machine-learning-model",
    "href": "Courses/01_Code-Assistant.html#training-a-supervised-machine-learning-model",
    "title": "2  Code Assistants",
    "section": "5.3 Training a supervised Machine learning model",
    "text": "5.3 Training a supervised Machine learning model\n\n\nClass of prediction functions f_\\theta: linear, quadratic, trees\nLoss \\mathcal{L}: L^2 norm, CrossEntropy, purity score\nOptimizer: SGD, Adam, …\n\nlearning rate \\eta: \\theta_{k+1} \\gets \\theta_k - \\eta \\nabla_\\theta \\mathcal{L}\nother hyperparameters\n\nDataset:\n\ntraining: \\{(x_i, y_i)\\}_{i} to compute loss between prediction f_{\\theta}(x_i) and label y_i to update \\theta\ntest: only compute performance scores (no more updates !)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#foreword-beware-the-alchemy",
    "href": "Courses/01_Code-Assistant.html#foreword-beware-the-alchemy",
    "title": "2  Code Assistants",
    "section": "6.1 Foreword, beware the Alchemy",
    "text": "6.1 Foreword, beware the Alchemy\n\n\n\n\n\n\n\nMore or less theoretical guarantees\n\nfield of research\ntype of network\nfrom theory to applications: a gap\n\nMyriad of ad-hoc choices, engeenering tricks and empirical observations\nCurrent choices are critical for success: what are their pros and cons?\nTry \\rightarrow Fail \\rightarrow Try again is the current pipeline",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#tensor-algebra",
    "href": "Courses/01_Code-Assistant.html#tensor-algebra",
    "title": "2  Code Assistants",
    "section": "7.1 Tensor algebra",
    "text": "7.1 Tensor algebra\n\nLinear algebra operations on tensors\nMultiLayerPerceptron = sequence of linear operations and non-linear activations\n\n\\Rightarrow input can be anything: images, videos, text, sound, …",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#automatic-differentiation",
    "href": "Courses/01_Code-Assistant.html#automatic-differentiation",
    "title": "2  Code Assistants",
    "section": "7.2 Automatic differentiation",
    "text": "7.2 Automatic differentiation\n\n\n\nchain rule to compute gradient with respect to \\theta\nkey tool: backpropagation\n\ndon’t need to store the computation graph entirely\ngradient is fast to compute (a single pass)\nbut memory intensive\n\n\n\nf(x)=\\nabla\\frac{x_{1}x_{2} sin(x_3) +e^{x_{1}x_{2}}}{x_3}\n\n\n\\begin{darray}{rcl}\nx_4 & = & x_{1}x_{2}, \\\\\nx_5 & = & sin(x_3), \\\\\nx_6 & = & e^{x_4}, \\\\\nx_7 & = & x_{4}x_{5}, \\\\\nx_8 & = & x_{6}+x_7, \\\\\nx_9 & = & x_{8}/x_3.\n\\end{darray}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#gradient-descent",
    "href": "Courses/01_Code-Assistant.html#gradient-descent",
    "title": "2  Code Assistants",
    "section": "7.3 Gradient descent",
    "text": "7.3 Gradient descent\nExample with a non-convex function\nf(x_1, x_2) = (x_1^2 + x_2 - 11)^2 + (x_1 + x_2^2 - 7)^2\n\n\n\n\nFigure 7.1\n\n\n\nPlotly = require(\"plotly.js@2.35.2/dist/plotly.min.js\");\n\nminX = -5;\nmaxX = 5;\n\nf = ([x1, x2]) =&gt; (x1**2 + x2 - 11)**2 + (x1 + x2**2 - 7)**2;\n\n{\n  const linspace = d3.scaleLinear().domain([0, 49]).range([minX, maxX]);\n  const X1 = Array.from({length: 50}, (_, i) =&gt; linspace(i));\n  const X2 = Array.from({length: 50}, (_, i) =&gt; linspace(i));\n\n  // Define your function f here\n  const f = ([x1, x2]) =&gt; (x1**2 + x2 - 11)**2 + (x1 + x2**2 - 7)**2;\n\n  const Z = X1.map((x1,i) =&gt; X2.map((x2,j) =&gt; f([x1,x2])));\n\n  const data = [{\n    x: X1.flat(),\n    y: X2.flat(),\n    z: Z,\n    type: 'surface'\n  }];\n\n  const layout = {\n    // title: '',\n    autosize: false,\n    width: 400,\n    height: 400,\n    paper_bgcolor: \"rgba(0,0,0,0)\",\n    plot_bgcolor: \"rgba(0,0,0,0)\",\n    template: 'plotly_dark',\n    margin: {\n      l: 0,\n      r: 0,\n      b: 0,\n      t: 0\n    }\n  };\n\n  const div = document.createElement('div');\n  Plotly.newPlot(div, data, layout,{displayModeBar: false});\n  return div;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunction grad_descent(x1,x2,step,max_iter) {\n  let grad = f_grad(x1, x2);\n  let iterations = [[x1, x2]];\n  function f_grad(x1, x2) {\n    let df_x1 = 2 * (-7 + x1 + x2**2 + 2 * x1 * (-11 + x1**2 + x2));\n    let df_x2 = 2 * (-11 + x1**2 + x2 + 2 * x2 * (-7 + x1 + x2**2));\n    return [df_x1, df_x2];\n  }\n  var count = 0;\n  while (count &lt; max_iter) {\n    x1 -= step * grad[0];\n    x2 -= step * grad[1];\n    grad = f_grad(x1, x2);\n    if (isFinite(x1) && isFinite(x2) &&\n      (minX &lt; x1) && (x1 &lt; maxX) &&\n      (minX &lt; x2) && (x2 &lt; maxX))\n        iterations.push([x1, x2]);\n    else iterations.push(iterations[count])\n    count += 1\n  }\n  return iterations;\n}\n\n\n\n\n\n\n\nviewof descent_params = Inputs.form({\n  x1: Inputs.range([minX, maxX], {step: 0.1, value: 0, label: 'x1 initial'}),\n  x2: Inputs.range([minX, maxX], {step: 0.1, value: 0, label: 'x2 initial'}),\n  step: Inputs.range([0.001, 0.04], {step: 0.001, value: 0.01, label: 'Step size'})\n})\n\n\n\n\n\n\n\nprefix = Inputs.text().classList[0];\n\n{\n  d3.selectAll(`.${prefix}`).style(\"font-size\", \"16px\");\n  var iterations = grad_descent(descent_params.x1,descent_params.x2,descent_params.step,20)\n  return Plot.plot({\n    aspectRatio: 1,\n    x: {tickSpacing: 50, label: \"x1 →\"},\n    y: {tickSpacing: 50, label: \"x2 →\"},\n    width: 600,\n    style: {\n      backgroundColor: 'rgba(0,0,0,0)'\n    },\n    marks: [\n      Plot.contour({\n        fill: (x1, x2) =&gt; Math.sqrt((x1**2 + x2 - 11)**2 + (x1 + x2**2 - 7)**2),\n        x1: minX,\n        y1: minX,\n        x2: maxX,\n        y2: maxX,\n        showlegend: false,\n        colorscale: 'RdBu',\n        ncontours: 30\n      }),\n      Plot.line(iterations,{marker: true})\n    ]\n  })\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSensitivity to initial point and step size",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#stochastic-gradient-descent",
    "href": "Courses/01_Code-Assistant.html#stochastic-gradient-descent",
    "title": "2  Code Assistants",
    "section": "7.4 (Stochastic) Gradient descent",
    "text": "7.4 (Stochastic) Gradient descent\n\n\n\n\nnot use all the data at once to compute the gradient\n\nnot feasible in practice (memory wise)\n\nUse mini-batch of data (boostrap samples)\n\none more hyperparameter…\n\n\n\n\n\n\\theta_{k+1} \\leftarrow \\theta_k - \\frac{\\eta}{n}\\sum_{i\\in\\text{batch}}\\nabla_\\theta \\mathcal{L}(f_\\theta(x_i), y_i)\n\n\n\n\n\\Rightarrow No general guarantees of convergence in DL setting",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#optimizers",
    "href": "Courses/01_Code-Assistant.html#optimizers",
    "title": "2  Code Assistants",
    "section": "7.5 Optimizers",
    "text": "7.5 Optimizers\nSGD, Adam, RMSProp\n\nNon-convex optimization research on the subject is still very active, and there is no clear consensus on what is the best optimizer to use in a given situation.\nNo guarantee of global minimum, only local minimum\nNo guarantee of convergence, only convergence in probability",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#more-than-a-pinch-of-non-linearities",
    "href": "Courses/01_Code-Assistant.html#more-than-a-pinch-of-non-linearities",
    "title": "2  Code Assistants",
    "section": "7.6 (More than) a pinch of non-linearities",
    "text": "7.6 (More than) a pinch of non-linearities\n\n\n\n\nLinear Transformations + Non-linear activation functions\nradically enhance the expressive power of the model\nability to explore the space of functions in gradient descent.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#from-text-to-numbers",
    "href": "Courses/01_Code-Assistant.html#from-text-to-numbers",
    "title": "2  Code Assistants",
    "section": "8.1 From text to numbers",
    "text": "8.1 From text to numbers\n\nMain problem: we can’t multiply or do convolutions with words\nSecond problem: many words (for a single language)\nThird problem: how to capture semantics?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#embeddings-2",
    "href": "Courses/01_Code-Assistant.html#embeddings-2",
    "title": "2  Code Assistants",
    "section": "8.2 Embeddings",
    "text": "8.2 Embeddings\n\nDistance between words should not be character based\n\n\n\n\n\n\n\n\\Rightarrow\n\n\n\n\n\n\n\nTanguy Lefort, 2023",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#multi-scale-learning-from-text",
    "href": "Courses/01_Code-Assistant.html#multi-scale-learning-from-text",
    "title": "2  Code Assistants",
    "section": "8.3 Multi-scale learning from text",
    "text": "8.3 Multi-scale learning from text\n\nDL layers = capture different levels of dependencies in the data\nattention mechansim applies “multi-scale learning” to data sequences \\Rightarrow e.g. not only words in sentences, but sentences in paragraphs, paragraphs in documents and so on.\n\n\\Rightarrow transformers capture dependencies in the “whole”",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#multi-facets-learning-from-text",
    "href": "Courses/01_Code-Assistant.html#multi-facets-learning-from-text",
    "title": "2  Code Assistants",
    "section": "8.4 Multi-facets learning from text",
    "text": "8.4 Multi-facets learning from text\nMulti-head attention mechanism extends the attention mechanism to multifaceted dependencies of the same text components.\nIn the sentence “the cat sat on the rug, and after a few hours, it moved to the mat.” :\n\ncat/rug/mat\nrug/mat\ncat/he\nsat/moved to\n\n\\Rightarrow All those groups of words/tokens are multiple facets of the same text and its meaning.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#transformers",
    "href": "Courses/01_Code-Assistant.html#transformers",
    "title": "2  Code Assistants",
    "section": "8.5 Transformers",
    "text": "8.5 Transformers\n\n\nVaswani et al. (2017)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#heart-of-transformers-attention-mechanism",
    "href": "Courses/01_Code-Assistant.html#heart-of-transformers-attention-mechanism",
    "title": "2  Code Assistants",
    "section": "8.6 Heart of Transformers: Attention mechanism",
    "text": "8.6 Heart of Transformers: Attention mechanism\n\n\n\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\n\n\n\n\nThree matrices: Query, Key, Value, derived from the input sequence\nd_k: dimension of the key matrix, typically 64 or 128\nwe want to compute a weighted sum of the values V with weights given by the compatibility between the query and the keys\nsoftmax to get a probability distribution\nmulti-head attention: several attention mechanisms in parallel\n\n\n\n\n\n\n\n\nVaswani et al. (2017)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#head-view-of-attention",
    "href": "Courses/01_Code-Assistant.html#head-view-of-attention",
    "title": "2  Code Assistants",
    "section": "8.7 Head view of attention",
    "text": "8.7 Head view of attention\n\n\n\n\n\n\nFigure 8.1: The model view visualizes attention across all heads in a single Transformer layer.\n\n\n\n\n\n\n\n\n\n\n\nEach line shows the attention from one token (left) to another (right).\nLine weight reflects the attention value (ranges from 0 to 1),\nline color identifies the attention head",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#bert-bidirectional-encoder-representations-from-transformers",
    "href": "Courses/01_Code-Assistant.html#bert-bidirectional-encoder-representations-from-transformers",
    "title": "2  Code Assistants",
    "section": "8.8 BERT: Bidirectional Encoder Representations from Transformers",
    "text": "8.8 BERT: Bidirectional Encoder Representations from Transformers\n\nembeddings: represent words as vectors in high dimensions",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#gpt-generative-pre-trained-transformer",
    "href": "Courses/01_Code-Assistant.html#gpt-generative-pre-trained-transformer",
    "title": "2  Code Assistants",
    "section": "8.9 GPT : Generative Pre-trained Transformer",
    "text": "8.9 GPT : Generative Pre-trained Transformer\n\nautoregressive model\ngenerates text by predicting the next token\npre-trained on large corpora of text",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#bert-vs-gpt",
    "href": "Courses/01_Code-Assistant.html#bert-vs-gpt",
    "title": "2  Code Assistants",
    "section": "8.10 BERT vs GPT",
    "text": "8.10 BERT vs GPT\n\n\n\n\n\nBERT\n\n\n\n\n\n\n\nGPT",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#summary-of-llm-types",
    "href": "Courses/01_Code-Assistant.html#summary-of-llm-types",
    "title": "2  Code Assistants",
    "section": "8.11 Summary of LLM types",
    "text": "8.11 Summary of LLM types\n\n\n\nType\nArchitecture\nTraining Objective\nAttention\nUse Cases\n\n\n\n\nBERT (Encoder-Only)\nEncoder stack only\nMasked Language Modeling (MLM)\nBidirectional (sees left and right context)\nClassification, QA, NER, sentiment analysis\n\n\nGPT (Decoder-Only)\nDecoder stack only\nAutoregressive Language Modeling (next token prediction)\nUnidirectional (left-to-right, autoregressive)\nText generation, chatbots, open-ended tasks\n\n\nSeq2Seq (Encoder-Decoder)\nEncoder + Decoder stacks\nSequence-to-sequence (e.g., translation, summarization)\nEncoder: Bidirectional; Decoder: Unidirectional (autoregressive)\nTranslation, summarization, speech recognition, data-to-text\n\n\n\n\n\n\nType\nStrengths\nWeaknesses\nExample Models\nTraining Data\nInference Speed\n\n\n\n\nBERT (Encoder-Only)\nDeep understanding of input; strong for discriminative tasks\nNot designed for generation\nBERT, RoBERTa, DistilBERT\nLarge corpus (masked tokens)\nFast (parallelizable)\n\n\nGPT (Decoder-Only)\nCoherent, fluent generation; open-ended creativity\nNo bidirectional context; limited to left-to-right generation\nGPT-3, GPT-4, Llama\nLarge corpus (autoregressive)\nSlower (autoregressive)\n\n\nSeq2Seq (Encoder-Decoder)\nExplicit input-output mapping; handles sequence transformation\nMore complex; requires aligned input-output pairs\nT5, BART, Transformer (original), Whisper\nParallel corpora (input-output pairs)\nModerate (depends on sequence length)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#generative-llms-base-vs-instruct",
    "href": "Courses/01_Code-Assistant.html#generative-llms-base-vs-instruct",
    "title": "2  Code Assistants",
    "section": "8.12 Generative LLMs, Base vs Instruct",
    "text": "8.12 Generative LLMs, Base vs Instruct\n\nBase models are just predicting the next word (pre-training phase, no task-specific fine-tuning)\nInstruct models are fine-tuned on specific tasks and follow user instructions more effectively.\n\n\n\n\n\n\n\nImportant\n\n\n\nNever use the base model for specific tasks without fine-tuning.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#generative-llms-reasoning-vs-non-reasoning",
    "href": "Courses/01_Code-Assistant.html#generative-llms-reasoning-vs-non-reasoning",
    "title": "2  Code Assistants",
    "section": "8.13 Generative LLMs, Reasoning vs non-Reasoning",
    "text": "8.13 Generative LLMs, Reasoning vs non-Reasoning\n\n(Non-reasoning) models focus on generating coherent text without explicit reasoning capabilities\nReasoning models are designed to perform complex reasoning tasks and can handle multi-step problems, at the cost of increased computational requirements (and budget)\n\n\n\n\n\n\n\nTip\n\n\n\nReasoning addition to LLM have been a breakthrough in the field since end of 2024.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#the-importance-of-the-context-window",
    "href": "Courses/01_Code-Assistant.html#the-importance-of-the-context-window",
    "title": "2  Code Assistants",
    "section": "8.14 The importance of the context window",
    "text": "8.14 The importance of the context window\n\nThe context window is crucial for understanding and generating text.\nIt determines how much information the model can consider at once.\nLarger context windows allow for better understanding of complex queries and generation of more coherent responses\nTypical max context window are in a 16k tokens, latest open-weights local llms are 128/256/512k tokens, frontier llms are 1M+ tokens\nlong context window are computationally expensive and require more memory/gpu ressources.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#what-happens-when-the-context-window-is-exceeded",
    "href": "Courses/01_Code-Assistant.html#what-happens-when-the-context-window-is-exceeded",
    "title": "2  Code Assistants",
    "section": "8.15 What happens when the context window is exceeded?",
    "text": "8.15 What happens when the context window is exceeded?\n\nWhen the context window is exceeded, the model may lose track of important information, leading to less coherent responses.\nStrategies to handle this include:\n\nSummarizing previous context\nUsing external memory stores\nChunking input data\n\n\n\n\n\n\n\n\nCaution\n\n\n\nVery large context (when permitted by the model) isn’t always a good thing: there is chances that the model may become overwhelmed with information, leading to decreased performance AND quality.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#rag-retrieval-augmented-generation",
    "href": "Courses/01_Code-Assistant.html#rag-retrieval-augmented-generation",
    "title": "2  Code Assistants",
    "section": "8.16 RAG (Retrieval-Augmented Generation)",
    "text": "8.16 RAG (Retrieval-Augmented Generation)\n\n\n\n\nRAG combines retrieval-based and generation-based approaches.\nIt retrieves relevant documents from a knowledge base and uses them to inform the generation process.\nThis allows for more accurate and contextually relevant responses.\n\n\n\n\n\n\n\n\nTurtlecrown, Wikipedia",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#reactive-agents",
    "href": "Courses/01_Code-Assistant.html#reactive-agents",
    "title": "2  Code Assistants",
    "section": "9.1 Reactive agents",
    "text": "9.1 Reactive agents\n\n\n\nExamples:\n\nChatbots that respond to user queries with pre-defined answers.\nSimple automation scripts that trigger actions based on specific events, like web search.\nAgentic mode in Code Assistants\n\n\n\n\n\n\n\nWarning\n\n\n\n\\Rightarrow Control is done by the LLM itself with all risks : infinite loops, unsupervised and potentially dangerous actions etc.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#pipeline-agents",
    "href": "Courses/01_Code-Assistant.html#pipeline-agents",
    "title": "2  Code Assistants",
    "section": "9.2 Pipeline Agents",
    "text": "9.2 Pipeline Agents\n\n\n\nExamples:\n\nRAG queries\nSummarizing documents\nCommunicating with other agents\n\n\n\n\n\n\n\nNote\n\n\n\n\\Rightarrow Control is done by normal, program logic",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#reactive-agent-2025-mcp",
    "href": "Courses/01_Code-Assistant.html#reactive-agent-2025-mcp",
    "title": "2  Code Assistants",
    "section": "9.3 Reactive agent 2025 : MCP",
    "text": "9.3 Reactive agent 2025 : MCP\n\n\nUjjwal Khadka Source",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#ai-for-pipelinesgraphs",
    "href": "Courses/01_Code-Assistant.html#ai-for-pipelinesgraphs",
    "title": "2  Code Assistants",
    "section": "9.4 AI for pipelines/graphs",
    "text": "9.4 AI for pipelines/graphs\n\nLow level (API, almost request-level)\n\nOpenAI API (ubiquitous)\nHuggingface\n\nHigh level (Framework)\n\nLangchain (most popular)\nSemantic Kernel (Microsoft)\nHaystack\nand many many others…\n\n\nSimple request with OpenAI API :\nfrom openai import OpenAI\nclient = OpenAI()\n\nchat_response = client.chat.completions.create(\n    model= \"gpt-4o\",\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": \"What is the best French cheese?\",\n        },\n    ]\n)\nprint(chat_response.choices[0].message.content)\nSimple request with LangChain :\nfrom langchain.chat_models import init_chat_model\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\nmodel = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n\nmessages = [\n    SystemMessage(\"Translate the following from English into Italian\"),\n    HumanMessage(\"hi!\"),\n]\n\nmodel.invoke(messages)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#mcp-server-example",
    "href": "Courses/01_Code-Assistant.html#mcp-server-example",
    "title": "2  Code Assistants",
    "section": "9.5 MCP Server example",
    "text": "9.5 MCP Server example\nfrom datetime import datetime\nfrom mcp.server.fastmcp import FastMCP\n\n# Create an MCP server\nmcp = FastMCP(\"GetTime\")\n\n@mcp.tool()\ndef get_date() -&gt; str:\n    \"\"\"Returns the current date in YYYY-MM-DD format.\"\"\"\n    return datetime.today().strftime('%Y-%m-%d')",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#the-real-question",
    "href": "Courses/01_Code-Assistant.html#the-real-question",
    "title": "2  Code Assistants",
    "section": "10.1 The real question",
    "text": "10.1 The real question\n\n\n\n\nAre the benefits of using generative AI worth the cost of extra supervision and the additional engineering effort?\n\n\n\nThe general answer is:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/01_Code-Assistant.html#on-the-other-hand",
    "href": "Courses/01_Code-Assistant.html#on-the-other-hand",
    "title": "2  Code Assistants",
    "section": "10.2 On the other hand…",
    "text": "10.2 On the other hand…\n\n\n\n\n\n\n\n\n\nBeware of vibe-coding",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Code Assistants</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html",
    "href": "Courses/02_Parallel-intro.html",
    "title": "3  Introduction to parallel computing",
    "section": "",
    "text": "4 Parallel computing: the intuition",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#computing",
    "href": "Courses/02_Parallel-intro.html#computing",
    "title": "3  Introduction to parallel computing",
    "section": "4.1 Computing ?",
    "text": "4.1 Computing ?\n\na computation = a succession of tasks to complete\na task \\approx a single command/action or a group of commands/actions\n\n\n\n\nExample 1:\n\n\nExample 2:\n\n\n\n\n# task i:\n# sum of elements at index i\n# from two vectors\nfor i in range(10):\n    res[i] = a[i] + b[i]\n\n\n# task 1: matrix product\nC = A @ B\n# task 2: colwise sum over matrix C\nnp.sum(C,axis=0)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#why-parallel-computing",
    "href": "Courses/02_Parallel-intro.html#why-parallel-computing",
    "title": "3  Introduction to parallel computing",
    "section": "4.2 Why parallel computing?",
    "text": "4.2 Why parallel computing?\n\nObjective: accelerate computations &lt;=&gt; reduce computation time\nIdea: run multiple tasks in parallel instead of sequentially",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#context-level-1",
    "href": "Courses/02_Parallel-intro.html#context-level-1",
    "title": "3  Introduction to parallel computing",
    "section": "4.3 Context (level 1)",
    "text": "4.3 Context (level 1)\n\ndifferent tasks to complete\none or more workers to complete the tasks",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#sequential-computing",
    "href": "Courses/02_Parallel-intro.html#sequential-computing",
    "title": "3  Introduction to parallel computing",
    "section": "4.4 Sequential computing",
    "text": "4.4 Sequential computing\n\n\n\n\nn tasks to complete (n&gt;1)\n1 worker\n\nTotal time (exercise)\n\n\\sum_{i=1}^n t_i \\sim O(n)\\ with t_i time to complete task i}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#parallel-computing-the-most-simple-case",
    "href": "Courses/02_Parallel-intro.html#parallel-computing-the-most-simple-case",
    "title": "3  Introduction to parallel computing",
    "section": "4.5 Parallel computing (the most simple case)",
    "text": "4.5 Parallel computing (the most simple case)\n\n\n\n\n\n\nn tasks to complete (n&gt;1)\np workers (p&gt;=n)\n\n\n\n\nTotal time (exercise)\n\n\\underset{i=1,\\dots,n}{\\text{max}}\\{t_i\\}\\sim O(1)\\ with t_i time to complete task i\n\n\n\nPotential bottleneck? (exercise)\n\nnot enough workers to complete all tasks",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#task-scheduling",
    "href": "Courses/02_Parallel-intro.html#task-scheduling",
    "title": "3  Introduction to parallel computing",
    "section": "4.6 Task scheduling",
    "text": "4.6 Task scheduling\n\n\n\n\nn tasks to complete (n&gt;1)\np workers (p&lt;n)\n\nNeed: assign multiple tasks to each worker (and manage this assignment)\n\n\n\n\n\n\n\n\n\n\nTotal time (exercise)\n\n\\underset{k=1,\\dots,p}{\\text{max}}\\{T_k\\}\\sim O(n/p)\\ with T_k = \\sum_{i\\in I_k} t_i, total time to complete all tasks assigned to worker k (where I_k is the set of indexes of tasks assigned to worker k)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#illustration-parallel-computing-simple-case",
    "href": "Courses/02_Parallel-intro.html#illustration-parallel-computing-simple-case",
    "title": "3  Introduction to parallel computing",
    "section": "4.7 Illustration: parallel computing (simple case)",
    "text": "4.7 Illustration: parallel computing (simple case)\n\n\n\na task = “wait 1 \\mus”\nObjective: run 100 tasks\nNumber of workers: 1, 2, 4, 6, 8\n\nWhy is the time gain not linear?\n\n\n\n10 repetitions in each configurations",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#context-level-2",
    "href": "Courses/02_Parallel-intro.html#context-level-2",
    "title": "3  Introduction to parallel computing",
    "section": "4.8 Context (level 2)",
    "text": "4.8 Context (level 2)\n\ndifferent tasks to complete\nmultiple workers to complete the tasks\none or more working resources1\n\nPotential bottleneck? (exercise)\n\nnot enough resources for all workers",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#resource-management",
    "href": "Courses/02_Parallel-intro.html#resource-management",
    "title": "3  Introduction to parallel computing",
    "section": "4.9 Resource management",
    "text": "4.9 Resource management\n\n\n\n\nn tasks to complete (n&gt;1)\np workers (p&lt;n)\nq working resources (q&lt;p)\n\nNeed:\n\nassign workers to each resource (and manage this assignment)\n\nTotal time = ? (exercise)\nPotential issues? (exercise)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#resource-management-1",
    "href": "Courses/02_Parallel-intro.html#resource-management-1",
    "title": "3  Introduction to parallel computing",
    "section": "4.10 Resource management",
    "text": "4.10 Resource management\nTotal time = \\text{max}_{\\ell=1,\\dots,q}\\{\\tau_\\ell\\}\\sim O(n/q)\nwith \\tau_\\ell = \\sum_{i\\in J_\\ell} t_i = total time to complete all tasks done on resource \\ell (where J_\\ell is the set of indexes of tasks assigned done on resource \\ell)\nPotential issues? multiple workers want to use the same working resources\n\nthey have to wait for their turn (workers are not working all the time)\nrisk to jam2 resource access (organizing resource access takes time)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#illustration-overhead-for-resource-access",
    "href": "Courses/02_Parallel-intro.html#illustration-overhead-for-resource-access",
    "title": "3  Introduction to parallel computing",
    "section": "4.11 Illustration: overhead for resource access",
    "text": "4.11 Illustration: overhead for resource access\n\n\n\na task = “wait 1 \\mus”\nObjective: run 100 tasks\n8 computing units\nNumber of workers: 1, 2, 4, 8, 16, 32\n\n\n\n\n10 repetitions in each configurations",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#context-level-3-realistic",
    "href": "Courses/02_Parallel-intro.html#context-level-3-realistic",
    "title": "3  Introduction to parallel computing",
    "section": "4.12 Context (level 3: realistic)",
    "text": "4.12 Context (level 3: realistic)\n\ndifferent tasks to complete\nmultiple workers to complete the tasks\none or more working resources\n\nInput/Output (I/O)\n\nInput: each task requires some materials (data) to be completed, these materials are stored in a storage area (memory)\nOutput: each task returns a result that need to be put in the storage area (memory)\n\nExamples: vector/matrix/array operations, process the content of multiple files",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#inputoutput-management",
    "href": "Courses/02_Parallel-intro.html#inputoutput-management",
    "title": "3  Introduction to parallel computing",
    "section": "4.13 Input/Output management",
    "text": "4.13 Input/Output management\n\nn tasks to complete (n&gt;1)\np workers (p&lt;n)\nq working resources (q&lt;p)\ntasks need input (data) and produce output (results)\n\nNeed:\n\nload input (data) from storage when needed by a worker to complete a task\nwrite output (result) to storage when a task is completed\n\nTotal time = ? (exercise)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#parallel-computing-realistic-model",
    "href": "Courses/02_Parallel-intro.html#parallel-computing-realistic-model",
    "title": "3  Introduction to parallel computing",
    "section": "4.14 Parallel computing: realistic model",
    "text": "4.14 Parallel computing: realistic model",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#computing-time-and-potential-bottleneck",
    "href": "Courses/02_Parallel-intro.html#computing-time-and-potential-bottleneck",
    "title": "3  Introduction to parallel computing",
    "section": "4.15 Computing time and potential bottleneck",
    "text": "4.15 Computing time and potential bottleneck\nTotal time = \\text{max}_{\\ell=1,\\dots,q}\\{\\tau_\\ell\\}\nwith \\tau_\\ell = \\sum_{i\\in J_\\ell} t_{i,\\text{in}} + t_i + t_{i,\\text{out}} = total time to complete all tasks done on resource \\ell (where J_\\ell is the set of indexes of tasks done on resource \\ell)\nPotential bottlenecks:\n\ninput (data) are not ready/available when a worker need them to complete a task (the worker have to wait)\noutput (results) cannot be written when a worker complete a task (the worker have to wait)\n\nOverhead on memory access\n\nconcurrent access to a memory space when reading input and/or when writing output\nconcurrent data transfer from or to memory (the “pipe” are jammed)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#illustration-1-overhead-for-io-access",
    "href": "Courses/02_Parallel-intro.html#illustration-1-overhead-for-io-access",
    "title": "3  Introduction to parallel computing",
    "section": "4.16 Illustration 1: overhead for I/O access",
    "text": "4.16 Illustration 1: overhead for I/O access\n\n\n\na task\n\nsimulate a vector of 10 values\ncompute the mean\n\nObjective: run 10000 tasks\nResources: 8 computing units\nNumber of workers: 1, 2, 4, 6, 8\n\n\n\n\n10 repetitions in each configurations",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#illustration-2-overhead-for-io-access",
    "href": "Courses/02_Parallel-intro.html#illustration-2-overhead-for-io-access",
    "title": "3  Introduction to parallel computing",
    "section": "4.17 Illustration 2: overhead for I/O access",
    "text": "4.17 Illustration 2: overhead for I/O access\n\n\n\na task = “compute the sum of a given row in a matrix”\nObjective: compute all row-wise sums for a 10000 \\times 1000 matrix (i.e. 10000 tasks)\nResources: 8 computing units\nNumber of workers: 1, 2, 4, 6, 8\n\n\n\n\n20 repetitions in each configurations",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#the-vocabulary-of-parallel-computing",
    "href": "Courses/02_Parallel-intro.html#the-vocabulary-of-parallel-computing",
    "title": "3  Introduction to parallel computing",
    "section": "4.18 The vocabulary of parallel computing",
    "text": "4.18 The vocabulary of parallel computing\n\ntasks = a command or a group of commands\nworker = a program or a sub-program (like a thread or a sub-process) → Software\nworking resources = processing units → Hardware\ninput = data\noutput = result\nstorage = memory\n\nAttention: “worker” may sometimes refer to a working resource in the literature",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#task-synchronization",
    "href": "Courses/02_Parallel-intro.html#task-synchronization",
    "title": "3  Introduction to parallel computing",
    "section": "4.19 Task synchronization",
    "text": "4.19 Task synchronization\n\nSometimes tasks cannot be done in parallel\n\nSpecific case: output of task i_1 is input of task i_2\nNeed: wait for task i_1 before task i_2 starts\n\n\n\n\nExample 1:\n# task 1: matrix product\nC = A @ B\n# task 2: colwise sum over matrix C\nnp.sum(C,axis=0)\n\nExample 2:\n\ntask 1: train a predictive model\ntask 2: use the trained model to predict new labels",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#trend-over-50years",
    "href": "Courses/02_Parallel-intro.html#trend-over-50years",
    "title": "3  Introduction to parallel computing",
    "section": "5.1 Trend over ~50years",
    "text": "5.1 Trend over ~50years\n\n\nMoore’s Law (doubling the transistor counts every two years) is live\nSingle thread performance hit a wall in 2000s\nAlong with typical power usage and frequency\nNumber of logical cores is doubling every ~3 years since mid-2000\n\n\n\n\nOriginal data up to the year 2010 collected and plotted by M. Horowitz, F. Labonte, O. Shacham, K. Olukotun, L. Hammond, and C. Batten New plot and data collected for 2010-2021 by K. Rupp\n\n\n\n\nGithub repo for data",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#computing-units",
    "href": "Courses/02_Parallel-intro.html#computing-units",
    "title": "3  Introduction to parallel computing",
    "section": "5.2 Computing units",
    "text": "5.2 Computing units\n\n\nCPU :\n\n4/8/16+ execution cores (depending on context, laptop, desktop, server)\nHyperthreading (Intel) or SMT (AMD), x2\nVector units (multiple instructions processed on a vector of data)\n\nGPU computing : 100/1000 “simple” cores per card",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#the-reality",
    "href": "Courses/02_Parallel-intro.html#the-reality",
    "title": "3  Introduction to parallel computing",
    "section": "5.3 The reality",
    "text": "5.3 The reality\n\n\nA serial application only accesses 0.8% of the processing power of a 16-core CPU.\n\n\n\n0.08\\% = \\frac{1}{16 * 2 (cores + hyperthreading) * \\frac{256 (bitwide vector unit}{64(bit double)} = 128}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#faster-for-less-development",
    "href": "Courses/02_Parallel-intro.html#faster-for-less-development",
    "title": "3  Introduction to parallel computing",
    "section": "6.1 Faster for less development",
    "text": "6.1 Faster for less development\n\\frac{S_{up}}{T_{par}} \\gg \\frac{S_{up}}{T_{seq}}\nRatio of speedup improvment S_{up} over time of development (T_{seq|par}) comparison.\nFrom a development time perspective, return on investment (speedup) is often several magnitudes of order better than pure “serial/sequential” improvment.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#scaling",
    "href": "Courses/02_Parallel-intro.html#scaling",
    "title": "3  Introduction to parallel computing",
    "section": "6.2 Scaling",
    "text": "6.2 Scaling\nSimple “divide and conquer” strategies in parallel programming allow to handle data with previously almost untractable sizes and scale before.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#energy-efficiency",
    "href": "Courses/02_Parallel-intro.html#energy-efficiency",
    "title": "3  Introduction to parallel computing",
    "section": "6.3 Energy efficiency",
    "text": "6.3 Energy efficiency\n\n\n\n\n\n\nNote\n\n\n\nThis is a huge one, in the present context 😬\n\n\nDifficult to estimate but the Thermal Design Power (TDP), given by hardware manufacturers, is a good rule of thumb. Just factor the number of units, and usual proportionality rules.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#energy-efficiency-a-bunch-of-cpus",
    "href": "Courses/02_Parallel-intro.html#energy-efficiency-a-bunch-of-cpus",
    "title": "3  Introduction to parallel computing",
    "section": "6.4 Energy efficiency, a bunch of CPUs",
    "text": "6.4 Energy efficiency, a bunch of CPUs\nExample of “standard” use : 20 16-core Intel Xeon E5-4660 which is 120~W of TDP\nP = (20~Processors) * (120~W/~Processors) * (24~hours) = 57.60~kWhrs",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#energy-efficiency-just-a-few-big-gpus",
    "href": "Courses/02_Parallel-intro.html#energy-efficiency-just-a-few-big-gpus",
    "title": "3  Introduction to parallel computing",
    "section": "6.5 Energy efficiency, just a few (big) GPUs",
    "text": "6.5 Energy efficiency, just a few (big) GPUs\nA Tesla V100 GPU is of 300~W of TDP. Let’s use 4 of them.\nP = (4~GPUs) * (300~W/~GPUs) * (24~hours) = 28.80~kWhrs\n\\Longrightarrow half of the power use",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#terms-and-definitions",
    "href": "Courses/02_Parallel-intro.html#terms-and-definitions",
    "title": "3  Introduction to parallel computing",
    "section": "7.1 Terms and definitions",
    "text": "7.1 Terms and definitions\n\nSpeedup S_{up}(N): ratio of the time of execution in serial and parallel mode\nNumber of computing units N\nP (resp. S) is the parallel (resp. serial) fraction of the time spent in the parallel (resp. serial) part of the program (P+S=1).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#asymptote-of-parallel-computing-amdahls-law",
    "href": "Courses/02_Parallel-intro.html#asymptote-of-parallel-computing-amdahls-law",
    "title": "3  Introduction to parallel computing",
    "section": "7.2 Asymptote of parallel computing : Amdahl’s Law",
    "text": "7.2 Asymptote of parallel computing : Amdahl’s Law\nThere P is the fraction of the time spent in the parallel part of the program in a sequential execution.\nS_{up}(N) \\le \\frac{1}{S+\\frac{P}{N}}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#asymptote-of-parallel-computing-amdahls-law-graphic",
    "href": "Courses/02_Parallel-intro.html#asymptote-of-parallel-computing-amdahls-law-graphic",
    "title": "3  Introduction to parallel computing",
    "section": "7.3 Asymptote of parallel computing : Amdahl’s Law, Graphic",
    "text": "7.3 Asymptote of parallel computing : Amdahl’s Law, Graphic\n\n\nIdeal speedup : 100% of the code parallelized; 90%, 75%, and 50% : limited by the fractions of code that remain serial. (Robey and Zamora 2021)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#more-with-almost-less-the-pump-it-up-approach",
    "href": "Courses/02_Parallel-intro.html#more-with-almost-less-the-pump-it-up-approach",
    "title": "3  Introduction to parallel computing",
    "section": "7.4 More with (almost) less : the pump it up approach",
    "text": "7.4 More with (almost) less : the pump it up approach\nGustafson’s law\nThere now, P is the fraction of the time spent in the parallel part of the program in a parallel execution.\n\n\n\n\nWhen the size of the problem grows up proportionnaly to the number of computing units.\nS_{up}(N) \\le N - S*(N-1)\nwhere N is the number of computing units and S the serial fraction as before.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#more-with-almost-less-graphic",
    "href": "Courses/02_Parallel-intro.html#more-with-almost-less-graphic",
    "title": "3  Introduction to parallel computing",
    "section": "7.5 More with (almost) less : graphic",
    "text": "7.5 More with (almost) less : graphic\n\n\nLinear growth with the number of processor (and data size too)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#strong-vs-weak-scaling-definitions",
    "href": "Courses/02_Parallel-intro.html#strong-vs-weak-scaling-definitions",
    "title": "3  Introduction to parallel computing",
    "section": "7.6 Strong vs Weak Scaling, definitions",
    "text": "7.6 Strong vs Weak Scaling, definitions\n\n\nStrong Scaling\n\nStrong scaling represents the time to solution with respect to the number of processors for a fixed total size.\n\n\n\\Rightarrow Amdahl’s law\n\nWeak Scaling\n\nWeak scaling represents the time to solution with respect to the number of processors for a fixed-sized problem per processor.\n\n\n\\Rightarrow Gustafson’s law",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#strong-vs-weak-scaling-schemas",
    "href": "Courses/02_Parallel-intro.html#strong-vs-weak-scaling-schemas",
    "title": "3  Introduction to parallel computing",
    "section": "7.7 Strong vs Weak Scaling, schemas",
    "text": "7.7 Strong vs Weak Scaling, schemas",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#flynns-taxonomy",
    "href": "Courses/02_Parallel-intro.html#flynns-taxonomy",
    "title": "3  Introduction to parallel computing",
    "section": "8.1 Flynn’s taxonomy",
    "text": "8.1 Flynn’s taxonomy\n\n\n\n\nSimple Instruction\nMultiple Instructions\n\n\n\n\nSimple Data\n\n\n\n\nMultiple Data",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#a-different-approach",
    "href": "Courses/02_Parallel-intro.html#a-different-approach",
    "title": "3  Introduction to parallel computing",
    "section": "8.2 A different approach",
    "text": "8.2 A different approach\n\n\n\nParallelism level\nHardware\nSoftware\nParallelism extraction\n\n\n\n\nInstruction\nSIMD (or VLIW)\nIntrinsics\nCompiler\n\n\nThread\nMulti-core RTOS\nLibrary or language extension\nPartitioning/Scheduling (dependency control)\n\n\nTask\nMulti-core (w/o RTOS)\nProcesses (OS level)\nPartitioning/Scheduling",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#multi-processing-vs-multi-threading",
    "href": "Courses/02_Parallel-intro.html#multi-processing-vs-multi-threading",
    "title": "3  Introduction to parallel computing",
    "section": "8.3 Multi-processing vs Multi-threading",
    "text": "8.3 Multi-processing vs Multi-threading\n\n\n\n\n\nMulti-Processing\n\n\n\n\n\n\n\nMulti-Threading\n\n\n\n\n\n\n\n\n\n\nMulti-processing\nMulti-threading\n\n\n\n\nMemory\nExclusive\nShared\n\n\nCommunication\nInter-process\nAt caller site\n\n\nCreation overhead\nHeavy\nMinimal\n\n\nConcurrency\nAt OS level\nLibrary/language",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/02_Parallel-intro.html#footnotes",
    "href": "Courses/02_Parallel-intro.html#footnotes",
    "title": "3  Introduction to parallel computing",
    "section": "",
    "text": "i.e. a set of tools/machines used by a worker to complete a task↩︎\noverhead on resource access↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html",
    "href": "Courses/03_Asynchronous.html",
    "title": "4  Asynchronous Programming with Python",
    "section": "",
    "text": "5 Asynchronous, Basics",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#what-is-asynchronous-programming",
    "href": "Courses/03_Asynchronous.html#what-is-asynchronous-programming",
    "title": "4  Asynchronous Programming with Python",
    "section": "5.1 What is Asynchronous Programming?",
    "text": "5.1 What is Asynchronous Programming?\n\nAsynchronous programming is a programming paradigm that allows the program to continue executing other tasks before the current task is finished.\nIt is a way to achieve concurrency in a program.\n\n\\Rightarrow it is an abstraction over concurrency, it does not necessarily mean that the program is executed in parallel.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#io-bound-vs.-cpu-bound",
    "href": "Courses/03_Asynchronous.html#io-bound-vs.-cpu-bound",
    "title": "4  Asynchronous Programming with Python",
    "section": "5.2 I/O Bound vs. CPU Bound",
    "text": "5.2 I/O Bound vs. CPU Bound\nimport requests\n \n1response = requests.get('https://www.example.com')\n \nitems = response.headers.items()\n \n2headers = [f'{key}: {header}' for key, header in items]\n \n3formatted_headers = '\\n'.join(headers)\n \nwith open('headers.txt', 'w') as file:\n4    file.write(formatted_headers)\n\n1\n\nI/O-bound web request\n\n2\n\nCPU-bound response processing\n\n3\n\nCPU-bound string concatenation\n\n4\n\nI/O-bound write to disk",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#concurrency-vs.-parallelism",
    "href": "Courses/03_Asynchronous.html#concurrency-vs.-parallelism",
    "title": "4  Asynchronous Programming with Python",
    "section": "6.1 Concurrency vs. Parallelism",
    "text": "6.1 Concurrency vs. Parallelism\n\n\nOne baker and two cakes to prepare.\n\nCan preheat the oven while preparing the first cake.\nCan start the second cake while the first one is in the oven.\n\n\\Rightarrow Switching between tasks is concurrency (or concurrent behavior).\n\nTwo bakers and two cakes to prepare.\n\nCan prepare both cakes at the same time.\n\n\\Rightarrow Doing multiple tasks in parallel is parallelism (or parallel behavior).\n\n\n\n\nWith concurrency, we have multiple tasks happening at the same time, but only one we’re actively doing at a given point in time. With parallelism, we have multiple tasks happening and are actively doing more than one simultaneously.\n\n\n\n\nFrom Fowler (2022)\n\n\n\nWith concurrency, we switch between running two applications. With parallelism, we actively run two applications simultaneously.\n\n\n\n\nFrom Fowler (2022)\n\n\nConcurrency is about multiple independent tasks that can happen.\nParallelism is concurrency AND simultaneous execution.\n\nWhile parallelism implies concurrency, concurrency does not always imply parallelism.\n\\Rightarrow Concurrency is a broader concept than parallelism.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#multitasking",
    "href": "Courses/03_Asynchronous.html#multitasking",
    "title": "4  Asynchronous Programming with Python",
    "section": "6.2 Multitasking",
    "text": "6.2 Multitasking\n\n\n\n6.2.1 Preemptive multitasking\n\nThe operating system decides when to switch between tasks.\nThe tasks are not aware of each other.\n\n\n\n\n6.2.2 Cooperative multitasking\n\nIn this model we have to explicitly to decide when to switch between tasks.\nThe tasks are aware of each other.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#benefits-of-cooperative-multitasking",
    "href": "Courses/03_Asynchronous.html#benefits-of-cooperative-multitasking",
    "title": "4  Asynchronous Programming with Python",
    "section": "6.3 Benefits of cooperative multitasking",
    "text": "6.3 Benefits of cooperative multitasking\n\nLess overhead than preemptive multitasking.\nGranular/optimal control over when to switch between tasks.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#multi-processing-vs-multi-threading",
    "href": "Courses/03_Asynchronous.html#multi-processing-vs-multi-threading",
    "title": "4  Asynchronous Programming with Python",
    "section": "7.1 Multi-processing vs Multi-threading",
    "text": "7.1 Multi-processing vs Multi-threading\n\n\n\n\n\nMulti-Processing\n\n\n\n\n\n\n\nMulti-Threading",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#processes-and-threads",
    "href": "Courses/03_Asynchronous.html#processes-and-threads",
    "title": "4  Asynchronous Programming with Python",
    "section": "7.2 Processes and threads",
    "text": "7.2 Processes and threads\n\nimport os\nimport threading\n \nprint(f'Python process running with process id: {os.getpid()}')\ntotal_threads = threading.active_count()\nthread_name = threading.current_thread().name\n \nprint(f'Python is currently running {total_threads} thread(s)')\nprint(f'The current thread is {thread_name}')\n\nPython process running with process id: 89280\nPython is currently running 8 thread(s)\nThe current thread is MainThread",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#creating-processes",
    "href": "Courses/03_Asynchronous.html#creating-processes",
    "title": "4  Asynchronous Programming with Python",
    "section": "7.3 Creating processes",
    "text": "7.3 Creating processes\n\nimport multiprocessing\nimport os\n \n \ndef hello_from_process():\n    print(f'Hello from child process {os.getpid()}!')\nif __name__ == '__main__':\n    hello_process = multiprocessing.Process(target=hello_from_process)\n    hello_process.start()\n \n    print(f'Hello from parent process {os.getpid()}')\n \n    hello_process.join()\n\nHello from parent process 89280\nHello from child process 89329!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#creating-threads",
    "href": "Courses/03_Asynchronous.html#creating-threads",
    "title": "4  Asynchronous Programming with Python",
    "section": "7.4 Creating threads",
    "text": "7.4 Creating threads\n\nimport threading\n \n \ndef hello_from_thread():\n    print(f'Hello from thread {threading.current_thread()}!')\n \n \nhello_thread = threading.Thread(target=hello_from_thread)\nhello_thread.start()\n \ntotal_threads = threading.active_count()\nthread_name = threading.current_thread().name\n \nprint(f'Python is currently running {total_threads} thread(s)')\nprint(f'The current thread is {thread_name}')\n \nhello_thread.join()\n\nHello from thread &lt;Thread(Thread-6 (hello_from_thread), started 6226817024)&gt;!Python is currently running 9 thread(s)\nThe current thread is MainThread",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#what-about-python",
    "href": "Courses/03_Asynchronous.html#what-about-python",
    "title": "4  Asynchronous Programming with Python",
    "section": "8.1 What about Python?",
    "text": "8.1 What about Python?\n\n\nDesigned for sequential and single-core architecture from the beginning\nEverything is interpreted\nAll dynamic (no static types)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#the-gil",
    "href": "Courses/03_Asynchronous.html#the-gil",
    "title": "4  Asynchronous Programming with Python",
    "section": "8.2 The GIL",
    "text": "8.2 The GIL\nAka Global Interpreter Lock\n. . .\n\nThe GIL allows thread usage, you can create threads and launch them: YES!\n\n. . .\n\nbut…\n\n. . .\n\nOnly ONE thread can actually execute code at python level..",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#multi-threaded-parallel-execution",
    "href": "Courses/03_Asynchronous.html#multi-threaded-parallel-execution",
    "title": "4  Asynchronous Programming with Python",
    "section": "8.3 Multi-threaded != Parallel execution",
    "text": "8.3 Multi-threaded != Parallel execution\nMulti-threading doesn’t guarantee parallel execution…\n\n\n\n\n\n\\Longrightarrow Python seems to have started off with the wrong foot by a long way…",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#high-performance-python",
    "href": "Courses/03_Asynchronous.html#high-performance-python",
    "title": "4  Asynchronous Programming with Python",
    "section": "8.4 High performance Python 😬",
    "text": "8.4 High performance Python 😬\n\n\n\n\n\n\nBut wait!\n\n\nActually we can run (real) parallel programs with the multiprocessing package.\n\\Rightarrow But this is an “OS level” multiprocessing, with associated huge overhead (relatively)\nPython actually releases the GIL when executing everything that is not Python code (e.g. C/C++ extensions and libraries)\n\\Rightarrow It means we can parallelize our code by using I/O bound and CPU bound libraries that release the GIL (this is the case for most of them)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#socket",
    "href": "Courses/03_Asynchronous.html#socket",
    "title": "4  Asynchronous Programming with Python",
    "section": "9.1 Socket",
    "text": "9.1 Socket\n\n\nWriting bytes to a socket and reading bytes from a socket\n\n\n\n\nFrom Fowler (2022)\n\n\nThis a mailbox metaphor\nBy default, the socket is blocking, i.e. the program will wait until the socket is ready to be read or written.\nWe can make the socket non-blocking, i.e. the program will not wait for the socket to be ready to be read or written. \\Rightarrow Later on, the OS will tell us we received byte and we deal with it.\n\n\n\n\n\n\n\nMaking a non-blocking I/O request returns immediately\ntells the O/S to watch sockets for data \\Rightarrow This allows execute_other_code() to run right away instead of waiting for the I/O requests to finish\nLater, we can be alerted when I/O is complete and process the response.\n\n\n\n\n\nFrom Fowler (2022)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#event-loop",
    "href": "Courses/03_Asynchronous.html#event-loop",
    "title": "4  Asynchronous Programming with Python",
    "section": "9.2 Event Loop",
    "text": "9.2 Event Loop\n\n\nfrom collections import deque\n \nmessages = deque()\n \nwhile True:\n    if messages:\n        message = messages.pop()\n        process_message(message)\n\n\n\nThe event loop is a loop that runs forever.\nIt checks if there are any messages to process.\nIf there are, it processes them.\nIf there are not, it waits for messages to arrive.\n\n\n\n\n\\Rightarrow In asyncio, the event loop is queue of tasks instead of messages, Tasks are wrapped coroutines.\n\ndef make_request():\n    cpu_bound_setup()\n    io_bound_web_request()\n    cpu_bound_postprocess()\n \ntask_one = make_request()\ntask_two = make_request()\ntask_three = make_request()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#what-is-it",
    "href": "Courses/03_Asynchronous.html#what-is-it",
    "title": "4  Asynchronous Programming with Python",
    "section": "10.1 What is it?",
    "text": "10.1 What is it?\n\nasync def coroutine_add_one(number: int) -&gt; int:\n    return number + 1\n \ndef add_one(number: int) -&gt; int:\n    return number + 1\n \n1function_result = add_one(1)\n2coroutine_result = coroutine_add_one(1)\n \nprint(f'Function result is {function_result}\\n\\\n    and the type is {type(function_result)}')\nprint(f'Coroutine result is {coroutine_result}\\n\\\n    and the type is {type(coroutine_result)}')\n\n\n1\n\nfunction call, is executed immediately.\n\n2\n\ncoroutine call, is not executed at all, but returns a coroutine object.\n\n\n\n\nFunction result is 2\n    and the type is &lt;class 'int'&gt;\nCoroutine result is &lt;coroutine object coroutine_add_one at 0x121356440&gt;\n    and the type is &lt;class 'coroutine'&gt;\n\n\n\nFrom Fowler (2022)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#how-to-execute-a-coroutine",
    "href": "Courses/03_Asynchronous.html#how-to-execute-a-coroutine",
    "title": "4  Asynchronous Programming with Python",
    "section": "10.2 How to execute a coroutine?",
    "text": "10.2 How to execute a coroutine?\nYou need an event loop.\nimport asyncio\n \nasync def coroutine_add_one(number: int) -&gt; int:\n    return number + 1\n \n1result = asyncio.run(coroutine_add_one(1))\n\nprint(result)\n\n1\n\nThis launches the event loop, executes the coroutine, and returns the result.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis code will not work in a Jupyter notebook, because the event loop is already running (by Jupyter itself). So you just have to replace the line 4 by:\nresult = await coroutine_add_one(1)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#await-keyword",
    "href": "Courses/03_Asynchronous.html#await-keyword",
    "title": "4  Asynchronous Programming with Python",
    "section": "10.3 await keyword",
    "text": "10.3 await keyword\n\nimport asyncio\n \nasync def add_one(number: int) -&gt; int:\n    return number + 1\n \n \nasync def main() -&gt; None:\n1    one_plus_one = await add_one(1)\n2    two_plus_one = await add_one(2)\n    print(one_plus_one)\n    print(two_plus_one)\n \n3await main()\n\n\n1\n\nPause, and wait for the result of add_one(1).\n\n2\n\nPause, and wait for the result of add_one(2).\n\n3\n\nPause, and wait for the result of main(). (outside of a Jupyter notebook, you have to launch the event loop somewhere, like asyncio.run(main()) instead of await main())\n\n\n\n\n2\n3\n\n\n\n\nFrom Fowler (2022)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#simulating-the-real-thing-with-asyncio.sleep",
    "href": "Courses/03_Asynchronous.html#simulating-the-real-thing-with-asyncio.sleep",
    "title": "4  Asynchronous Programming with Python",
    "section": "10.4 Simulating the real thing with asyncio.sleep",
    "text": "10.4 Simulating the real thing with asyncio.sleep\n\nimport asyncio\n \nasync def hello_world_message() -&gt; str:\n1    await asyncio.sleep(1)\n    return 'Hello World!'\n \nasync def main() -&gt; None:\n2    message = await hello_world_message()\n    print(message)\n \nawait main()\n\n\n1\n\nPause hello_world_message for 1 second.\n\n2\n\nPause main until hello_world_message is finished.\n\n\n\n\nHello World!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#utility-function-delayseconds",
    "href": "Courses/03_Asynchronous.html#utility-function-delayseconds",
    "title": "4  Asynchronous Programming with Python",
    "section": "10.5 Utility function delay(seconds)",
    "text": "10.5 Utility function delay(seconds)\n\nimport asyncio\n \n \n1async def delay(delay_seconds: int) -&gt; int:\n2    print(f'sleeping for {delay_seconds} second(s)')\n    await asyncio.sleep(delay_seconds)\n    print(f'finished sleeping for {delay_seconds} second(s)')\n3    return delay_seconds\n\n\n1\n\nTakes an integer of the duration in seconds that we’d like the function to sleep.\n\n2\n\nPrints when sleep begins and ends.\n\n3\n\nReturns that integer to the caller once it has finished sleeping.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#running-two-coroutines",
    "href": "Courses/03_Asynchronous.html#running-two-coroutines",
    "title": "4  Asynchronous Programming with Python",
    "section": "10.6 Running two coroutines",
    "text": "10.6 Running two coroutines\n\nimport asyncio\n \nasync def add_one(number: int) -&gt; int:\n    return number + 1\n \nasync def hello_world_message() -&gt; str:\n    await delay(1)\n    return 'Hello World!'\n \nasync def main() -&gt; None:\n1    message = await hello_world_message()\n2    one_plus_one = await add_one(1)\n    print(one_plus_one)\n    print(message)\n \nawait main()\n\n\n1\n\nPause main until hello_world_message is finished.\n\n2\n\nPause main until add_one is finished.\n\n\n\n\nsleeping for 1 second(s)\nfinished sleeping for 1 second(s)\n2\nHello World!\n\n\n\n\nFrom Fowler (2022)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#what-to-do-next",
    "href": "Courses/03_Asynchronous.html#what-to-do-next",
    "title": "4  Asynchronous Programming with Python",
    "section": "10.7 What to do next?",
    "text": "10.7 What to do next?\nMoving away from sequential execution and run add_one and hello_world_message concurrently.\nFor that we need…",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#creating-tasks",
    "href": "Courses/03_Asynchronous.html#creating-tasks",
    "title": "4  Asynchronous Programming with Python",
    "section": "11.1 Creating tasks",
    "text": "11.1 Creating tasks\n\nimport asyncio\n\nasync def main():\n    sleep_for_three = asyncio.create_task(delay(3))\n    print(type(sleep_for_three))\n    result = await sleep_for_three\n    print(result)\n \nawait main()\n\n&lt;class '_asyncio.Task'&gt;\nsleeping for 3 second(s)\nfinished sleeping for 3 second(s)\n3\n\n\n\nthe coroutine is scheduled to run in the event loop as soon as possible.\nbefore, it was just run at the await statement (pausing the caller).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#running-tasks-concurrently",
    "href": "Courses/03_Asynchronous.html#running-tasks-concurrently",
    "title": "4  Asynchronous Programming with Python",
    "section": "11.2 Running tasks concurrently",
    "text": "11.2 Running tasks concurrently\n\nimport asyncio\n \nasync def main():\n    sleep_for_three = \\\n        asyncio.create_task(delay(3))\n    sleep_again = \\\n        asyncio.create_task(delay(3))\n    sleep_once_more = \\\n        asyncio.create_task(delay(3))\n \n    await sleep_for_three\n    await sleep_again\n    await sleep_once_more\n\nawait main()\n\nsleeping for 3 second(s)\nsleeping for 3 second(s)\nsleeping for 3 second(s)\nfinished sleeping for 3 second(s)\nfinished sleeping for 3 second(s)\nfinished sleeping for 3 second(s)\n\n\n\n\nFrom Fowler (2022)\n\n\nimport asyncio\n \nasync def hello_every_second():\n    for i in range(2):\n        await asyncio.sleep(1)\n        print(\"I'm running other code while I'm waiting!\")\n \nasync def main():\n    first_delay = asyncio.create_task(delay(3))\n    second_delay = asyncio.create_task(delay(3))\n    await hello_every_second()\n    await first_delay\n    await second_delay\n\nawait main()\n\nsleeping for 3 second(s)\nsleeping for 3 second(s)\nI'm running other code while I'm waiting!\nI'm running other code while I'm waiting!\nfinished sleeping for 3 second(s)\nfinished sleeping for 3 second(s)\n\n\n\n\nFrom Fowler (2022)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#canceling-tasks",
    "href": "Courses/03_Asynchronous.html#canceling-tasks",
    "title": "4  Asynchronous Programming with Python",
    "section": "11.3 Canceling tasks",
    "text": "11.3 Canceling tasks\n\nimport asyncio\nfrom asyncio import CancelledError\n\nasync def main():\n    long_task = asyncio.create_task(delay(10))\n \n    seconds_elapsed = 0\n \n    while not long_task.done():\n        print('Task not finished, checking again in a second.')\n        await asyncio.sleep(1)\n        seconds_elapsed = seconds_elapsed + 1\n        if seconds_elapsed == 5:\n            long_task.cancel()\n \n    try:\n        await long_task\n    except CancelledError:\n        print('Our task was cancelled')\n \nawait main()\n\nTask not finished, checking again in a second.\nsleeping for 10 second(s)\nTask not finished, checking again in a second.\nTask not finished, checking again in a second.\nTask not finished, checking again in a second.\nTask not finished, checking again in a second.\nTask not finished, checking again in a second.\nOur task was cancelled",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#setting-a-timeout",
    "href": "Courses/03_Asynchronous.html#setting-a-timeout",
    "title": "4  Asynchronous Programming with Python",
    "section": "11.4 Setting a timeout",
    "text": "11.4 Setting a timeout\n\nimport asyncio\n\nasync def main():\n    delay_task = asyncio.create_task(delay(2))\n    try:\n        result = await asyncio.wait_for(delay_task, timeout=1)\n        print(result)\n    except asyncio.exceptions.TimeoutError:\n        print('Got a timeout!')\n        print(f'Was the task cancelled? {delay_task.cancelled()}')\n \nawait main()\n\nsleeping for 2 second(s)\nGot a timeout!\nWas the task cancelled? True",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#introducing-futures",
    "href": "Courses/03_Asynchronous.html#introducing-futures",
    "title": "4  Asynchronous Programming with Python",
    "section": "12.1 Introducing futures",
    "text": "12.1 Introducing futures\n\nfrom asyncio import Future\n \nmy_future = Future()\n \nprint(f'Is my_future done? {my_future.done()}')\n \nmy_future.set_result(42)\n \nprint(f'Is my_future done? {my_future.done()}')\nprint(f'What is the result of my_future? {my_future.result()}')\n\nIs my_future done? False\nIs my_future done? True\nWhat is the result of my_future? 42",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#awaiting-futures",
    "href": "Courses/03_Asynchronous.html#awaiting-futures",
    "title": "4  Asynchronous Programming with Python",
    "section": "12.2 Awaiting futures",
    "text": "12.2 Awaiting futures\n\nfrom asyncio import Future\nimport asyncio\n \n \ndef make_request() -&gt; Future:\n    future = Future()\n1    asyncio.create_task(set_future_value(future))\n    return future\n \n \nasync def set_future_value(future) -&gt; None:\n2    await asyncio.sleep(1)\n    future.set_result(42)\n \n \nasync def main():\n    future = make_request()\n    print(f'Is the future done? {future.done()}')\n3    value = await future\n    print(f'Is the future done? {future.done()}')\n    print(value)\n \nawait main()\n\n\n1\n\nCreate a task to asynchronously set the value of the future.\n\n2\n\nWait 1 second before setting the value of the future.\n\n3\n\nPause main until the future’s value is set.\n\n\n\n\nIs the future done? False\nIs the future done? True\n42",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#comparing-tasks-coroutines-futures-and-awaitables",
    "href": "Courses/03_Asynchronous.html#comparing-tasks-coroutines-futures-and-awaitables",
    "title": "4  Asynchronous Programming with Python",
    "section": "12.3 Comparing tasks, coroutines, futures, and awaitables",
    "text": "12.3 Comparing tasks, coroutines, futures, and awaitables\n\n\n\n\n\n\n\nAwaitables\n\nObjects that can be awaited in an async function, including coroutines, tasks, and futures.\n\nCoroutines\n\nSpecial functions that can be paused and resumed later, defined using async def, and can be awaited to allow other coroutines to run.\n\nFutures\n\nRepresent the result of an asynchronous operation, manage its state, and can be awaited to get the result.\n\nTasks\n\nSchedule and run coroutines concurrently, and can be used to cancel or check their status.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#with-a-decorator",
    "href": "Courses/03_Asynchronous.html#with-a-decorator",
    "title": "4  Asynchronous Programming with Python",
    "section": "13.1 With a decorator",
    "text": "13.1 With a decorator\n\n\n\nimport functools\nimport time\nfrom typing import Callable, Any\n \ndef async_timed():\n    def wrapper(func: Callable) -&gt; Callable:\n        @functools.wraps(func)\n        async def wrapped(*args, **kwargs) -&gt; Any:\n            print(f'starting {func} with args {args} {kwargs}')\n            start = time.time()\n            try:\n                return await func(*args, **kwargs)\n            finally:\n                end = time.time()\n                total = end - start\n                print(f'finished {func} in {total:.4f} second(s)')\n \n        return wrapped\n \n    return wrapper\n\n\nOfficial Python documentation for decorators\n\nadd functionality to an existing function\nwithout modifying the function itself\nit intercepts the function call and runs “decorated” code before and after it",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#using-it",
    "href": "Courses/03_Asynchronous.html#using-it",
    "title": "4  Asynchronous Programming with Python",
    "section": "13.2 Using it",
    "text": "13.2 Using it\n\nimport asyncio\n \n@async_timed()\nasync def delay(delay_seconds: int) -&gt; int:\n    print(f'sleeping for {delay_seconds} second(s)')\n    await asyncio.sleep(delay_seconds)\n    print(f'finished sleeping for {delay_seconds} second(s)')\n    return delay_seconds\n \n \n@async_timed()\nasync def main():\n    task_one = asyncio.create_task(delay(2))\n    task_two = asyncio.create_task(delay(3))\n \n    await task_one\n    await task_two\n\nawait main()\n\nstarting &lt;function main at 0x1213b5120&gt; with args () {}\nstarting &lt;function delay at 0x1213b47c0&gt; with args (2,) {}\nsleeping for 2 second(s)\nstarting &lt;function delay at 0x1213b47c0&gt; with args (3,) {}\nsleeping for 3 second(s)\nfinished sleeping for 2 second(s)\nfinished &lt;function delay at 0x1213b47c0&gt; in 2.0011 second(s)\nfinished sleeping for 3 second(s)\nfinished &lt;function delay at 0x1213b47c0&gt; in 3.0013 second(s)\nfinished &lt;function main at 0x1213b5120&gt; in 3.0016 second(s)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#asyncio.gather",
    "href": "Courses/03_Asynchronous.html#asyncio.gather",
    "title": "4  Asynchronous Programming with Python",
    "section": "13.3 asyncio.gather",
    "text": "13.3 asyncio.gather\nasyncio.gather() runs multiple asynchronous operations, wraps a coroutine as a task, and returns a list of results in the same order of awaitables.\n\nimport asyncio\n\n\nasync def call_api(message, result, delay=3):\n    print(message)\n    await asyncio.sleep(delay)\n    return result\n\n\nasync def main():\n    return await asyncio.gather(\n        call_api('Calling API 1 ...', 1),\n        call_api('Calling API 2 ...', 2)\n    )\n\nawait main()\n\nCalling API 1 ...\nCalling API 2 ...\n\n\n[1, 2]\n\n\n\n\n\n\n\n\nCaution\n\n\n\nasyncio.gather takes a tuple of awaitables, not a list of awaitables, but returns a list of results in the same order of awaitables.\nIf you want to pass a list, use the * operator to unpack it as a tuple.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#running-cpu-bound-code",
    "href": "Courses/03_Asynchronous.html#running-cpu-bound-code",
    "title": "4  Asynchronous Programming with Python",
    "section": "14.1 Running CPU-bound code",
    "text": "14.1 Running CPU-bound code\n\nimport asyncio\n\n@async_timed()\nasync def cpu_bound_work() -&gt; int:\n    counter = 0\n    for i in range(100000000):\n        counter = counter + 1\n    return counter\n \n \n@async_timed()\nasync def main():\n    task_one = asyncio.create_task(cpu_bound_work())\n    task_two = asyncio.create_task(cpu_bound_work())\n    await task_one\n    await task_two\n \nawait main()\n\nstarting &lt;function main at 0x1213b5940&gt; with args () {}\nstarting &lt;function cpu_bound_work at 0x1213b5760&gt; with args () {}\nfinished &lt;function cpu_bound_work at 0x1213b5760&gt; in 1.2570 second(s)\nstarting &lt;function cpu_bound_work at 0x1213b5760&gt; with args () {}\nfinished &lt;function cpu_bound_work at 0x1213b5760&gt; in 1.2420 second(s)\nfinished &lt;function main at 0x1213b5940&gt; in 2.4994 second(s)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#running-blocking-apis",
    "href": "Courses/03_Asynchronous.html#running-blocking-apis",
    "title": "4  Asynchronous Programming with Python",
    "section": "14.2 Running blocking APIs",
    "text": "14.2 Running blocking APIs\n\nimport asyncio\nimport requests\n \n@async_timed()\nasync def get_example_status() -&gt; int:\n    return requests.get('http://www.example.com').status_code\n \n \n@async_timed()\nasync def main():\n    task_1 = asyncio.create_task(get_example_status())\n    task_2 = asyncio.create_task(get_example_status())\n    task_3 = asyncio.create_task(get_example_status())\n    await task_1\n    await task_2\n    await task_3\n \nawait main()\n\nstarting &lt;function main at 0x121a04400&gt; with args () {}\nstarting &lt;function get_example_status at 0x1213b56c0&gt; with args () {}\nfinished &lt;function get_example_status at 0x1213b56c0&gt; in 0.1187 second(s)\nstarting &lt;function get_example_status at 0x1213b56c0&gt; with args () {}\nfinished &lt;function get_example_status at 0x1213b56c0&gt; in 0.0649 second(s)\nstarting &lt;function get_example_status at 0x1213b56c0&gt; with args () {}\nfinished &lt;function get_example_status at 0x1213b56c0&gt; in 0.0680 second(s)\nfinished &lt;function main at 0x121a04400&gt; in 0.2522 second(s)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#example-of-blocking-code",
    "href": "Courses/03_Asynchronous.html#example-of-blocking-code",
    "title": "4  Asynchronous Programming with Python",
    "section": "15.1 Example of blocking code",
    "text": "15.1 Example of blocking code\n\nimport requests\n \n \ndef get_status_code(url: str) -&gt; int:\n    response = requests.get(url)\n    return response.status_code\n \n \nurl = 'https://www.example.com'\nprint(get_status_code(url))\nprint(get_status_code(url))\n\n200\n200",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#thread-pool",
    "href": "Courses/03_Asynchronous.html#thread-pool",
    "title": "4  Asynchronous Programming with Python",
    "section": "15.2 Thread Pool",
    "text": "15.2 Thread Pool\n\nimport time\nimport requests\nfrom concurrent.futures import ThreadPoolExecutor\n \n \ndef get_status_code(url: str) -&gt; int:\n    response = requests.get(url)\n    return response.status_code\n \n \nstart = time.time()\n \nwith ThreadPoolExecutor() as pool:\n    urls = ['https://www.example.com' for _ in range(10)]\n    results = pool.map(get_status_code, urls)\n    for result in results:\n        # print(result)\n        pass\n\n \nend = time.time()\n \nprint(f'finished requests in {end - start:.4f} second(s)')\n\nfinished requests in 0.8679 second(s)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#compare-with-sequential-code",
    "href": "Courses/03_Asynchronous.html#compare-with-sequential-code",
    "title": "4  Asynchronous Programming with Python",
    "section": "15.3 Compare with sequential code",
    "text": "15.3 Compare with sequential code\n\nstart = time.time()\n \nurls = ['https://www.example.com' for _ in range(10)]\n \nfor url in urls:\n    result = get_status_code(url)\n    # print(result)\n \nend = time.time()\n \nprint(f'finished requests in {end - start:.4f} second(s)')\n\nfinished requests in 1.4278 second(s)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#thread-pool-with-asyncio",
    "href": "Courses/03_Asynchronous.html#thread-pool-with-asyncio",
    "title": "4  Asynchronous Programming with Python",
    "section": "15.4 Thread pool with asyncio",
    "text": "15.4 Thread pool with asyncio\n\nimport functools\nimport requests\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor\n \ndef get_status_code(url: str) -&gt; int:\n    response = requests.get(url)\n    return response.status_code\n \n \n@async_timed()\nasync def main():\n    loop = asyncio.get_running_loop()\n    with ThreadPoolExecutor() as pool:\n        urls = ['https://www.example.com' for _ in range(10)]\n        tasks = [loop.run_in_executor(pool, functools.partial(get_status_code, url)) for url in urls]\n        results = await asyncio.gather(*tasks)\n        print(results)\n \nawait main()\n\nstarting &lt;function main at 0x1213b5940&gt; with args () {}\n[200, 200, 200, 200, 200, 200, 200, 200, 200, 200]\nfinished &lt;function main at 0x1213b5940&gt; in 0.8599 second(s)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/03_Asynchronous.html#multithreading-with-numpy",
    "href": "Courses/03_Asynchronous.html#multithreading-with-numpy",
    "title": "4  Asynchronous Programming with Python",
    "section": "15.5 Multithreading with numpy",
    "text": "15.5 Multithreading with numpy\nLet’s define a big matrix on which we will compute the mean of each row.\nNow process the matrix sequentially.\n\ns = time.time()\n \nres_seq = np.mean(matrix, axis=1)\n \ne = time.time()\nprint(e - s)\n\n0.30220913887023926\n\n\nAnd then the same with multithreading (we check that the results are exactly the same).\n\nimport functools\nfrom concurrent.futures.thread import ThreadPoolExecutor\nimport asyncio\n \ndef mean_for_row(arr, row):\n    return np.mean(arr[row])\n \n@async_timed()\nasync def main():\n    loop = asyncio.get_running_loop()\n    with ThreadPoolExecutor() as pool:\n        tasks = []\n        for i in range(rows):\n            mean = functools.partial(mean_for_row, matrix, i)\n            tasks.append(loop.run_in_executor(pool, mean))\n \n        return await asyncio.gather(*tasks)\n\nres_threads = np.array(await main())\nnp.testing.assert_array_equal(res_seq, res_threads)\n\nstarting &lt;function main at 0x121a05580&gt; with args () {}\nfinished &lt;function main at 0x121a05580&gt; in 0.0163 second(s)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html",
    "href": "Courses/04_IPC-and-Locking.html",
    "title": "5  IPC and locking",
    "section": "",
    "text": "6 Inter-Process Communication",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#remainder-on-process-level-parallelization",
    "href": "Courses/04_IPC-and-Locking.html#remainder-on-process-level-parallelization",
    "title": "5  IPC and locking",
    "section": "6.1 Remainder on Process-level parallelization",
    "text": "6.1 Remainder on Process-level parallelization\n\n\n\n\n\nMulti-Processing\n\n\n\n\n\n\n\nMulti-Threading",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#inter-process-is-easy",
    "href": "Courses/04_IPC-and-Locking.html#inter-process-is-easy",
    "title": "5  IPC and locking",
    "section": "6.2 Inter-process is easy…",
    "text": "6.2 Inter-process is easy…\n\n\nBut if my algorithm is not “embarrassingly parallel”, what if we want to share data between processes ?\nlet’s go for Shared Memory",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#shared-memory-model",
    "href": "Courses/04_IPC-and-Locking.html#shared-memory-model",
    "title": "5  IPC and locking",
    "section": "6.3 Shared Memory Model",
    "text": "6.3 Shared Memory Model\n\n\nShared Memory Model",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#aside-memory-models",
    "href": "Courses/04_IPC-and-Locking.html#aside-memory-models",
    "title": "5  IPC and locking",
    "section": "6.4 Aside : memory models",
    "text": "6.4 Aside : memory models\n\n\n\n\n\nUMA\n\n\n\n\n\n \n\n\n\n\nNUMA\n\n\n\n\n\n\n\nThere are differents models",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#shared-fifos-queues",
    "href": "Courses/04_IPC-and-Locking.html#shared-fifos-queues",
    "title": "5  IPC and locking",
    "section": "6.5 Shared FIFOs : Queues",
    "text": "6.5 Shared FIFOs : Queues\nAn ubiquitous tool in multiprocessing (and distributed computing) is shared memory FIFO list, aka Queues.\nA FIFO is a :\n\nLinked list\nwith FIFO (First In First Out) semantics, with enqueue(x) et dequeue() function (or push(x)/pop())\n\n\n\n\n\n\nIn the context of multi-processing (or multi-threading) :\nShared Memory + FIFO list = Queue\nQueues are the basis of the consumer/producer model, which is widely used in concurrent and distributed applications.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#when-to-use-queues",
    "href": "Courses/04_IPC-and-Locking.html#when-to-use-queues",
    "title": "5  IPC and locking",
    "section": "6.6 When to use queues?",
    "text": "6.6 When to use queues?\nAn algorithm with two computations A and B where :\n\nB depends on the result of A\nA is independent of B\n\n. . .\nA could be a producer for B, and B a consumer for A.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#how-to-use-queues",
    "href": "Courses/04_IPC-and-Locking.html#how-to-use-queues",
    "title": "5  IPC and locking",
    "section": "6.7 How to use queues?",
    "text": "6.7 How to use queues?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#producerconsumer-examples",
    "href": "Courses/04_IPC-and-Locking.html#producerconsumer-examples",
    "title": "5  IPC and locking",
    "section": "6.8 Producer/consumer, Examples",
    "text": "6.8 Producer/consumer, Examples\n\nA finds primes in a list of number, B formats and prints them every 10 numbers found.\nA fetches a bunch of images on the web, B downloads them and saves them to disk.\nA takes the orders in the restaurant, B cooks them.\n\n. . .",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#the-main-gotcha",
    "href": "Courses/04_IPC-and-Locking.html#the-main-gotcha",
    "title": "5  IPC and locking",
    "section": "7.1 The main gotcha",
    "text": "7.1 The main gotcha\nwhat if several processes want to write/read the same shared memory portions at the same time?\n. . .\nEnter the realm of the dreaded race condition",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#simple-example",
    "href": "Courses/04_IPC-and-Locking.html#simple-example",
    "title": "5  IPC and locking",
    "section": "7.2 Simple example",
    "text": "7.2 Simple example\nPrinting from several processes a string with 10 times the same char.\n\n\nfrom multiprocessing.pool import Pool\nfrom itertools import repeat\n# print \"AAAAAAAAA\", \"BBBBBBBBBBB\" etc.\ndef repeat10Cap(c): \n    print(\"\".join(repeat(chr(c+65),10))) \nwith Pool(8) as pool:\n    pool.map(repeat10Cap, range(10))\n\n\nOutput:\nAAAAAAAAAACCCCCCCCCCBBBBBBBBBBDDDDDDDDDDEEEEEEEEEE\n\n\nFFFFFFFFFFGGGGGGGGGG\nIIIIIIIIII\n\nHHHHHHHHHH\nJJJJJJJJJJ",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#critical-section-workflow",
    "href": "Courses/04_IPC-and-Locking.html#critical-section-workflow",
    "title": "5  IPC and locking",
    "section": "8.1 Critical section workflow",
    "text": "8.1 Critical section workflow\n\n\nThree processes with critical section",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#a-simple-implementation-in-python-lock",
    "href": "Courses/04_IPC-and-Locking.html#a-simple-implementation-in-python-lock",
    "title": "5  IPC and locking",
    "section": "8.2 A simple implementation in Python : Lock",
    "text": "8.2 A simple implementation in Python : Lock\n\n\nfrom multiprocessing.pool import Pool\nfrom multiprocessing import Lock\nfrom itertools import repeat\nlock = Lock()\ndef safe_repeat10Cap(c):\n    with lock: \n        # Beginning of critical section\n        print(\"\".join(repeat(chr(c+65),10)))\n        # End of critical section\nwith Pool(8) as pool:\n    pool.map(safe_repeat10Cap, range(10))\n\n\nOutput:\nAAAAAAAAAA\nBBBBBBBBBB\nCCCCCCCCCC\nDDDDDDDDDD\nEEEEEEEEEE\nFFFFFFFFFF\nGGGGGGGGGG\nHHHHHHHHHH\nIIIIIIIIII\nJJJJJJJJJJ",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#consistency-problems-with-fifo-example-i",
    "href": "Courses/04_IPC-and-Locking.html#consistency-problems-with-fifo-example-i",
    "title": "5  IPC and locking",
    "section": "9.1 Consistency problems with FIFO example I",
    "text": "9.1 Consistency problems with FIFO example I\nProcess A (resp. B) wants to push x (resp. y) on the list.\n\n\n\\Longrightarrow Consistency problem if they both create a new linked node to node 3.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#consistency-problems-with-fifo-example-2",
    "href": "Courses/04_IPC-and-Locking.html#consistency-problems-with-fifo-example-2",
    "title": "5  IPC and locking",
    "section": "9.2 Consistency problems with FIFO example 2",
    "text": "9.2 Consistency problems with FIFO example 2\nProcess A and B both want to pop the list.\n\n\n\\Longrightarrow Consistency problem if they both pop the same node.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#no-consistency-problems-with-fifo-example-3",
    "href": "Courses/04_IPC-and-Locking.html#no-consistency-problems-with-fifo-example-3",
    "title": "5  IPC and locking",
    "section": "9.3 (No) Consistency problems with FIFO example 3",
    "text": "9.3 (No) Consistency problems with FIFO example 3\n\n\nNo problem there.\n\n\n\n. . .\n\n\n\n\n\n\nWarning\n\n\n\n⚠ ⚠ As long the list is not empty ⚠ ⚠",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#deadlock-example",
    "href": "Courses/04_IPC-and-Locking.html#deadlock-example",
    "title": "5  IPC and locking",
    "section": "10.1 Deadlock example",
    "text": "10.1 Deadlock example",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#deadlock-serious-example",
    "href": "Courses/04_IPC-and-Locking.html#deadlock-serious-example",
    "title": "5  IPC and locking",
    "section": "10.2 Deadlock (serious) example",
    "text": "10.2 Deadlock (serious) example\n\n\nDeadlock illustration\n\n\n\n\nProcess A acquires lock L1. Process B acquires lock L2. Process A tries to acquire lock L2, but it is already held by B. Process B tries to acquire lock L1, but it is already held by A. Both processes are blocked.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/04_IPC-and-Locking.html#avoiding-deadlocks",
    "href": "Courses/04_IPC-and-Locking.html#avoiding-deadlocks",
    "title": "5  IPC and locking",
    "section": "10.3 Avoiding Deadlocks",
    "text": "10.3 Avoiding Deadlocks\nThere is several ways to avoid deadlocks. One of them is the Dijkstra’s Resource Hiearchy Solution.\n. . .\nIn the previous example, processes should try the lowest numbered locks first. Instead of B acquiring L2 first, it should tries to acquire L1 instead and L2 after.\n. . .\nThis solution isn’t universal but is pretty usable in general case.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/05_Distributed.html",
    "href": "Courses/05_Distributed.html",
    "title": "6  Distributed Computing models",
    "section": "",
    "text": "7 Map-Reduce",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/05_Distributed.html#the-real-beating-heart-of-big-data",
    "href": "Courses/05_Distributed.html#the-real-beating-heart-of-big-data",
    "title": "6  Distributed Computing models",
    "section": "7.1 The (real) beating Heart of Big Data",
    "text": "7.1 The (real) beating Heart of Big Data\nMap\\rightarrow{}Reduce pattern is the most common pattern to process data in (real) Big Data.\n\nIt is heavily used by Google, Facebook, and IBM.\n\n\nHadoop from Apache is a popular Map-Reduce framework (also called MapReduce in the Hadoop framework, not to be confused with the more general Map\\rightarrow{}Reduce Pattern).\n\n\nHadoop is backed by a HDFS (Hadoop Distributed File System) and a YARN (Yet Another Resource Manager)\n\n\n\nHDFS is a distributed file system (a file system that is distributed across a cluster of computers)\nYARN is a resource manager (a program that manages the resources of a cluster)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/05_Distributed.html#split-apply-combine-pattern",
    "href": "Courses/05_Distributed.html#split-apply-combine-pattern",
    "title": "6  Distributed Computing models",
    "section": "7.2 Split-Apply-Combine pattern",
    "text": "7.2 Split-Apply-Combine pattern\n\n\n\n\nSplit:\n\nSplit the data into smaller pieces\n\nApply:\n\nProcess the data in the pieces\n\nCombine:\n\nMerge the results",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/05_Distributed.html#map",
    "href": "Courses/05_Distributed.html#map",
    "title": "6  Distributed Computing models",
    "section": "7.3 Map",
    "text": "7.3 Map\nMap takes one pair of data with a type in one data domain, and returns a list of pairs in a different domain:\nMap(k1,v1) → list(k2,v2)\n\\Longrightarrow heavily parallelized",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/05_Distributed.html#reduce",
    "href": "Courses/05_Distributed.html#reduce",
    "title": "6  Distributed Computing models",
    "section": "7.4 Reduce",
    "text": "7.4 Reduce\nThe values associated from the same key are combined.\nThe Reduce function is then applied in parallel to each group, which in turn produces a collection of values in the same domain:\nReduce(k2, list (v2)) → list((k3, v3))",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/05_Distributed.html#schema",
    "href": "Courses/05_Distributed.html#schema",
    "title": "6  Distributed Computing models",
    "section": "7.5 Schema",
    "text": "7.5 Schema",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/05_Distributed.html#canonical-example-word-count-i",
    "href": "Courses/05_Distributed.html#canonical-example-word-count-i",
    "title": "6  Distributed Computing models",
    "section": "7.6 Canonical example : Word Count, I",
    "text": "7.6 Canonical example : Word Count, I\nThe canonical MapReduce example counts the appearance of each word in a set of documents\ndef map(name, document):\n  // name: document name\n  // document: document contents (list of words)\n  for word in document:\n    emit (word, 1)\n\ndef reduce(word, partialCounts):\n  // word: a word\n  // partialCounts: a list of aggregated partial counts\n  sum = 0\n  for pc in partialCounts:\n    sum += pc\n  emit (word, sum)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/05_Distributed.html#canonical-example-word-count-ii",
    "href": "Courses/05_Distributed.html#canonical-example-word-count-ii",
    "title": "6  Distributed Computing models",
    "section": "7.7 Canonical example : Word Count, II",
    "text": "7.7 Canonical example : Word Count, II",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/05_Distributed.html#spark-spiritual-son-of-mapreduce",
    "href": "Courses/05_Distributed.html#spark-spiritual-son-of-mapreduce",
    "title": "6  Distributed Computing models",
    "section": "7.8 Spark, spiritual son of MapReduce",
    "text": "7.8 Spark, spiritual son of MapReduce\nSpark is widely used for machine learning on scalable data sets (faster than MapReduce by an order of magnitude).\n\nSpark is largely inspired by the MapReduce pattern but extends it by using a distributed graph rather than a “linear” data flow like Map\\rightarrow{}Reduce.\n\n\n\\Longrightarrow Complex distributed computing.\n\n\nSpark emphasizes ease of use of the cluster resources in a simple and functional way",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/05_Distributed.html#spark-code-example-word-count",
    "href": "Courses/05_Distributed.html#spark-code-example-word-count",
    "title": "6  Distributed Computing models",
    "section": "7.9 Spark, code example : Word Count",
    "text": "7.9 Spark, code example : Word Count\ntext_file = sc.textFile(\"hdfs://...\")\ncounts = text_file.flatMap(lambda line: line.split(\" \")) \\\n             .map(lambda word: (word, 1)) \\\n             .reduceByKey(lambda a, b: a + b)\ncounts.saveAsTextFile(\"hdfs://...\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/05_Distributed.html#spark-another-example-machine-learning",
    "href": "Courses/05_Distributed.html#spark-another-example-machine-learning",
    "title": "6  Distributed Computing models",
    "section": "7.10 Spark, another example : machine learning",
    "text": "7.10 Spark, another example : machine learning\n# Every record of this DataFrame contains the label and\n# features represented by a vector.\ndf = sqlContext.createDataFrame(data, [\"label\", \"features\"])\n\n# Set parameters for the algorithm.\n# Here, we limit the number of iterations to 10.\nlr = LogisticRegression(maxIter=10)\n\n# Fit the model to the data.\nmodel = lr.fit(df)\n\n# Given a dataset, predict each point's label, \n# and show the results.\nmodel.transform(df).show()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/05_Distributed.html#schema-1",
    "href": "Courses/05_Distributed.html#schema-1",
    "title": "6  Distributed Computing models",
    "section": "8.1 Schema",
    "text": "8.1 Schema",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/05_Distributed.html#main-message-passing-functions",
    "href": "Courses/05_Distributed.html#main-message-passing-functions",
    "title": "6  Distributed Computing models",
    "section": "8.2 Main message-passing functions",
    "text": "8.2 Main message-passing functions\n\n\nScatter\n\npartition the data into smaller pieces and send them to the different processes\n\nGather\n\ncollect the data from the different processes and merge them.\n\nBroadcast\n\nSend the same data to all the processes.\n\nReduce\n\nMerge the data from all the processes and produce a single result.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/06_Dask-Ray.html",
    "href": "Courses/06_Dask-Ray.html",
    "title": "7  Dask and Ray",
    "section": "",
    "text": "8 Dask vs. Ray: Programming Paradigm Comparison\nHere’s a structured outline for your lecture comparing Dask and Ray from a programming paradigm perspective",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask and Ray</span>"
    ]
  },
  {
    "objectID": "Courses/06_Dask-Ray.html#dask",
    "href": "Courses/06_Dask-Ray.html#dask",
    "title": "7  Dask and Ray",
    "section": "8.1  Dask",
    "text": "8.1  Dask\n\n\n\n\nDask is a flexible parallel computing library for analytics.\nIntegrates seamlessly with NumPy, Pandas, and Scikit-learn.\nUses a task graph paradigm to break down computations into smaller tasks.\nEnables parallel execution of tasks for scalable analytics.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask and Ray</span>"
    ]
  },
  {
    "objectID": "Courses/06_Dask-Ray.html#section",
    "href": "Courses/06_Dask-Ray.html#section",
    "title": "7  Dask and Ray",
    "section": "8.2 ",
    "text": "8.2 \n\n\n\n\nRay is a distributed execution framework for building and running distributed applications.\nSupports a wide range of workloads, including machine learning and reinforcement learning.\nUses an actor model paradigm for stateful computations.\nAllows dynamic task scheduling and fine-grained control over distributed execution.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask and Ray</span>"
    ]
  },
  {
    "objectID": "Courses/06_Dask-Ray.html#dask-task-graph-computing",
    "href": "Courses/06_Dask-Ray.html#dask-task-graph-computing",
    "title": "7  Dask and Ray",
    "section": "8.3 Dask: Task Graph Computing",
    "text": "8.3 Dask: Task Graph Computing\n\nimport numpy as np\nimport dask.array as da\n\ndata = np.arange(100_000).reshape(200, 500)\na = da.from_array(data, chunks=(100, 100))\na\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n781.25 kiB\n78.12 kiB\n\n\nShape\n(200, 500)\n(100, 100)\n\n\nDask graph\n10 chunks in 1 graph layer\n\n\nData type\nint64 numpy.ndarray\n\n\n\n\n              500 200\n\n\n\n\n\n\n\nmean_graph = a.mean()\nmean_graph.visualize()\n\n\n\n\n\n\n\n\n\nresult = mean_graph.compute()\nresult\n\nnp.float64(49999.5)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask and Ray</span>"
    ]
  },
  {
    "objectID": "Courses/06_Dask-Ray.html#rays-actor-model-example",
    "href": "Courses/06_Dask-Ray.html#rays-actor-model-example",
    "title": "7  Dask and Ray",
    "section": "8.4 Ray’s actor model example",
    "text": "8.4 Ray’s actor model example\n\nimport ray\n\n@ray.remote\nclass Counter:\n    def __init__(self):\n        self.value = 0\n\n    def increment(self):\n        self.value += 1\n        return self.value\n\n# Create actor instances\ncounters = [Counter.remote() for _ in range(4)]\n# Increment counters in parallel\nray.get([counter.increment.remote() for counter in counters])\n\n[1, 1, 1, 1]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask and Ray</span>"
    ]
  },
  {
    "objectID": "Courses/06_Dask-Ray.html#key-differences-dask-vs-ray",
    "href": "Courses/06_Dask-Ray.html#key-differences-dask-vs-ray",
    "title": "7  Dask and Ray",
    "section": "8.5 Key Differences Dask vs Ray",
    "text": "8.5 Key Differences Dask vs Ray\n\n\n\n\n\n\n\n\nFeature\nDask\nRay\n\n\n\n\nParadigm\nTask graph (functional)\nActor model (OOP)\n\n\nState\nStateless (pure functions)\nStateful (actor instances)\n\n\nScheduling\nStatic graph optimization\nDynamic task scheduling\n\n\nBest for\nArray/collection operations\nHeterogeneous workloads",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask and Ray</span>"
    ]
  },
  {
    "objectID": "Courses/06_Dask-Ray.html#practical-examples",
    "href": "Courses/06_Dask-Ray.html#practical-examples",
    "title": "7  Dask and Ray",
    "section": "8.6 Practical Examples",
    "text": "8.6 Practical Examples\n\n\n\n# Dask version (graph-based)\nimport dask.bag as db\n\nbag = db.read_text('large_file.txt')\nresult = bag.map(lambda x: x.upper()).compute()\n\n\n# Ray version (actor-based)\n@ray.remote\ndef process_line(line):\n    return line.upper()\n\nwith open('large_file.txt') as f:\n    lines = f.readlines()\n\nresults = ray.get([process_line.remote(line) for line in lines])",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask and Ray</span>"
    ]
  },
  {
    "objectID": "Courses/06_Dask-Ray.html#example-2-machine-learning",
    "href": "Courses/06_Dask-Ray.html#example-2-machine-learning",
    "title": "7  Dask and Ray",
    "section": "8.7 Example 2: Machine Learning",
    "text": "8.7 Example 2: Machine Learning\n\n\n\n# Dask-ML example\nfrom dask_ml.linear_model import LogisticRegression\nfrom dask_ml.datasets import make_classification\n\nX, y = make_classification(n_samples=100000, chunks=1000)\nclf = LogisticRegression()\nclf.fit(X, y)\n\n\n# Ray Train example\nfrom ray.train.xgboost import XGBoostTrainer\n\ntrainer = XGBoostTrainer(\n    label_column=\"target\",\n    params={\"objective\": \"binary:logistic\"},\n    datasets={\"train\": ray.data.from_pandas(df)},\n)\nresult = trainer.fit()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask and Ray</span>"
    ]
  },
  {
    "objectID": "Courses/06_Dask-Ray.html#performance-considerations",
    "href": "Courses/06_Dask-Ray.html#performance-considerations",
    "title": "7  Dask and Ray",
    "section": "8.8 Performance Considerations",
    "text": "8.8 Performance Considerations\n\n\n\nDask excels at:\n\n\nLarge array operations\nPredictable workloads\nNumpy/Pandas-like workflows\n\n\n\n\nRay excels at:\n\n\nHeterogeneous tasks\nStateful applications\nReinforcement learning",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask and Ray</span>"
    ]
  },
  {
    "objectID": "Courses/06_Dask-Ray.html#hybrid-approaches",
    "href": "Courses/06_Dask-Ray.html#hybrid-approaches",
    "title": "7  Dask and Ray",
    "section": "8.9 Hybrid Approaches",
    "text": "8.9 Hybrid Approaches\n# Using both together\nimport dask.dataframe as dd\nimport ray\n\n# Process data with Dask\nddf = dd.read_csv('large_dataset.csv')\nprocessed = ddf.groupby('category').mean()\n\n# Use Ray for model serving\n@ray.remote\nclass ModelServer:\n    def __init__(self, model):\n        self.model = model\n    def predict(self, data):\n        return self.model.predict(data)\n\n# Convert Dask results to Ray tasks\nmodel = train_model(processed.compute())\nservers = [ModelServer.remote(model) for _ in range(4)]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask and Ray</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html",
    "href": "Courses/07_Dask-delayed.html",
    "title": "8  Dask delayed",
    "section": "",
    "text": "9 Dask Delayed\nSometimes problems don't fit into one of the collections like dask.array or dask.dataframe. In these cases, users can parallelize custom algorithms using the simpler dask.delayed interface. This allows you to create graphs directly with a light annotation of normal python code:",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#dask-delayed-1",
    "href": "Courses/07_Dask-delayed.html#dask-delayed-1",
    "title": "8  Dask delayed",
    "section": "Dask Delayed",
    "text": "Dask Delayed\n\n\nA Dask Delayed task graph with two \"inc\" functions combined using an \"add\" function resulting in an output node.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#example",
    "href": "Courses/07_Dask-delayed.html#example",
    "title": "8  Dask delayed",
    "section": "9.1 Example",
    "text": "9.1 Example\nVisit https://examples.dask.org/delayed.html to see and run examples using Dask Delayed.\nSometimes we face problems that are parallelizable, but don't fit into high-level abstractions like Dask Array or Dask DataFrame. Consider the following example:\ndef inc(x):\n    return x + 1\n\ndef double(x):\n    return x * 2\n\ndef add(x, y):\n    return x + y\n\ndata = [1, 2, 3, 4, 5]\n\noutput = []\nfor x in data:\n    a = inc(x)\n    b = double(x)\n    c = add(a, b)\n    output.append(c)\n\ntotal = sum(output)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#example-1",
    "href": "Courses/07_Dask-delayed.html#example-1",
    "title": "8  Dask delayed",
    "section": "Example",
    "text": "Example\nThere is clearly parallelism in this problem (many of the inc, double, and add functions can be evaluated independently), but it's not clear how to convert this to an array or DataFrame computation. As written, this code runs sequentially in a single thread. However, we see that a lot of this could be executed in parallel.\nThe Dask delayed function decorates your functions so that they operate lazily. Rather than executing your function immediately, it will defer execution, placing the function and its arguments into a task graph.\n\ndask.delayed\n\n\ndelayed",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#example-2",
    "href": "Courses/07_Dask-delayed.html#example-2",
    "title": "8  Dask delayed",
    "section": "Example",
    "text": "Example\nWe slightly modify our code by wrapping functions in delayed. This delays the execution of the function and generates a Dask graph instead:\nimport dask\n\noutput = []\nfor x in data:\n    a = dask.delayed(inc)(x)\n    b = dask.delayed(double)(x)\n    c = dask.delayed(add)(a, b)\n    output.append(c)\n\ntotal = dask.delayed(sum)(output)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#example-3",
    "href": "Courses/07_Dask-delayed.html#example-3",
    "title": "8  Dask delayed",
    "section": "Example",
    "text": "Example\nWe used the dask.delayed function to wrap the function calls that we want to turn into tasks. None of the inc, double, add, or sum calls have happened yet. Instead, the object total is a Delayed result that contains a task graph of the entire computation. Looking at the graph we see clear opportunities for parallel execution. The Dask schedulers &lt;scheduling&gt; will exploit this parallelism, generally improving performance (although not in this example, because these functions are already very small and fast.)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#example-4",
    "href": "Courses/07_Dask-delayed.html#example-4",
    "title": "8  Dask delayed",
    "section": "Example",
    "text": "Example\ntotal.visualize()  # see image to the right\n\n\nA task graph with many nodes for \"inc\" and \"double\" that combine with \"add\" nodes. The output of the \"add\" nodes finally aggregate with a \"sum\" node.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#example-5",
    "href": "Courses/07_Dask-delayed.html#example-5",
    "title": "8  Dask delayed",
    "section": "Example",
    "text": "Example\nWe can now compute this lazy result to execute the graph in parallel:\n&gt;&gt;&gt; total.compute()\n45",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#decorator",
    "href": "Courses/07_Dask-delayed.html#decorator",
    "title": "8  Dask delayed",
    "section": "9.2 Decorator",
    "text": "9.2 Decorator\nIt is also common to see the delayed function used as a decorator. Here is a reproduction of our original problem as a parallel code:\nimport dask\n\n@dask.delayed\ndef inc(x):\n    return x + 1\n\n@dask.delayed\ndef double(x):\n    return x * 2\n\n@dask.delayed\ndef add(x, y):\n    return x + y\n\ndata = [1, 2, 3, 4, 5]\n\noutput = []\nfor x in data:\n    a = inc(x)\n    b = double(x)\n    c = add(a, b)\n    output.append(c)\n\ntotal = dask.delayed(sum)(output)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#real-time",
    "href": "Courses/07_Dask-delayed.html#real-time",
    "title": "8  Dask delayed",
    "section": "9.3 Real time",
    "text": "9.3 Real time\nSometimes you want to create and destroy work during execution, launch tasks from other tasks, etc. For this, see the Futures &lt;futures&gt; interface.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#best-practices",
    "href": "Courses/07_Dask-delayed.html#best-practices",
    "title": "8  Dask delayed",
    "section": "9.4 Best Practices",
    "text": "9.4 Best Practices\nFor a list of common problems and recommendations see Delayed Best Practices &lt;delayed-best-practices&gt;.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#indirect-dependencies",
    "href": "Courses/07_Dask-delayed.html#indirect-dependencies",
    "title": "8  Dask delayed",
    "section": "9.5 Indirect Dependencies",
    "text": "9.5 Indirect Dependencies\nSometimes you might find yourself wanting to add a dependency to a task that does not take the result of that dependency as an input. For example when a task depends on the side-effect of another task. In these cases you can use dask.graph_manipulation.bind.\nimport dask\nfrom dask.graph_manipulation import bind\n\nDATA = []\n\n@dask.delayed\ndef inc(x):\n    return x + 1\n\n@dask.delayed\ndef add_data(x):\n    DATA.append(x)\n\n@dask.delayed\ndef sum_data(x):\n    return sum(DATA) + x\n\na = inc(1)\nb = add_data(a)\nc = inc(3)\nd = add_data(c)\ne = inc(5)\nf = bind(sum_data, [b, d])(e)\nf.compute()\nsum_data will operate on DATA only after both the expected items have been appended to it. bind can also be used along with direct dependencies passed through the function arguments.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#call-delayed-on-the-function-not-the-result",
    "href": "Courses/07_Dask-delayed.html#call-delayed-on-the-function-not-the-result",
    "title": "8  Dask delayed",
    "section": "10.1 Call delayed on the function, not the result",
    "text": "10.1 Call delayed on the function, not the result\nDask delayed operates on functions like dask.delayed(f)(x, y), not on their results like dask.delayed(f(x, y)). When you do the latter, Python first calculates f(x, y) before Dask has a chance to step in.\n\n\n\n\n\n\n\nDon't\nDo\n\n\n# This executes immediately\n\ndask.delayed(f(x, y))\n# This ma\nkes a delayed function, acting lazily\n\ndask.delayed(f)(x, y)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#compute-on-lots-of-computation-at-once",
    "href": "Courses/07_Dask-delayed.html#compute-on-lots-of-computation-at-once",
    "title": "8  Dask delayed",
    "section": "10.2 Compute on lots of computation at once",
    "text": "10.2 Compute on lots of computation at once\nTo improve parallelism, you want to include lots of computation in each compute call. Ideally, you want to make many dask.delayed calls to define your computation and then call dask.compute only at the end. It is ok to call dask.compute in the middle of your computation as well, but everything will stop there as Dask computes those results before moving forward with your code.\n\n\n\n\n\n\n\nDon't\nDo\n\n\n# Avoid calling compute repeatedly\n\nresults = []\nfor x in L:\n    y = dask.delayed(f)(x)\n    results.append(y.compute())\n\nresults\n# Collec\nt many calls for one compute\n\nresults = []\nfor x in L:\n    y = dask.delayed(f)(x)\n    results.append(y)\n\nresu\nlts = dask.compute(*results)\n\n\n\nCalling y.compute() within the loop would await the result of the computation every time, and so inhibit parallelism.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#dont-mutate-inputs",
    "href": "Courses/07_Dask-delayed.html#dont-mutate-inputs",
    "title": "8  Dask delayed",
    "section": "10.3 Don't mutate inputs",
    "text": "10.3 Don't mutate inputs\nYour functions should not change the inputs directly.\n\n\n\n\n\n\n\nDon't\nDo\n\n\n# Mutate inputs in functions\n\n@dask.delayed\ndef f(x):\n    x += 1\n    return x\n# Return new values or copies\n\n@dask.delayed\ndef f(x):\n    x = x + 1\n    return x\n\n\n\nIf you need to use a mutable operation, then make a copy within your function first:\n@dask.delayed\ndef f(x):\n    x = copy(x)\n    x += 1\n    return x",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#avoid-global-state",
    "href": "Courses/07_Dask-delayed.html#avoid-global-state",
    "title": "8  Dask delayed",
    "section": "10.4 Avoid global state",
    "text": "10.4 Avoid global state\nIdeally, your operations shouldn't rely on global state. Using global state might work if you only use threads, but when you move to multiprocessing or distributed computing then you will likely encounter confusing errors.\n\n\n\n\n\n\nDon't\n\n\nL = []\n\n# This references global variable L\n\n@dask.delayed\ndef f(x):\n    L.append(x)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#dont-rely-on-side-effects",
    "href": "Courses/07_Dask-delayed.html#dont-rely-on-side-effects",
    "title": "8  Dask delayed",
    "section": "10.5 Don't rely on side effects",
    "text": "10.5 Don't rely on side effects\nDelayed functions only do something if they are computed. You will always need to pass the output to something that eventually calls compute.\n\n\n\n\n\n\n\nDon't\nDo\n\n\n# Forget to call compute\n\ndask.delayed(f)(1, 2, 3)\n\n...\n# Ensure delayed tasks are computed\n\nx = dask.delayed(f)(1, 2, 3)\n...\ndask.compute(x, ...)\n\n\n\nIn the first case here, nothing happens, because compute() is never called.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#break-up-computations-into-many-pieces",
    "href": "Courses/07_Dask-delayed.html#break-up-computations-into-many-pieces",
    "title": "8  Dask delayed",
    "section": "10.6 Break up computations into many pieces",
    "text": "10.6 Break up computations into many pieces\nEvery dask.delayed function call is a single operation from Dask's perspective. You achieve parallelism by having many delayed calls, not by using only a single one: Dask will not look inside a function decorated with @dask.delayed and parallelize that code internally. To accomplish that, it needs your help to find good places to break up a computation.\n\n\n\n\n\n\n\nDon't\nDo\n\n\n# One giant task\n\n\ndef load(filename):\n    ...\n\n\ndef process(data):\n    ...\n\n\ndef save(data):\n    ...\n\n@dask.delayed\ndef f(filenames):\n    results = []\n    for filename in filenames:\n        data = load(filename)\n        data = process(data)\n        result = save(data)\n        results.append(result)\n\n    return results\n\ndask.compute(f(filenames))\n# Break up into many tasks\n\n@dask.delayed\ndef load(filename):\n    ...\n\n@dask.delayed\ndef process(data):\n    ...\n\n@dask.delayed\ndef save(data):\n    ...\n\n\ndef f(filenames):\n    results = []\n    for filename in filenames:\n        data = load(filename)\n        data = process(data)\n        result = save(data)\n        results.append(result)\n\n    return results\n\ndask.compute(f(filenames))\n\n\n\nThe first version only has one delayed task, and so cannot parallelize.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#avoid-too-many-tasks",
    "href": "Courses/07_Dask-delayed.html#avoid-too-many-tasks",
    "title": "8  Dask delayed",
    "section": "10.7 Avoid too many tasks",
    "text": "10.7 Avoid too many tasks\nEvery delayed task has an overhead of a few hundred microseconds. Usually this is ok, but it can become a problem if you apply dask.delayed too finely. In this case, it's often best to break up your many tasks into batches or use one of the Dask collections to help you.\n\n\n\n\n\n\n\nDon't\nDo\n\n\n# Too many tasks\n\nresults = []\nfo\nr x in range(10000000):\n\n y = dask.delayed(f)(x)\n    results.append(y)\n# Use collections\n\nimport dask.bag as db\nb = db.from_s\nequence(range(10000000), npartitions=1000)\nb = b.map(f)\n...",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#avoid-too-many-tasks-1",
    "href": "Courses/07_Dask-delayed.html#avoid-too-many-tasks-1",
    "title": "8  Dask delayed",
    "section": "Avoid too many tasks",
    "text": "Avoid too many tasks\nHere we use dask.bag to automatically batch applying our function. We could also have constructed our own batching as follows\ndef batch(seq):\n    sub_results = []\n    for x in seq:\n        sub_results.append(f(x))\n    return sub_results\n\n batches = []\n for i in range(0, 10000000, 10000):\n     result_batch = dask.delayed(batch)(range(i, i + 10000))\n     batches.append(result_batch)\nHere we construct batches where each delayed function call computes for many data points from the original input.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#avoid-calling-delayed-within-delayed-functions",
    "href": "Courses/07_Dask-delayed.html#avoid-calling-delayed-within-delayed-functions",
    "title": "8  Dask delayed",
    "section": "10.8 Avoid calling delayed within delayed functions",
    "text": "10.8 Avoid calling delayed within delayed functions\nOften, if you are new to using Dask delayed, you place dask.delayed calls everywhere and hope for the best. While this may actually work, it's usually slow and results in hard-to-understand solutions.\nUsually you never call dask.delayed within dask.delayed functions.\n\n\n\n\n\n\n\nDon't\nDo\n\n\n# Delayed function calls delayed\n\n@dask.delayed\ndef process_all(L):\n    result = []\n    for x in L:\n        y = dask.delayed(f)(x)\n        result.append(y)\n    return result\n# Normal function calls delayed\n\n\ndef process_all(L):\n    result = []\n    for x in L:\n        y = dask.delayed(f)(x)\n        result.append(y)\n    return result\n\n\n\nBecause the normal function only does delayed work it is very fast and so there is no reason to delay it.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#dont-call-dask.delayed-on-other-dask-collections",
    "href": "Courses/07_Dask-delayed.html#dont-call-dask.delayed-on-other-dask-collections",
    "title": "8  Dask delayed",
    "section": "10.9 Don't call dask.delayed on other Dask collections",
    "text": "10.9 Don't call dask.delayed on other Dask collections\nWhen you place a Dask array or Dask DataFrame into a delayed call, that function will receive the NumPy or Pandas equivalent. Beware that if your array is large, then this might crash your workers.\nInstead, it's more common to use methods like da.map_blocks\n\n\n\n\n\n\n\nDon't\nDo\n\n\n# Call del\nayed functions on Dask collections\n\nimport dask.dataframe as dd\ndf = dd.read_csv('/path/to/*.csv')\n\ndask.delayed(train)(df)\n# Us\ne mapping methods if applicable\n\nimport dask.dataframe as dd\ndf\n= dd.read_csv('/path/to/*.csv')\n\ndf.map_partitions(train)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#dont-call-dask.delayed-on-other-dask-collections-1",
    "href": "Courses/07_Dask-delayed.html#dont-call-dask.delayed-on-other-dask-collections-1",
    "title": "8  Dask delayed",
    "section": "Don't call dask.delayed on other Dask collections",
    "text": "Don't call dask.delayed on other Dask collections\nAlternatively, if the procedure doesn't fit into a mapping, you can always turn your arrays or dataframes into many delayed objects, for example\npartitions = df.to_delayed()\ndelayed_values = [dask.delayed(train)(part)\n                  for part in partitions]\nHowever, if you don't mind turning your Dask array/DataFrame into a single chunk, then this is ok.\ndask.delayed(train)(..., y=df.sum())",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#avoid-repeatedly-putting-large-inputs-into-delayed-calls",
    "href": "Courses/07_Dask-delayed.html#avoid-repeatedly-putting-large-inputs-into-delayed-calls",
    "title": "8  Dask delayed",
    "section": "10.10 Avoid repeatedly putting large inputs into delayed calls",
    "text": "10.10 Avoid repeatedly putting large inputs into delayed calls\nEvery time you pass a concrete result (anything that isn't delayed) Dask will hash it by default to give it a name. This is fairly fast (around 500 MB/s) but can be slow if you do it over and over again. Instead, it is better to delay your data as well.\nThis is especially important when using a distributed cluster to avoid sending your data separately for each function call.\n\n\n\n\n\n\n\nDon't\nDo\n\n\nx = np.arr\nay(...)  # some large array\n\nresults =\n [dask.delayed(train)(x, i)\n\n      for i in range(1000)]\nx\n = np.array(...)    # some large array\nx =\ndask.delayed(x)  # delay the data once\nresults = [dask.delayed(train)(x, i)\n           for i in range(1000)]\n\n\n\nEvery call to dask.delayed(train)(x, ...) has to hash the NumPy array x, which slows things down.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#working-with-collections-1",
    "href": "Courses/07_Dask-delayed.html#working-with-collections-1",
    "title": "8  Dask delayed",
    "section": "Working with Collections",
    "text": "Working with Collections\nAs an example, consider the case where we store tabular data in a custom format not known by Dask DataFrame. This format is naturally broken apart into pieces and we have a function that reads one piece into a Pandas DataFrame. We use dask.delayed to lazily read these files into Pandas DataFrames, use dd.from_delayed to wrap these pieces up into a single Dask DataFrame, use the complex algorithms within the DataFrame (groupby, join, etc.), and then switch back to dask.delayed to save our results back to the custom format:",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#working-with-collections-2",
    "href": "Courses/07_Dask-delayed.html#working-with-collections-2",
    "title": "8  Dask delayed",
    "section": "Working with Collections",
    "text": "Working with Collections\nimport dask.dataframe as dd\nfrom dask.delayed import delayed\n\nfrom my_custom_library import load, save\n\nfilenames = ...\ndfs = [delayed(load)(fn) for fn in filenames]\n\ndf = dd.from_delayed(dfs)\ndf = ... # do work with dask.dataframe\n\ndfs = df.to_delayed()\nwrites = [delayed(save)(df, fn) for df, fn in zip(dfs, filenames)]\n\ndd.compute(*writes)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/07_Dask-delayed.html#working-with-collections-3",
    "href": "Courses/07_Dask-delayed.html#working-with-collections-3",
    "title": "8  Dask delayed",
    "section": "Working with Collections",
    "text": "Working with Collections\nData science is often complex, and dask.delayed provides a release valve for users to manage this complexity on their own, and solve the last mile problem for custom formats and complex situations.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/08_SIMD.html",
    "href": "Courses/08_SIMD.html",
    "title": "9  Hardware Vectorization with SIMD",
    "section": "",
    "text": "10 Generalities on SIMD",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/08_SIMD.html#why-vectorization",
    "href": "Courses/08_SIMD.html#why-vectorization",
    "title": "9  Hardware Vectorization with SIMD",
    "section": "10.1 Why vectorization ?",
    "text": "10.1 Why vectorization ?\n\n\nUbiquitousness\n\nAlmost every CPU made on the market since 2010 got a SIMD unit and it operates on multiple elements at the same time on a single instruction.\n\nNatural congruence with parallel progamming\n\nIn a good number of cases, parallelization of algorithm is often achieved by vectorization and the use of SIMD is then a free bonus.\n\nPerformance boost almost guaranteed\n\nWith a little effort it could boost your performance by a factor of two or more. It is even more energy efficient than raw CPU (non SIMD) computing(Inoue 2016).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/08_SIMD.html#scalar-vs-vector-operation",
    "href": "Courses/08_SIMD.html#scalar-vs-vector-operation",
    "title": "9  Hardware Vectorization with SIMD",
    "section": "10.2 Scalar vs Vector operation",
    "text": "10.2 Scalar vs Vector operation\n\n\n\n\n\nA scalar operation does a single double-precision addition in one cycle. It takes eight cycles to process a 64-byte cache line. In comparison, a vector operation on a 512-bit vector unit can process all eight double-precision values in one cycle.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/08_SIMD.html#terminology",
    "href": "Courses/08_SIMD.html#terminology",
    "title": "9  Hardware Vectorization with SIMD",
    "section": "10.3 Terminology",
    "text": "10.3 Terminology\n\nVector (SIMD) lane\n\nA pathway through a vector operation on vector registers for a single data element much like a lane on a multi-lane freeway.\n\nVector width\n\nThe width of the vector unit, usually expressed in bits.\n\nVector length\n\nThe number of data elements that can be processed by the vector in one operation.\n\nVector (SIMD) instruction sets\n\nThe set of instructions that extend the regular scalar processor instructions to utilize the vector processor.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/08_SIMD.html#hardwaresoftware-requirements",
    "href": "Courses/08_SIMD.html#hardwaresoftware-requirements",
    "title": "9  Hardware Vectorization with SIMD",
    "section": "10.4 Hardware/Software Requirements",
    "text": "10.4 Hardware/Software Requirements\n\nGeneration of instructions\n\nvector instructions are generated by the compiler OR manually by the programmer via the “intrisics” (explicit SIMD instructions)\n\nMatching of instructions\n\nmatching of the instructions and the hardware, because there is several units and instructions sets. (the compiler does the matching, most of the time).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/08_SIMD.html#evolution-of-the-vector-length",
    "href": "Courses/08_SIMD.html#evolution-of-the-vector-length",
    "title": "9  Hardware Vectorization with SIMD",
    "section": "11.1 Evolution of the vector length",
    "text": "11.1 Evolution of the vector length",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/08_SIMD.html#versions-i",
    "href": "Courses/08_SIMD.html#versions-i",
    "title": "9  Hardware Vectorization with SIMD",
    "section": "11.2 Versions, I",
    "text": "11.2 Versions, I\n\n\n\nRelease\nFunctionality\n\n\n\n\nMMX (trademark with no official meaning)\nTargeted towards the graphics market, but GPUs soon took over this function. Vector units shifted their focus to computation rather than graphics. AMD released its version under the name 3DNow! with single-precision support.\n\n\nSSE (Streaming SIMD Extensions)\nFirst Intel vector unit to offer floating-point operations with single-precision support\n\n\nSSE2\nDouble-precision support added",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/08_SIMD.html#versions-ii",
    "href": "Courses/08_SIMD.html#versions-ii",
    "title": "9  Hardware Vectorization with SIMD",
    "section": "11.3 Versions, II",
    "text": "11.3 Versions, II\n\n\n\nRelease\nFunctionality\n\n\n\n\nAVX (Advanced Vector Extensions)\nTwice the vector length. AMD added a fused multiply-add FMA vector instruction in its competing hardware, effectively doubling the performance for some loops.\n\n\nAVX2\nIntel added a fused multiply-add (FMA) to its vector processor.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/08_SIMD.html#versions-iii",
    "href": "Courses/08_SIMD.html#versions-iii",
    "title": "9  Hardware Vectorization with SIMD",
    "section": "11.4 Versions, III",
    "text": "11.4 Versions, III\n\n\n\nRelease\nFunctionality\n\n\n\n\nAVX512\nFirst offered on the Knights Landing processor; it came to the main-line multi-core processor hardware lineup in 2017.From the years 2018 and on, Intel and AMD (Advanced Micro Devices, Inc.) have created multiple variants of AVX512 as incremental improvements to vector hardware architectures.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/08_SIMD.html#vectors-types-avx",
    "href": "Courses/08_SIMD.html#vectors-types-avx",
    "title": "9  Hardware Vectorization with SIMD",
    "section": "12.1 Vectors types (AVX)",
    "text": "12.1 Vectors types (AVX)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/08_SIMD.html#encoding-scheme",
    "href": "Courses/08_SIMD.html#encoding-scheme",
    "title": "9  Hardware Vectorization with SIMD",
    "section": "12.2 Encoding scheme",
    "text": "12.2 Encoding scheme\n_mm256_{operation}{non-alignement}_{dataorganization}{datatype}",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/08_SIMD.html#add-example",
    "href": "Courses/08_SIMD.html#add-example",
    "title": "9  Hardware Vectorization with SIMD",
    "section": "12.3 Add example",
    "text": "12.3 Add example",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/08_SIMD.html#various-i",
    "href": "Courses/08_SIMD.html#various-i",
    "title": "9  Hardware Vectorization with SIMD",
    "section": "12.4 Various, I",
    "text": "12.4 Various, I\n\ns (single): single precision float (32bits)\nd (double): double precision float (64bits)\ni… (integer): integer\np (packed): contiguous, operates on the whole vector\ns (scalar): operates on a single element",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/08_SIMD.html#various-ii",
    "href": "Courses/08_SIMD.html#various-ii",
    "title": "9  Hardware Vectorization with SIMD",
    "section": "12.5 Various, II",
    "text": "12.5 Various, II\n\nu (unaligned): data non aligned in memory\nl (low): least significant bits\nh (high): most significant bits\nr (reversed): reversed order",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html",
    "href": "Courses/09_GPU-Caching-etc.html",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "",
    "text": "11 GPU Computing",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#gpu",
    "href": "Courses/09_GPU-Caching-etc.html#gpu",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "11.1 GPU ?",
    "text": "11.1 GPU ?\nGPU = graphical process unit\n\n“(…) a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device. GPUs are used in embedded systems, mobile phones, personal computers, workstations, and game consoles.” (Wikipedia)\n\nOriginal purpose: image creation and manipulation for graphical rendering (video games, video edition, 3D conception, etc.)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#gpu-for-computing",
    "href": "Courses/09_GPU-Caching-etc.html#gpu-for-computing",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "11.2 GPU for computing ?",
    "text": "11.2 GPU for computing ?\n\n“Modern GPUs are very efficient at manipulating computer graphics and image processing. Their highly parallel structure makes them more efficient than general-purpose central processing units (CPUs) for algorithms that process large blocks of data in parallel.” (Wikipedia)\n\nNowadays usage: more general massive computations based on matrix operations",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#von-neumann-architecture",
    "href": "Courses/09_GPU-Caching-etc.html#von-neumann-architecture",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "11.3 Von Neumann architecture",
    "text": "11.3 Von Neumann architecture\nBasic principle (present in most modern computing units) with three interconnected parts:\n\ncontrol unit: does most of the work, such as deciding what operation to do next, fetching the next instruction from memory, etc.\narithmetic/logic unit (ALU): implements operations such as addition, subtraction, multiplication, etc.\nmemory unit: stores the data processed by the chip, such as its inputs, outputs, and any intermediate data",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#von-neumann-architecture-schema",
    "href": "Courses/09_GPU-Caching-etc.html#von-neumann-architecture-schema",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "11.4 Von Neumann architecture, schema",
    "text": "11.4 Von Neumann architecture, schema\n\n\nYgor Serpa (TowardsDataScience)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#gpu-architecture",
    "href": "Courses/09_GPU-Caching-etc.html#gpu-architecture",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "11.5 GPU architecture",
    "text": "11.5 GPU architecture\n\n\nYgor Serpa (TowardsDataScience)\n\nPrinciple: control units are large and expensive, while arithmetic units are more straightforward and cheaper\nDesign: a sub-processor = one control unity commanding several ALUs (= GPU cores) to operate on larger chunks of data\nLimitation: all ALUs (connected to the same control unity) must obey the same commands (i.e. do the same thing at the same moment)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#gpu-trades-control-for-arithmetic-power",
    "href": "Courses/09_GPU-Caching-etc.html#gpu-trades-control-for-arithmetic-power",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "11.6 GPU trades “control” for “arithmetic power”",
    "text": "11.6 GPU trades “control” for “arithmetic power”\n\nALU blocks (= gpu sub-processors) not as versatile as CPU cores but can operate over large amounts of data in batches\nCentral control unit syncs all the sub-processors (each one can do a different task) but the same set of ALUs cannot do different things in parallel (e.g. if statements costly for GPUs)\n\nExample: each block of 16 ALUs is limited to processing the same instruction over 16 pairs of operands",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#latency-vs-throughput",
    "href": "Courses/09_GPU-Caching-etc.html#latency-vs-throughput",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "11.7 Latency vs Throughput",
    "text": "11.7 Latency vs Throughput\n\nLatency: how long does it take to finish a given task\nThroughput: how many times can you complete the task within a period\n\nCPU cores optimized for latency: to finish a task as fast as possible.\n\\rightarrow scales with more complex operations but not with larger data\nGPU cores optimized for throughput: individually slower, but they operate on bulks of data at once.\n\\rightarrow scales with larger data but not with more complex operations",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#latency-vs-throughput-an-analogy-i",
    "href": "Courses/09_GPU-Caching-etc.html#latency-vs-throughput-an-analogy-i",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "11.8 Latency vs Throughput: an analogy (I)",
    "text": "11.8 Latency vs Throughput: an analogy (I)\nTravelling by road:\n\nlatency: use a sport car (faster but can only take 2-5 persons)\nthroughput: use a bus (slower but can take up to 80 persons)\n\nTotal time to transport 5 persons over 100km (latency):\n\n1 trip with a 80-seat bus at 50km/h: 2hours\n1 trip with a 5-seat car at 200km/h: 0.5hours\n\nTotal time to transport 160 persons over 100km (throughput):\n\n2 round trips with a 80-seat bus at 50km/h: 8hours\n32 round trips with a 5-seat car at 200km/h: 32hours",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#latency-vs-throughput-an-analogy-ii",
    "href": "Courses/09_GPU-Caching-etc.html#latency-vs-throughput-an-analogy-ii",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "11.9 Latency vs Throughput: an analogy (II)",
    "text": "11.9 Latency vs Throughput: an analogy (II)\nFastest way to learn?\n\n\nLatency: private teacher or small classroom\nThroughput: big classroom or online course\n\n\n\n\n\nYgor Serpa (TowardsDataScience)\n\n\\rightarrow none is arguably better than the others, all work and fill a specific need",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#cpu-vs-gpu-hardware-side",
    "href": "Courses/09_GPU-Caching-etc.html#cpu-vs-gpu-hardware-side",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "11.10 CPU vs GPU: hardware side",
    "text": "11.10 CPU vs GPU: hardware side\n\nUnits: B = Byte (memory), W = Watts (power), Hz = Hertz (frequency)\n\n\nType\nNb of cores\nMemory (cache1)\nMemory (device)\nPower per core\nClock rate2\n\n\n\n\nCPU\n10\\times\n10\\times - 100\\times KB\n10\\times - 100\\times GB3\n10\\times W\n3 - 5 GHz\n\n\nGPU\n1000\\times\n100\\times B\n10\\times GB4\n0.01\\times - 0.1\\times W\n0.5 - 1 GHz",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#cpu-vs-gpu-concept-side",
    "href": "Courses/09_GPU-Caching-etc.html#cpu-vs-gpu-concept-side",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "11.11 CPU vs GPU: concept side",
    "text": "11.11 CPU vs GPU: concept side\n\n\n\nCPU\nGPU\n\n\n\n\nTask parallelism\nData parallelism\n\n\nA few “heavyweight” cores\nMany “ligthweight” cores\n\n\nHigh memory size\nHigh memory throughput\n\n\nMany diverse instruction sets\nA few highly optimized instruction sets\n\n\nSoftware thread management\nHardware thread management\n\n\n\n\nMantas Levinas (CherryServers)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#gpu-dedicated-chip",
    "href": "Courses/09_GPU-Caching-etc.html#gpu-dedicated-chip",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "11.12 GPU = dedicated chip",
    "text": "11.12 GPU = dedicated chip\n\n\nCPU directly mounted on the motherboard\n&lt;5\\times 5 cm dimension5\n \n\nGPU is a HUGE chip (\\sim 30\\times 10 cm)\nmounted through PCI-express connection\n\n\n\n\nWikipedia",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#cpu-vs-gpu-comparison-an-example",
    "href": "Courses/09_GPU-Caching-etc.html#cpu-vs-gpu-comparison-an-example",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "11.13 CPU vs GPU comparison: an example",
    "text": "11.13 CPU vs GPU comparison: an example\n\n\nYgor Serpa (TowardsDataScience)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#cpu-vs-gpu-comparison-an-example-cpu",
    "href": "Courses/09_GPU-Caching-etc.html#cpu-vs-gpu-comparison-an-example-cpu",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "11.14 CPU vs GPU comparison: an example, CPU",
    "text": "11.14 CPU vs GPU comparison: an example, CPU\nAMD Ryzen Threadripper 2990X processor: 32cores (each capable of running two independent threads), 3 GHz up to 4.2 GHz (boost) clock rate, 2 instructions per clock cycle, each thread processes 8 floating-point values at the same time",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#cpu-vs-gpu-comparison-an-example-gpu",
    "href": "Courses/09_GPU-Caching-etc.html#cpu-vs-gpu-comparison-an-example-gpu",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "11.15 CPU vs GPU comparison: an example, GPU",
    "text": "11.15 CPU vs GPU comparison: an example, GPU\nNvidia RTX 2080 TI GPU: 4352 ALUs, 1.35 GHz up to 1.545 GHz (boost) clock rate, 2 operations per clock cycle, one floating point operation per ALU",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#performance-illustration",
    "href": "Courses/09_GPU-Caching-etc.html#performance-illustration",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "11.16 Performance illustration",
    "text": "11.16 Performance illustration\nThroughput:\nMaximum theoretical number of floating-point operations they can handle per second (FLOPS)\n\nCPU (Ryzen): (4.2 \\times 10^9) \\times 2 \\times (8 \\times 64) = 4309.8 \\times 10^9 FLOPS \\approx 4.3 TFLOPS\nGPU (2080 TI): (1.545 \\times 10^9) \\times 2 \\times 4352 = 13447.68 \\times 10^9 FLOPS \\approx 13.4 TFLOPS\n\nLatency: the CPU clocks at 4.2 GHz, while the GPU clocks at \\sim 1.5 GHz, nearly three times slower\n\\rightarrow one is not better than the other, they serve different purposes",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#from-3d-image-processing-for-games-to-general-purpose-computing",
    "href": "Courses/09_GPU-Caching-etc.html#from-3d-image-processing-for-games-to-general-purpose-computing",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "11.17 From 3D image processing (for games) to general-purpose computing",
    "text": "11.17 From 3D image processing (for games) to general-purpose computing\nimage processing = large matrix operations\n\\rightarrow why not using GPU for general matrix operations?\nExample: deep-learning = combination of linear transformation (=matrix product) and simple non-linear operations\nGeneral-Purpose Graphics Processing Units (GPGPU): computations on GPU not dedicated to image processing",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#hardware",
    "href": "Courses/09_GPU-Caching-etc.html#hardware",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "11.18 Hardware",
    "text": "11.18 Hardware\n\nNvidia (with CUDA drivers) \nAMD (with AMD drivers or openCL)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#software-python",
    "href": "Courses/09_GPU-Caching-etc.html#software-python",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "11.19 Software (Python)",
    "text": "11.19 Software (Python)\n\ndeep learning oriented: tensorflow, PyTorch, keras \nmore general purpose computing: see numba, cupy, RAPIDS (cuDF, cuML, etc.), keops, and more",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#software-r",
    "href": "Courses/09_GPU-Caching-etc.html#software-r",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "11.20 Software (R)",
    "text": "11.20 Software (R)\nSee this page6\nExample with gpuR:\nORDER = 1024\nA = matrix(rnorm(ORDER^2), nrow=ORDER)\nB = matrix(rnorm(ORDER^2), nrow=ORDER)\ngpuA = gpuMatrix(A, type=\"double\")\ngpuB = gpuMatrix(B, type=\"double\")\nC = A %*% B\ngpuC = gpuA %*% gpuB\nall(C == gpuC[])\n[1] TRUE",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#in-practice",
    "href": "Courses/09_GPU-Caching-etc.html#in-practice",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "11.21 In practice",
    "text": "11.21 In practice\nSee the dedicated notebook “GPU computing with Numba (introduction)” in the Applications list.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#cpu-cache",
    "href": "Courses/09_GPU-Caching-etc.html#cpu-cache",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "12.1 CPU cache ?",
    "text": "12.1 CPU cache ?\nCache = on-board memory unit for CPU cores\n\n\nYgor Serpa (TowardsDataScience)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#example-of-architecture",
    "href": "Courses/09_GPU-Caching-etc.html#example-of-architecture",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "12.2 Example of architecture",
    "text": "12.2 Example of architecture\n\n\nCredit: Wikipedia (AMD Bulldozer server)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#cache-memory-size",
    "href": "Courses/09_GPU-Caching-etc.html#cache-memory-size",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "12.3 Cache memory size",
    "text": "12.3 Cache memory size\n\n\nVery small\n\nL1 Cache = 10\\times KB (per core) \nL2 Cache = 100\\times KB (per core) \nL3 Cache = 1000\\times KB (shared per CPU)\n\n\nExample:\n\n\n\n\nWikipedia (K8 core in the AMD Athlon 64 CPU)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#cache-access-time-latency",
    "href": "Courses/09_GPU-Caching-etc.html#cache-access-time-latency",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "12.4 Cache access time (latency)",
    "text": "12.4 Cache access time (latency)\n\n\nStackoverflow",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#cpu-cache-miss",
    "href": "Courses/09_GPU-Caching-etc.html#cpu-cache-miss",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "12.5 CPU cache miss",
    "text": "12.5 CPU cache miss\n\ndata transferred from memory to cache by blocks of contiguous data\nto be efficient: necessary to use contiguous data in computations\n\n\n\nData analysis workflows with R and Python course",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#in-practice-1",
    "href": "Courses/09_GPU-Caching-etc.html#in-practice-1",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "12.6 In practice",
    "text": "12.6 In practice\nSee the dedicated notebook “CPU Cache and its impact on computations” in the Applications list.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#cluster",
    "href": "Courses/09_GPU-Caching-etc.html#cluster",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "13.1 Cluster",
    "text": "13.1 Cluster\n\na front-end server: user interface to submit computations, accessible from the internet (e.g. by ssh connection or through a web interface)\nmany computing servers (also called computing nodes or workers) that run the computations\none or more storage servers: to store the data and results\n\n\n\nWikipedia",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#usage",
    "href": "Courses/09_GPU-Caching-etc.html#usage",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "13.2 Usage",
    "text": "13.2 Usage\n\nsingle-node multi-core computations (multi-threading or multi-processing) \nmulti-node computations (distributed computations) \nGPU computing \ncombinations of two or more",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#illustration",
    "href": "Courses/09_GPU-Caching-etc.html#illustration",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "13.3 Illustration",
    "text": "13.3 Illustration\nMUSE cluster at MESO@LR computing center in Montpellier\n\n\nmeso@LR",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#resource-management",
    "href": "Courses/09_GPU-Caching-etc.html#resource-management",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "13.4 Resource management",
    "text": "13.4 Resource management\nHow to share the computing resources (cores, memory, full nodes) between user?\n\\rightarrow a resource management system (also called a scheduler) to assign resources to users depending on their request\nFunctions:\n\nallocating exclusive and/or non-exclusive access to resources (computer nodes) to users for some duration of time so they can perform work \nproviding a framework for starting, executing, and monitoring work, typically a parallel job \narbitrating contention for resources by managing a queue of pending jobs",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#an-example",
    "href": "Courses/09_GPU-Caching-etc.html#an-example",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "13.5 An example",
    "text": "13.5 An example\nSLURM = a job scheduler for clusters and supercomputers\nJob submission:\n\nIn-session command submission (e.g. requesting 16 cores with 32GB of RAM memory and a RTX 2080 TI GPU, on a single node, during 3 days):\n\nsrun -N 1 -c 16 --mem=32G --gpus=rtx2080:1 -t 3-0 my_command\n\nScript submission to be run off-session (e.g. requesting 3 full nodes during 96hours):\n\nsbatch -N 3 -t 96:0:0 my_script.sh",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/09_GPU-Caching-etc.html#footnotes",
    "href": "Courses/09_GPU-Caching-etc.html#footnotes",
    "title": "10  GPU computing, CPU cache, Computing cluster",
    "section": "",
    "text": "per core↩︎\ncycles per second (one operation = 1\\times - 10\\times cycles↩︎\nRAM = host memory↩︎\nGPU on-board memory↩︎\nsize fixed for decades↩︎\ncorresponding to CRAN task view on high performance computing↩︎",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html",
    "href": "Courses/10_WebGPU-First-App.html",
    "title": "11  Your First WebGPU App",
    "section": "",
    "text": "12 Introduction",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html#what-is-webgpu",
    "href": "Courses/10_WebGPU-First-App.html#what-is-webgpu",
    "title": "11  Your First WebGPU App",
    "section": "12.1 What is WebGPU?",
    "text": "12.1 What is WebGPU?\n\n\n\n\nModern, cross-platform GPU API for the web\nSuccessor to WebGL; maps to Metal, Vulkan, Direct3D 12\nTwo main capabilities:\n\nRendering: draw 2D/3D graphics\nCompute: massively parallel general-purpose workloads\n\n\n\n\n\n\n\n\n\nPosition WebGPU vs WebGL (modern API mapping to Metal/Vulkan/D3D12). Emphasize compute shaders and how the web finally gains first-class GPGPU.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html#what-youll-build",
    "href": "Courses/10_WebGPU-First-App.html#what-youll-build",
    "title": "11  Your First WebGPU App",
    "section": "12.2 What you’ll build",
    "text": "12.2 What you’ll build\n\nMini pipeline to render a square, the building block for a Game of Life simulation\nLearn the primitives: device, canvas context, buffers, shaders, pipeline, draw\n\n\nSet expectations: we start with a single square, then scale to an instanced grid and outline the compute step.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html#requirements",
    "href": "Courses/10_WebGPU-First-App.html#requirements",
    "title": "11  Your First WebGPU App",
    "section": "12.3 Requirements",
    "text": "12.3 Requirements\n\nChrome 113+ (or a browser with WebGPU enabled)\nBasic HTML/JS\n\nLinks: https://gpuweb.github.io/gpuweb/ • https://codelabs.developers.google.com/your-first-webgpu-app",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html#minimal-html-page",
    "href": "Courses/10_WebGPU-First-App.html#minimal-html-page",
    "title": "11  Your First WebGPU App",
    "section": "13.1 Minimal HTML page",
    "text": "13.1 Minimal HTML page\n&lt;!doctype html&gt;\n\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;meta charset=\"utf-8\"&gt;\n    &lt;title&gt;WebGPU Life&lt;/title&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;canvas width=\"512\" height=\"512\"&gt;&lt;/canvas&gt;\n    &lt;script type=\"module\"&gt;\n      const canvas = document.querySelector(\"canvas\"); \n1      // Your WebGPU code will begin here!\n \n    &lt;/script&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n\n1\n\nThis is where we will add our WebGPU code in the next steps.\n\n\n\nBasic HTML5 page with a canvas and a script module. We will fill in the script.\n\n&lt;canvas id=\"gfx\" width=\"512\" height=\"512\"&gt;&lt;/canvas&gt;\n&lt;script type=\"module\"&gt;\nif (!navigator.gpu) {\n  throw new Error(\"WebGPU not supported on this browser.\");\n}\n\n1const adapter = await navigator.gpu.requestAdapter();\nif (!adapter) {\n  throw new Error(\"No appropriate GPUAdapter found.\");\n}\n\n1\n\nRequest a GPU adapter (the physical GPU).\n\n\n\nKey contracts: navigator.gpu, adapter-&gt;device, canvas.getContext(“webgpu”), getPreferredCanvasFormat(). Mention single device driving multiple canvases.\n\n&lt;canvas id=\"gfx\" width=\"512\" height=\"512\"&gt;&lt;/canvas&gt;\n&lt;script type=\"module\"&gt;\nif (!navigator.gpu) {\n  throw new Error(\"WebGPU not supported on this browser.\");\n}\n\n1const adapter = await navigator.gpu.requestAdapter();\nif (!adapter) {\n  throw new Error(\"No appropriate GPUAdapter found.\");\n}\n2const device = await adapter.requestDevice();\n\n1\n\nRequest a GPU adapter (the physical GPU).\n\n2\n\nRequest a logical device from the adapter.\n\n\n\nKey contracts: navigator.gpu, adapter-&gt;device, canvas.getContext(“webgpu”), getPreferredCanvasFormat(). Mention single device driving multiple canvases.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html#check-support-and-prepare-canvas",
    "href": "Courses/10_WebGPU-First-App.html#check-support-and-prepare-canvas",
    "title": "11  Your First WebGPU App",
    "section": "13.2 Check support and prepare canvas",
    "text": "13.2 Check support and prepare canvas\n&lt;canvas id=\"gfx\" width=\"512\" height=\"512\"&gt;&lt;/canvas&gt;\n&lt;script type=\"module\"&gt;\nif (!navigator.gpu) {\n  throw new Error(\"WebGPU not supported on this browser.\");\n}\n\n1const adapter = await navigator.gpu.requestAdapter();\nif (!adapter) {\n  throw new Error(\"No appropriate GPUAdapter found.\");\n}\n2const device = await adapter.requestDevice();\n\n3const canvas = document.getElementById(\"gfx\");\nconst context = canvas.getContext(\"webgpu\");\nconst canvasFormat = navigator.gpu.getPreferredCanvasFormat();\ncontext.configure({ device, format: canvasFormat });\n\n1\n\nRequest a GPU adapter (the physical GPU).\n\n2\n\nRequest a logical device from the adapter.\n\n3\n\nGet the WebGPU context from the canvas and configure it with the device and preferred format.\n\n\n\nKey contracts: navigator.gpu, adapter-&gt;device, canvas.getContext(“webgpu”), getPreferredCanvasFormat(). Mention single device driving multiple canvases.\n\nconst encoder = device.createCommandEncoder(); &lt;!-- &lt;1&gt; --&gt;\n\nCreate a command encoder to record GPU commands.\n\n\nIn order to do that—or pretty much anything else in WebGPU—you need to provide some commands to the GPU instructing it what to do.\nTo do this, have the device create a GPUCommandEncoder, which provides an interface for recording GPU commands.\n\nconst encoder = device.createCommandEncoder(); &lt;!-- &lt;1&gt; --&gt;\nconst pass = encoder.beginRenderPass({ &lt;!-- &lt;2&gt; --&gt;\n  colorAttachments: [{\n    view: context.getCurrentTexture().createView(),\n    loadOp: \"clear\",\n    storeOp: \"store\",\n%  }],\n});\n\nCreate a command encoder to record GPU commands.\nBegin a render pass with a color attachment that clears the canvas to dark blue.\n\n\n\nRender passes in WebGPU execute all drawing commands.\nEach pass starts with beginRenderPass(), specifying output textures.\nFor this app, a single color attachment is used: context.getCurrentTexture().\nThe attachment matches the canvas size and format.\nThe colorAttachment requires a GPUTextureView (not GPUTexture); use createView() to get it.\nloadOp: “clear” clears the texture at the start of the render pass.\nstoreOp: “store” saves the results after the render pass ends.\nStarting the render pass with loadOp: “clear” is enough to clear the canvas, even if no drawing commands are issued.\n\n\nconst encoder = device.createCommandEncoder(); &lt;!-- &lt;1&gt; --&gt;\nconst pass = encoder.beginRenderPass({ &lt;!-- &lt;2&gt; --&gt;\n  colorAttachments: [{\n    view: context.getCurrentTexture().createView(),\n    loadOp: \"clear\",\n    storeOp: \"store\",\n  }],\n});\npass.end(); &lt;!-- &lt;3&gt; --&gt;\n\nCreate a command encoder to record GPU commands.\nBegin a render pass with a color attachment that clears the canvas to dark blue.\nEnd the render pass.\n\n\n\nThese calls only record commands; the GPU does nothing yet.\nCommands are stored in a command encoder for later execution.\nYou must finish recording and submit the command buffer to actually run them.\n\n\nconst encoder = device.createCommandEncoder(); &lt;!-- &lt;1&gt; --&gt;\nconst pass = encoder.beginRenderPass({ &lt;!-- &lt;2&gt; --&gt;\n  colorAttachments: [{\n    view: context.getCurrentTexture().createView(),\n    loadOp: \"clear\",\n    storeOp: \"store\",\n  }],\n});\npass.end(); &lt;!-- &lt;3&gt; --&gt;\nconst commandBuffer = encoder.finish(); &lt;!-- &lt;4&gt; --&gt;\n\nCreate a command encoder to record GPU commands.\nBegin a render pass with a color attachment that clears the canvas to dark blue.\nEnd the render pass.\nFinish recording commands and get a command buffer.\n\n\n\nThese calls only record commands; the GPU does nothing yet.\nCall finish() on the encoder to create a GPUCommandBuffer.\nThe command buffer is an opaque handle to the recorded commands.\n\n\nconst encoder = device.createCommandEncoder();\nconst pass = encoder.beginRenderPass({\n  colorAttachments: [{\n    view: context.getCurrentTexture().createView(),\n    loadOp: \"clear\",\n    storeOp: \"store\",\n  }],\n});\npass.end();\nconst commandBuffer = encoder.finish(); &lt;!-- &lt;4&gt; --&gt;\ndevice.queue.submit([commandBuffer]); &lt;!-- &lt;5&gt; --&gt;\n\nCreate a command encoder to record GPU commands.\nBegin a render pass with a color attachment that clears the canvas to dark blue.\nEnd the render pass.\nFinish recording commands and get a command buffer.\nSubmit the command buffer to the GPU queue for execution.\n\n\n\nSubmit the command buffer to the GPU using device.queue.submit().\nThe queue executes GPU commands in order and synchronizes them.\nsubmit() accepts an array of command buffers; here, only one is used.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html#clear-the-canvas",
    "href": "Courses/10_WebGPU-First-App.html#clear-the-canvas",
    "title": "11  Your First WebGPU App",
    "section": "13.3 Clear the canvas",
    "text": "13.3 Clear the canvas\nconst encoder = device.createCommandEncoder(); &lt;!-- &lt;1&gt; --&gt;\nconst pass = encoder.beginRenderPass({ &lt;!-- &lt;2&gt; --&gt;\n  colorAttachments: [{\n    view: context.getCurrentTexture().createView(),\n    loadOp: \"clear\",\n    storeOp: \"store\",\n  }],\n});\npass.end(); &lt;!-- &lt;3&gt; --&gt;\n// Finish the command buffer and immediately submit it.\ndevice.queue.submit([cencoder.finish()]); &lt;!-- &lt;4&gt; --&gt;\n\nCreate a command encoder to record GPU commands.\nBegin a render pass with a color attachment that clears the canvas to dark blue.\nEnd the render pass.\nSubmit the command buffer to the GPU queue for execution.\n\n\n\nCommand buffers are single-use; after submission, they cannot be reused.\nTo issue more GPU commands, create and submit a new command buffer.\nCommon practice is to create and submit the command buffer in one step, as shown in the sample code.\n\n\nAdd a color to the clear operation\n\nCode\nrenderToCanvas({ width: 512, height: 512 })\n\n\n\n\nconst pass = encoder.beginRenderPass({\n  colorAttachments: [{\n    view: context.getCurrentTexture().createView(),\n    loadOp: \"clear\",\n    storeOp: \"store\",\n  }],\n});",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html#clear-the-canvas---result",
    "href": "Courses/10_WebGPU-First-App.html#clear-the-canvas---result",
    "title": "11  Your First WebGPU App",
    "section": "13.4 Clear the canvas - Result",
    "text": "13.4 Clear the canvas - Result\nAdd a color to the clear operation\n\nCode\nrenderToCanvas({\n  width: 512,\n  height: 512,\n  clearColor: [0.02, 0.06, 0.15, 1.0]\n})\n\n\n\n\nconst pass = encoder.beginRenderPass({\n  colorAttachments: [{\n    view: context.getCurrentTexture().createView(),\n    loadOp: \"clear\",\n    storeOp: \"store\",\n1    clearValue: { r: 0.02, g: 0.06, b: 0.15, a: 1 },\n  }],\n});\n\n1\n\nDark blue RGBA clear color.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html#coordinate-system",
    "href": "Courses/10_WebGPU-First-App.html#coordinate-system",
    "title": "11  Your First WebGPU App",
    "section": "14.1 Coordinate system",
    "text": "14.1 Coordinate system\n\n\n\n\n🟦 GPUs mainly draw triangles (primitives: points, lines, triangles)\n📐 Vertices define triangle corners in X, Y (and Z for 3D) coordinates\n🗺️ WebGPU uses “clip space”: X/Y range from -1 (left/bottom) to +1 (right/top), center is (0,0)\n🧮 Vertex shaders transform vertices to clip space and apply effects\n🎨 Fragment shaders decide pixel colors for each triangle\n🖼️ Final result is shown on screen as a texture\n🕹️ You control the math and effects via shaders!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n🟩 Game of Life grid: visualize cells as colored squares (active) or empty (inactive)\n🟦 Each square needs 4 corner points (coordinates in normalized device space)\n🧮 Use TypedArrays (e.g., Float32Array) to store vertex positions for the GPU\n💡 TypedArrays ensure correct memory layout for GPU APIs like WebGPU\n📌 Place the vertex array declaration near the top, after context.configure()",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html#define-vertices-two-triangles-one-square",
    "href": "Courses/10_WebGPU-First-App.html#define-vertices-two-triangles-one-square",
    "title": "11  Your First WebGPU App",
    "section": "14.2 Define vertices (two triangles = one square)",
    "text": "14.2 Define vertices (two triangles = one square)\n\n\n\n//   X,    Y\nconst vertices = new Float32Array([\n  -0.8, -0.8,  // tri1\n   0.8, -0.8,\n   0.8,  0.8,\n  -0.8, -0.8,  // tri2\n   0.8,  0.8,\n  -0.8,  0.8,\n]);\n\n\n\n\n\n\nNote\n\n\n\n\n🧩 Index Buffers let you reuse vertex data by specifying which vertices form triangles.\n✏️ Avoids duplicating vertex coordinates—just “connect the dots” using indices.\n🚦 Useful for complex shapes, but skipped here for simplicity.\n📚 Recommended for advanced geometry in real-world apps.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGPU draws triangles; duplicate two corners to make two triangles for one quad. Clip space coordinates [-1,1].\n\n1const vertexBuffer = device.createBuffer({\n  label: \"Cell vertices\",\n  size: vertices.byteLength,\n  usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST,\n});\ndevice.queue.writeBuffer(vertexBuffer, 0, vertices);\n\n1\n\nGPU buffer in device-visible memory\n\n\n\n\n🖥️ GPU can’t use JavaScript arrays directly for drawing.\n📦 Use GPUBuffer objects to store data in GPU memory.\n🚀 Buffers are optimized for fast GPU access and flagged for specific uses.\n🧮 Think of GPUBuffer as a GPU-visible TypedArray.\n🛠️ Create a buffer with device.createBuffer() after defining your vertex array.\n\n\n1const vertexBuffer = device.createBuffer({\n  label: \"Cell vertices\",\n2  size: vertices.byteLength,\n  usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST,\n});\ndevice.queue.writeBuffer(vertexBuffer, 0, vertices);\n\n1\n\nGPU buffer in device-visible memory\n\n2\n\nExact byte size from TypedArray.byteLength\n\n\n\n\nAssign a descriptive label to every WebGPU object for easier debugging.\nSpecify the buffer size in bytes; use TypedArray.byteLength for accuracy.\nFor vertex buffers, size equals number of floats × 4 (bytes per float).\nLabels appear in error messages, helping identify issues quickly.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html#create-a-vertex-buffer",
    "href": "Courses/10_WebGPU-First-App.html#create-a-vertex-buffer",
    "title": "11  Your First WebGPU App",
    "section": "14.3 Create a vertex buffer",
    "text": "14.3 Create a vertex buffer\n1const vertexBuffer = device.createBuffer({\n  label: \"Cell vertices\",\n2  size: vertices.byteLength,\n  usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST,\n});\n3device.queue.writeBuffer(vertexBuffer, 0, vertices);\n\n1\n\nGPU buffer in device-visible memory\n\n2\n\nExact byte size from TypedArray.byteLength\n\n3\n\nUpload vertex data from CPU to GPU\n\n\n\n\nUse GPUBufferUsage flags to specify buffer purpose (e.g., VERTEX, COPY_DST).\nGPUBuffer objects are opaque and mostly immutable (size and usage can’t change).\nBuffer memory is zero-initialized on creation.\nUpdate buffer contents using device.queue.writeBuffer() with a TypedArray.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html#byte-sized-maths",
    "href": "Courses/10_WebGPU-First-App.html#byte-sized-maths",
    "title": "11  Your First WebGPU App",
    "section": "14.4 Byte-sized maths",
    "text": "14.4 Byte-sized maths\n\nFloat32Array: 4 bytes per float\nvertices.length = 12 (6 vertices × 2 components)\nvertices.byteLength = 48 (12 × 4 bytes)\n\n\n\n\n\n\n\nImportant\n\n\n\nGet comfortable with this kind of byte-size math. Working with a GPU requires a fair amount of it!\n\n\nconst vertexBufferLayout = {\n1  arrayStride: 2 * 4, // 2 float32 per vertex\n  attributes: [{\n    shaderLocation: 0,\n    offset: 0,\n    format: \"float32x2\",\n  }],\n};\n\n1\n\nStride in bytes between successive vertices\n\n\n\n\nDefine vertex layout using GPUVertexBufferLayout.\narrayStride specifies the byte distance between vertices (2 floats × 4 bytes = 8 bytes).\nEach vertex consists of two 32-bit floats (x, y).\nThis layout tells WebGPU how to interpret buffer data for the vertex shader.\n\n\nconst vertexBufferLayout = {\n1  arrayStride: 2 * 4, // 2 float32 per vertex\n  attributes: [{\n2    shaderLocation: 0,\n    offset: 0,\n    format: \"float32x2\",\n  }],\n};\n\n1\n\nStride in bytes between successive vertices\n\n2\n\nMatches @location(0) in the vertex shader\n\n\n\n\nshaderLocation links a buffer attribute to a vertex shader input.\nMust be a unique number (0-15) for each attribute.\nDefines how vertex data maps to shader variables.\nValues are set now, but used when creating the pipeline.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html#describe-vertex-layout",
    "href": "Courses/10_WebGPU-First-App.html#describe-vertex-layout",
    "title": "11  Your First WebGPU App",
    "section": "14.5 Describe vertex layout",
    "text": "14.5 Describe vertex layout\nconst vertexBufferLayout = {\n1  arrayStride: 2 * 4, // 2 float32 per vertex\n  attributes: [{\n2    shaderLocation: 0,\n    offset: 0,\n3    format: \"float32x2\",\n  }],\n};\n\n1\n\nStride in bytes between successive vertices\n\n2\n\nMatches @location(0) in the vertex shader\n\n3\n\nTwo float32 values per vertex (x,y)\n\n\n\n\nThe attributes array defines how each piece of vertex data is mapped for the GPU.\nEach attribute specifies its format (e.g., “float32x2” for two 32-bit floats).\nThe offset indicates where in the vertex structure the attribute starts (important for multiple attributes).\nThis example uses a single attribute for position; more complex cases may include color, normals, etc.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html#vertex-shader",
    "href": "Courses/10_WebGPU-First-App.html#vertex-shader",
    "title": "11  Your First WebGPU App",
    "section": "15.1 Vertex shader",
    "text": "15.1 Vertex shader\nconst cellShaderModule = device.createShaderModule({\n  code: /* wgsl */ `\n    @vertex\n1    fn vertexMain(@location(0) pos: vec2f) -&gt;\n3      @builtin(position) vec4f {\n2      return vec4f(pos, 0, 1);\n    }\n  `,\n});\n\n1\n\nMark it with the @builtin(position) attribute. A \\Rightarrow symbol is used to indicate that this is what the function returns.\n\n2\n\nReturn the center of clip space (0,0) with W=1\n\n3\n\nAccept vertex position from buffer at @location(0)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html#fragment-shader",
    "href": "Courses/10_WebGPU-First-App.html#fragment-shader",
    "title": "11  Your First WebGPU App",
    "section": "15.2 Fragment shader",
    "text": "15.2 Fragment shader\nconst cellShaderModule = device.createShaderModule({\n  code: /* wgsl */ `\n    @vertex\n    fn vertexMain(@location(0) pos: vec2f) -&gt; @builtin(position) vec4f {\n      return vec4f(pos, 0.0, 1.0);\n    }\n\n    @fragment\n1    fn fragmentMain() -&gt; @location(0) vec4f {\n2      return vec4f(1.0, 0.0, 0.0, 1.0);\n    }\n  `,\n});\n\n1\n\nOutput color to colorAttachment 0\n\n2\n\nSolid red\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can split your vertex and fragment shaders into separate shader modules.\n\\Rightarrow useful if you want to reuse a vertex shader with multiple fragment shaders.\n\n\n\n\nFragment shaders run for every pixel inside drawn triangles, after vertex shaders.\nThey output a color (vec4f: red, green, blue, alpha) for each pixel.\nThe @fragment attribute marks a WGSL fragment shader function.\nThe returned color uses @location(0) to target the first color attachment.\nFragment shaders are highly parallel and flexible in their inputs/outputs.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html#create-pipeline",
    "href": "Courses/10_WebGPU-First-App.html#create-pipeline",
    "title": "11  Your First WebGPU App",
    "section": "16.1 Create pipeline",
    "text": "16.1 Create pipeline\nconst cellPipeline = device.createRenderPipeline({\n  label: \"ellell pipeline\",\n1  layout: \"auto\",\n  vertex: { module: cellShaderModule, entryPoint: \"vertexMain\",\n            buffers: [vertexBufferLayout] },\n  fragment: { module: cellShaderModule, entryPoint: \"fragmentMain\",\n2              targets: [{ format: canvasFormat }] },\n  primitive: { topology: \"triangle-list\" },\n});\n\n1\n\nLet WebGPU infer bind group layout from shaders (no uniforms yet)\n\n2\n\nMatch the canvas texture format\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe render pipeline object centralizes configuration, ensuring only valid combinations are used.\nValidation occurs once at creation, making subsequent draw calls faster.\nBundling options reduces the number of JavaScript calls needed for rendering.\nImproves efficiency and performance compared to WebGL, which validates settings on every draw.\n\n\n\n\n\nShader modules must be used within a GPURenderPipeline to render.\nThe pipeline specifies shaders, vertex buffer layout, geometry type, and more.\nMost pipeline options are optional; only a few are needed to start.\nUse “auto” for layout if you have no uniforms or extra inputs.\nVertex stage requires the shader module, entry point, and buffer layout.\nFragment stage requires the shader module, entry point, and target formats.\nTarget formats must match the canvas/context format.\nThis setup covers the basics for rendering with WebGPU.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html#draw-the-square",
    "href": "Courses/10_WebGPU-First-App.html#draw-the-square",
    "title": "11  Your First WebGPU App",
    "section": "16.2 Draw the square",
    "text": "16.2 Draw the square\n// After encoder.beginRenderPass()\n\npass.setPipeline(cellPipeline);\npass.setVertexBuffer(0, vertexBuffer);\npass.draw(vertices.length / 2); // 6 vertices\n\n// before pass.end()\n\n\nsetPipeline() selects the GPU pipeline (shaders, vertex layout, state).\nsetVertexBuffer() binds the vertex buffer to the pipeline (index 0).\ndraw() issues the draw call, rendering the specified number of vertices.\nCalculating the vertex count from the array makes the code flexible for other shapes.\n\n\n\nCELL_CLEAR_COLOR = [0.02, 0.06, 0.15, 1.0]\n\ncellVertices = new Float32Array([\n  -0.8, -0.8,\n   0.8, -0.8,\n   0.8,  0.8,\n  -0.8, -0.8,\n   0.8,  0.8,\n  -0.8,  0.8,\n])\n\ncreateCellVertexBuffer = () =&gt; {\n  const buffer = device.createBuffer({\n    label: \"Cell vertices\",\n    size: cellVertices.byteLength,\n    usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST,\n  });\n  device.queue.writeBuffer(buffer, 0, cellVertices);\n  return buffer;\n}\n\ncellVertexBufferLayout = ({\n  arrayStride: (2 * Float32Array.BYTES_PER_ELEMENT),\n  attributes: [{\n    shaderLocation: 0,\n    offset: 0,\n    format: \"float32x2\",\n  }]\n})\n\ncreateCellPipeline = (shaderModule) =&gt; device.createRenderPipeline({\n  label: \"Cell pipeline\",\n  layout: \"auto\",\n  vertex: { module: shaderModule, entryPoint: \"vertexMain\", buffers: [cellVertexBufferLayout] },\n  fragment: { module: shaderModule, entryPoint: \"fragmentMain\", targets: [{ format: presentationFormat }] },\n  primitive: { topology: \"triangle-list\" },\n})",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html#the-square",
    "href": "Courses/10_WebGPU-First-App.html#the-square",
    "title": "11  Your First WebGPU App",
    "section": "16.3 The square",
    "text": "16.3 The square\n\nvertexBuffer = createCellVertexBuffer();\n\n\n\n\n\n\n\n{\n  return renderToCanvas({\n    width: 512,\n    height: 512,\n    clearColor: CELL_CLEAR_COLOR,\n    render: withBindings({\n      pipeline: pipeline_square,\n      vertex: [{ slot: 0, buffer: vertexBuffer }],\n      draw: { vertexCount: cellVertices.length / 2 }\n    })\n  });\n}\n\n\n\n\n\n\n\npipeline_square = {\n\n  const cellShaderModule = device.createShaderModule({\n    code: /* wgsl */ `\n      @vertex\n      fn vertexMain(@location(0) pos: vec2f) -&gt; @builtin(position) vec4f {\n        return vec4f(pos, 0.0, 1.0);\n      }\n\n      @fragment\n      fn fragmentMain() -&gt; @location(0) vec4f {\n        return vec4f(1.0, 0.0, 0.0, 1.0);\n      }\n    `,\n  });\n\n  return createCellPipeline(cellShaderModule);\n}",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html#define-the-grid",
    "href": "Courses/10_WebGPU-First-App.html#define-the-grid",
    "title": "11  Your First WebGPU App",
    "section": "17.1 Define the grid",
    "text": "17.1 Define the grid\n\n\nChoose GRID_SIZE (power-of-two is convenient); treat grid as square\nGoal: draw GRID_SIZE × GRID_SIZE quads efficiently via instancing\nPass common parameters via a small uniform buffer\n\n\nconst GRID_SIZE = 32;\nconst cell = 1.8 / GRID_SIZE; // leave a small gap between cells\n\n\nTo render a grid, you need to know its dimensions (width and height); for simplicity, use a square grid with a power-of-two size.\nStart with a small grid (e.g., 4x4) for easier math and demonstration; you can scale up later.\nRendering many squares requires making each cell smaller and drawing many instances.\nYou could manually define all square vertices, but this uses more memory and is less efficient.\nA more GPU-friendly approach is to use instancing, letting the GPU efficiently draw multiple squares with minimal data.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html#uniform-buffer-grid",
    "href": "Courses/10_WebGPU-First-App.html#uniform-buffer-grid",
    "title": "11  Your First WebGPU App",
    "section": "17.2 Uniform buffer (grid)",
    "text": "17.2 Uniform buffer (grid)\n// Create a uniform buffer that describes the grid.\nconst uniformArray = new Float32Array([GRID_SIZE, GRID_SIZE]);\nconst uniformBuffer = device.createBuffer({\n  label: \"Grid Uniforms\",\n  size: uniformArray.byteLength,\n  usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,\n});\ndevice.queue.writeBuffer(uniformBuffer, 0, uniformArray);\n\nUniform buffers must be 16-byte aligned\nGrid as two u32s\nCell size as two f32s\n\n\n\nCommunicate grid size to the shader using uniforms instead of hard-coding.\nUniforms provide values that are constant for all shader invocations (e.g., grid size, time).\nUniforms are stored in GPUBuffer objects with GPUBufferUsage.UNIFORM.\nUsing uniforms allows dynamic changes without recreating shaders or pipelines.\nData types for uniforms can be float or integer, depending on shader needs.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html#access-uniforms-in-a-shader",
    "href": "Courses/10_WebGPU-First-App.html#access-uniforms-in-a-shader",
    "title": "11  Your First WebGPU App",
    "section": "17.3 Access uniforms in a shader",
    "text": "17.3 Access uniforms in a shader\n// At the top of the `code` string in the createShaderModule() call\n1@group(0) @binding(0) var&lt;uniform&gt; grid: vec2f;\n\n@vertex\nfn vertexMain(@location(0) pos: vec2f) -&gt;\n  @builtin(position) vec4f {\n2  return vec4f(pos / grid, 0, 1);\n}\n\n// ...fragmentMain is unchanged \n\n1\n\nDeclare a uniform variable (vec2f for two floats)\n\n2\n\nUse it to scale vertex positions\n\n\n\n\nA uniform called grid (a 2D float vector) is defined in the shader and matches data in the uniform buffer.\nThe uniform is bound at @group(0) and @binding(0) in the shader.\nThe shader divides the vertex position by the grid vector, performing component-wise division.\nThis operation is common in GPU shaders for rendering and compute tasks.\nUsing a grid size of 4 makes the rendered square one-fourth its original size, allowing four squares per row or column.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html#bind-group",
    "href": "Courses/10_WebGPU-First-App.html#bind-group",
    "title": "11  Your First WebGPU App",
    "section": "17.4 Bind group",
    "text": "17.4 Bind group\nconst bindGroup = device.createBindGroup({\n  label: \"Cell renderer bind group\",\n  layout: cellPipeline.getBindGroupLayout(0),\n  entries: [{\n    binding: 0,\n    resource: { buffer: uniformBuffer }\n  }],\n});\n\n\nDeclaring a uniform in the shader does not automatically connect it to a buffer; you must create a bind group.\nA bind group is a collection of GPU resources (buffers, textures, etc.) made accessible to shaders.\nCreate a bind group using a layout that describes the resource types; with layout: “auto”, you can get it from the pipeline.\nEach bind group entry specifies a binding index (matching @binding in the shader) and the resource to bind.\nBind groups are immutable handles; you can update buffer contents, but not the resources the group points to.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html#bind-the-group",
    "href": "Courses/10_WebGPU-First-App.html#bind-the-group",
    "title": "11  Your First WebGPU App",
    "section": "17.5 Bind the group",
    "text": "17.5 Bind the group\npass.setPipeline(cellPipeline);\npass.setVertexBuffer(0, vertexBuffer);\n\npass.setBindGroup(0, bindGroup);\n\npass.draw(vertices.length / 2);",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html#binded-result",
    "href": "Courses/10_WebGPU-First-App.html#binded-result",
    "title": "11  Your First WebGPU App",
    "section": "17.6 Binded result",
    "text": "17.6 Binded result\n\nfunction createUniformBuffer(GRID_SIZE) {\n  const uniformArray = new Float32Array([GRID_SIZE, GRID_SIZE]);\n  const buffer = device.createBuffer({\n    label: \"Grid Uniforms\",\n    size: uniformArray.byteLength,\n    usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,\n  });\n  device.queue.writeBuffer(buffer, 0, uniformArray);\n  return buffer;\n}\n\n\n\n\n\n\n\nrenderGrid = (pipeline, GRID_SIZE, instanceCount) =&gt; {\n  const uniformBuffer = createUniformBuffer(GRID_SIZE);\n\n  const bindGroup = device.createBindGroup({\n    label: \"Cell renderer bind group\",\n    layout: pipeline.getBindGroupLayout(0),\n    entries: [{ binding: 0, resource: { buffer: uniformBuffer } }],\n  });\n\n  return renderToCanvas({\n    width: 512,\n    height: 512,\n    clearColor: CELL_CLEAR_COLOR,\n    render: withBindings({\n      pipeline: pipeline,\n      bindGroups: [bindGroup],\n      vertex: [{ slot: 0, buffer: vertexBuffer }],\n      draw: { vertexCount: cellVertices.length / 2, instanceCount: instanceCount }\n    })\n  });\n}\n\n\n\n\n\n\n\npipeline_grid1 = {\n\n  const cellShaderModule = device.createShaderModule({\n    code: /* wgsl */ `\n      @group(0) @binding(0) var&lt;uniform&gt; grid: vec2f;\n\n      @vertex\n      fn vertexMain(@location(0) pos: vec2f) -&gt; @builtin(position) vec4f {\n        return vec4f(pos / grid, 0.0, 1.0);\n      }\n\n      @fragment\n      fn fragmentMain() -&gt; @location(0) vec4f {\n        return vec4f(1.0, 0.0, 0.0, 1.0);\n      }\n    `,\n  });\n\n  return createCellPipeline(cellShaderModule);\n}\n\nrenderGrid(pipeline_grid1, 4);",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html#draw-a-grid-instancing-1",
    "href": "Courses/10_WebGPU-First-App.html#draw-a-grid-instancing-1",
    "title": "11  Your First WebGPU App",
    "section": "17.7 Draw a grid (instancing)",
    "text": "17.7 Draw a grid (instancing)\n\npipeline_grid2 = {\n  const cellShaderModule = device.createShaderModule({\n    code: /* wgsl */ `\n      @group(0) @binding(0) var&lt;uniform&gt; grid: vec2f;\n\n      @vertex\n      fn vertexMain(@location(0) pos: vec2f,\n                    @builtin(instance_index) instance: u32) -&gt;\n        @builtin(position) vec4f {\n\n        let i = f32(instance);\n        // Compute the cell coordinate from the instance_index\n        let cell = vec2f(i % grid.x, floor(i / grid.x));\n\n        let cellOffset = cell / grid * 2;\n        let gridPos = (pos + 1) / grid - 1 + cellOffset;\n\n        return vec4f(gridPos, 0, 1);\n      }\n\n      @fragment\n      fn fragmentMain() -&gt; @location(0) vec4f {\n        return vec4f(1.0, 0.0, 0.0, 1.0);\n      }\n    `,\n  });\n\n  return createCellPipeline(cellShaderModule);\n}\n\nrenderGrid(pipeline_grid2, 32, 32*32);",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html#color-the-grid",
    "href": "Courses/10_WebGPU-First-App.html#color-the-grid",
    "title": "11  Your First WebGPU App",
    "section": "17.8 Color the grid",
    "text": "17.8 Color the grid\n\npipeline_grid3 = {\n  const cellShaderModule = device.createShaderModule({\n    code: /* wgsl */ `\n      struct VertexInput {\n        @location(0) pos: vec2f,\n        @builtin(instance_index) instance: u32,\n      };\n\n      struct VertexOutput {\n        @builtin(position) pos: vec4f,\n        @location(0) cell: vec2f, // New line!\n      };\n\n      @group(0) @binding(0) var&lt;uniform&gt; grid: vec2f;\n\n      @vertex\n      fn vertexMain(input: VertexInput) -&gt; VertexOutput  {\n        let i = f32(input.instance);\n        let cell = vec2f(i % grid.x, floor(i / grid.x));\n        let cellOffset = cell / grid * 2;\n        let gridPos = (input.pos + 1) / grid - 1 + cellOffset;\n        \n        var output: VertexOutput;\n        output.pos = vec4f(gridPos, 0, 1);\n        output.cell = cell; // New line!\n        return output;\n      }\n\n      @fragment\n      fn fragmentMain(input: VertexOutput) -&gt; @location(0) vec4f {\n        let c = input.cell / grid;\n        return vec4f(c, 1-c.x, 1);\n      }\n    `,\n  });\n\n  return createCellPipeline(cellShaderModule);\n}\n\nrenderGrid(pipeline_grid3, 32, 32*32);",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html#cell-states-storage-buffer",
    "href": "Courses/10_WebGPU-First-App.html#cell-states-storage-buffer",
    "title": "11  Your First WebGPU App",
    "section": "17.9 Cell states (storage buffer)",
    "text": "17.9 Cell states (storage buffer)\n\npipeline_storage = {\n  const cellShaderModule = device.createShaderModule({\n    code: /* wgsl */ `\n      struct VertexInput {\n        @location(0) pos: vec2f,\n        @builtin(instance_index) instance: u32,\n      };\n\n      struct VertexOutput {\n        @builtin(position) pos: vec4f,\n        @location(0) cell: vec2f, // New line!\n      };\n\n      @group(0) @binding(0) var&lt;uniform&gt; grid: vec2f;\n      @group(0) @binding(1) var&lt;storage&gt; cellState: array&lt;u32&gt;; // New!          \n    \n      @vertex\n      fn vertexMain(@location(0) pos: vec2f,\n                    @builtin(instance_index) instance: u32) -&gt; VertexOutput {\n        let i = f32(instance);\n        let cell = vec2f(i % grid.x, floor(i / grid.x));\n        let state = f32(cellState[instance]); // New line!\n\n        let cellOffset = cell / grid * 2;\n        // New: Scale the position by the cell's active state.\n        let gridPos = (pos*state+1) / grid - 1 + cellOffset;\n\n        var output: VertexOutput;\n        output.pos = vec4f(gridPos, 0, 1);\n        output.cell = cell;\n        return output;\n      }\n\n      @fragment\n      fn fragmentMain(input: VertexOutput) -&gt; @location(0) vec4f {\n        let c = input.cell / grid;\n        return vec4f(c, 1-c.x, 1);\n      }\n    `,\n  });\n\n  return createCellPipeline(cellShaderModule);\n}\n\n\n\n\n\n\n\n{\n  const GRID_SIZE = 32;\n  const uniformBuffer = createUniformBuffer(GRID_SIZE);\n\n  // Create an array representing the active state of each cell.\n  const cellStateArray = new Uint32Array(GRID_SIZE * GRID_SIZE);\n\n  // Create a storage buffer to hold the cell state.\n  const cellStateStorage = device.createBuffer({\n    label: \"Cell State\",\n    size: cellStateArray.byteLength,\n    usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,\n  });\n\n  // Mark every third cell of the grid as active.\n  for (let i = 0; i &lt; cellStateArray.length; i += 3) {\n    cellStateArray[i] = 1;\n  }\n  device.queue.writeBuffer(cellStateStorage, 0, cellStateArray);\n\n  const bindGroup = device.createBindGroup({\n    label: \"Cell renderer bind group\",\n    layout: pipeline_storage.getBindGroupLayout(0),\n    entries: [{\n      binding: 0,\n      resource: { buffer: uniformBuffer }\n    },\n    // New entry!\n    {\n      binding: 1,\n      resource: { buffer: cellStateStorage }\n    }],\n  });\n\n  return renderToCanvas({\n    width: 512,\n    height: 512,\n    clearColor: CELL_CLEAR_COLOR,\n    render: withBindings({\n      pipeline: pipeline_storage,\n      bindGroups: [bindGroup],\n      vertex: [{ slot: 0, buffer: vertexBuffer }],\n      draw: { vertexCount: cellVertices.length / 2, instanceCount: GRID_SIZE*GRID_SIZE }\n    })\n  });\n\n}",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html#pingpong-buffers",
    "href": "Courses/10_WebGPU-First-App.html#pingpong-buffers",
    "title": "11  Your First WebGPU App",
    "section": "17.10 Pingpong buffers",
    "text": "17.10 Pingpong buffers\n\n{\n  const GRID_SIZE = 32;\n  const uniformBuffer = createUniformBuffer(GRID_SIZE);\n\n  // Create an array representing the active state of each cell.\n  const cellStateArray = new Uint32Array(GRID_SIZE * GRID_SIZE);\n\n  // Create two storage buffers to hold the cell state.\n  const cellStateStorage = [\n    device.createBuffer({\n      label: \"Cell State A\",\n      size: cellStateArray.byteLength,\n      usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,\n    }),\n    device.createBuffer({\n      label: \"Cell State B\",\n      size: cellStateArray.byteLength,\n      usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,\n    })\n  ];\n\n  // Mark every third cell of the first grid as active.\n  for (let i = 0; i &lt; cellStateArray.length; i+=3) {\n    cellStateArray[i] = 1;\n  }\n  device.queue.writeBuffer(cellStateStorage[0], 0, cellStateArray);\n\n  // Mark every other cell of the second grid as active.\n  for (let i = 0; i &lt; cellStateArray.length; i++) {\n    cellStateArray[i] = i % 2;\n  }\n  device.queue.writeBuffer(cellStateStorage[1], 0, cellStateArray);\n\n  const bindGroups = [\n    device.createBindGroup({\n      label: \"Cell renderer bind group A\",\n      layout: pipeline_storage.getBindGroupLayout(0),\n      entries: [{\n        binding: 0,\n        resource: { buffer: uniformBuffer }\n      }, {\n        binding: 1,\n        resource: { buffer: cellStateStorage[0] }\n      }],\n    }),\n    device.createBindGroup({\n      label: \"Cell renderer bind group B\",\n      layout: pipeline_storage.getBindGroupLayout(0),\n      entries: [{\n        binding: 0,\n        resource: { buffer: uniformBuffer }\n      }, {\n        binding: 1,\n        resource: { buffer: cellStateStorage[1] }\n      }],\n    })\n  ];\n\n  const canvas = createCanvas(512, 512);\n  const ctx = setupContext(canvas, device, presentationFormat);\n\n  const UPDATE_INTERVAL = 200; // Update every 200ms (5 times/sec)\n  let step = 0; // Track how many simulation steps have been run\n\n  // Move all of our rendering code into a function\n  function updateGrid() {\n    step++; // Increment the step count\n    \n    // Start a render pass \n    const { encoder, pass } = beginPass(device, ctx, CELL_CLEAR_COLOR);\n\n    // Draw the grid.\n    pass.setPipeline(pipeline_storage);\n    pass.setBindGroup(0, bindGroups[step % 2]); // Updated!\n    pass.setVertexBuffer(0, vertexBuffer);\n    pass.draw(cellVertices.length / 2, GRID_SIZE * GRID_SIZE);\n\n    // End the render pass and submit the command buffer\n    pass.end();\n    device.queue.submit([encoder.finish()]);\n  }\n\n  // Schedule updateGrid() to run repeatedly\n  setInterval(updateGrid, UPDATE_INTERVAL);\n\n  return canvas;\n}",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html#run-the-simulation",
    "href": "Courses/10_WebGPU-First-App.html#run-the-simulation",
    "title": "11  Your First WebGPU App",
    "section": "17.11 Run the simulation",
    "text": "17.11 Run the simulation\n\n{\n\n  const cellShaderModule = device.createShaderModule({\n    code: /* wgsl */ `\n      struct VertexInput {\n        @location(0) pos: vec2f,\n        @builtin(instance_index) instance: u32,\n      };\n\n      struct VertexOutput {\n        @builtin(position) pos: vec4f,\n        @location(0) cell: vec2f, // New line!\n      };\n\n      @group(0) @binding(0) var&lt;uniform&gt; grid: vec2f;\n      @group(0) @binding(1) var&lt;storage&gt; cellState: array&lt;u32&gt;; // New!          \n    \n      @vertex\n      fn vertexMain(@location(0) pos: vec2f,\n                    @builtin(instance_index) instance: u32) -&gt; VertexOutput {\n        let i = f32(instance);\n        let cell = vec2f(i % grid.x, floor(i / grid.x));\n        let state = f32(cellState[instance]); // New line!\n\n        let cellOffset = cell / grid * 2;\n        // New: Scale the position by the cell's active state.\n        let gridPos = (pos*state+1) / grid - 1 + cellOffset;\n\n        var output: VertexOutput;\n        output.pos = vec4f(gridPos, 0, 1);\n        output.cell = cell;\n        return output;\n      }\n\n      @fragment\n      fn fragmentMain(input: VertexOutput) -&gt; @location(0) vec4f {\n        let c = input.cell / grid;\n        return vec4f(c, 1-c.x, 1);\n      }\n    `,\n  });\n\n\n  // Create the compute shader that will process the simulation.\n  const WORKGROUP_SIZE = 8;\n  const simulationShaderModule = device.createShaderModule({\n    label: \"Game of Life simulation shader\",\n    code: `\n      @group(0) @binding(0) var&lt;uniform&gt; grid: vec2f;\n\n      @group(0) @binding(1) var&lt;storage&gt; cellStateIn: array&lt;u32&gt;;\n      @group(0) @binding(2) var&lt;storage, read_write&gt; cellStateOut: array&lt;u32&gt;;\n        \n      fn cellIndex(cell: vec2u) -&gt; u32 {\n        return cell.y * u32(grid.x) + cell.x;\n      }\n\n      @compute @workgroup_size(${WORKGROUP_SIZE}, ${WORKGROUP_SIZE})\n      fn computeMain(@builtin(global_invocation_id) cell: vec3u) {\n        // New lines. Flip the cell state every step.\n        if (cellStateIn[cellIndex(cell.xy)] == 1) {\n          cellStateOut[cellIndex(cell.xy)] = 0;\n        } else {\n          cellStateOut[cellIndex(cell.xy)] = 1;\n        }\n      }\n    `\n  });\n\n  const GRID_SIZE = 32;\n  const uniformBuffer = createUniformBuffer(GRID_SIZE);\n\n  // Create an array representing the active state of each cell.\n  const cellStateArray = new Uint32Array(GRID_SIZE * GRID_SIZE);\n\n  // Create two storage buffers to hold the cell state.\n  const cellStateStorage = [\n    device.createBuffer({\n      label: \"Cell State A\",\n      size: cellStateArray.byteLength,\n      usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,\n    }),\n    device.createBuffer({\n      label: \"Cell State B\",\n      size: cellStateArray.byteLength,\n      usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,\n    })\n  ];\n\n  // Mark every third cell of the first grid as active.\n  for (let i = 0; i &lt; cellStateArray.length; i+=3) {\n    cellStateArray[i] = 1;\n  }\n  device.queue.writeBuffer(cellStateStorage[0], 0, cellStateArray);\n\n  // Mark every other cell of the second grid as active.\n  for (let i = 0; i &lt; cellStateArray.length; i++) {\n    cellStateArray[i] = i % 2;\n  }\n  device.queue.writeBuffer(cellStateStorage[1], 0, cellStateArray);\n\n\n  // Create the bind group layout and pipeline layout.\n  const bindGroupLayout = device.createBindGroupLayout({\n    label: \"Cell Bind Group Layout\",\n    entries: [{\n      binding: 0,\n      // Fragment stage included because the `grid` uniform is also read in fragmentMain.\n      visibility: GPUShaderStage.VERTEX | GPUShaderStage.FRAGMENT | GPUShaderStage.COMPUTE,\n      buffer: {} // Grid uniform buffer\n    }, {\n      binding: 1,\n      visibility: GPUShaderStage.VERTEX | GPUShaderStage.COMPUTE,\n      buffer: { type: \"read-only-storage\"} // Cell state input buffer\n    }, {\n      binding: 2,\n      visibility: GPUShaderStage.COMPUTE,\n      buffer: { type: \"storage\"} // Cell state output buffer\n    }]\n  });\n\n  // Create a bind group to pass the grid uniforms into the pipeline\n  const bindGroups = [\n    device.createBindGroup({\n      label: \"Cell renderer bind group A\",\n      layout: bindGroupLayout, // Updated Line\n      entries: [{\n        binding: 0,\n        resource: { buffer: uniformBuffer }\n      }, {\n        binding: 1,\n        resource: { buffer: cellStateStorage[0] }\n      }, {\n        binding: 2, // New Entry\n        resource: { buffer: cellStateStorage[1] }\n      }],\n    }),\n    device.createBindGroup({\n      label: \"Cell renderer bind group B\",\n      layout: bindGroupLayout, // Updated Line\n\n      entries: [{\n        binding: 0,\n        resource: { buffer: uniformBuffer }\n      }, {\n        binding: 1,\n        resource: { buffer: cellStateStorage[1] }\n      }, {\n        binding: 2, // New Entry\n        resource: { buffer: cellStateStorage[0] }\n      }],\n    }),\n  ];\n\n  const pipelineLayout = device.createPipelineLayout({\n    label: \"Cell Pipeline Layout\",\n    bindGroupLayouts: [ bindGroupLayout ],\n  });\n\n  const cellPipeline = device.createRenderPipeline({\n    label: \"Cell pipeline\",\n    layout: pipelineLayout,\n    vertex: {\n      module: cellShaderModule,\n      entryPoint: \"vertexMain\",\n      buffers: [cellVertexBufferLayout]\n    },\n    fragment: {\n      module: cellShaderModule,\n      entryPoint: \"fragmentMain\",\n      targets: [{ format: presentationFormat }]\n    }\n  });\n\n  // Create a compute pipeline that updates the game state.\n  const simulationPipeline = device.createComputePipeline({\n    label: \"Simulation pipeline\",\n    layout: pipelineLayout,\n    compute: {\n      module: simulationShaderModule,\n      entryPoint: \"computeMain\",\n    }\n  });\n\n  const canvas = createCanvas(512, 512);\n  const ctx = setupContext(canvas, device, presentationFormat);\n\n  const UPDATE_INTERVAL = 200; // Update every 200ms (5 times/sec)\n  let step = 0; // Track how many simulation steps have been run\n\n  // Move all of our rendering code into a function\n  function updateGrid() {\n    \n    // Start a render pass \n    const encoder = device.createCommandEncoder();\n\n    const computePass = encoder.beginComputePass();\n\n    computePass.setPipeline(simulationPipeline);\n    computePass.setBindGroup(0, bindGroups[step % 2]);\n\n    // New lines\n    const workgroupCount = Math.ceil(GRID_SIZE / WORKGROUP_SIZE);\n    computePass.dispatchWorkgroups(workgroupCount, workgroupCount);\n\n    computePass.end();\n\n    step++; // Increment the step count\n\n    const view = ctx.getCurrentTexture().createView();\n\n    const pass = encoder.beginRenderPass({\n      colorAttachments: [{\n        view: view,\n        loadOp: \"clear\",\n        storeOp: \"store\",\n        clearValue: { r: 0.02, g: 0.06, b: 0.15, a: 1 },\n      }],\n    });\n\n\n    // Draw the grid.\n    pass.setPipeline(cellPipeline);\n    pass.setBindGroup(0, bindGroups[step % 2]); // Updated!\n    pass.setVertexBuffer(0, vertexBuffer);\n    pass.draw(cellVertices.length / 2, GRID_SIZE * GRID_SIZE);\n\n    // End the render pass and submit the command buffer\n    pass.end();\n    device.queue.submit([encoder.finish()]);\n  }\n\n  // Schedule updateGrid() to run repeatedly\n  setInterval(updateGrid, UPDATE_INTERVAL);\n\n  return canvas;\n\n}",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/10_WebGPU-First-App.html#run-the-simulation-final",
    "href": "Courses/10_WebGPU-First-App.html#run-the-simulation-final",
    "title": "11  Your First WebGPU App",
    "section": "17.12 Run the simulation (final)",
    "text": "17.12 Run the simulation (final)\n\n{\n\n  const cellShaderModule = device.createShaderModule({\n    code: /* wgsl */ `\n      struct VertexInput {\n        @location(0) pos: vec2f,\n        @builtin(instance_index) instance: u32,\n      };\n\n      struct VertexOutput {\n        @builtin(position) pos: vec4f,\n        @location(0) cell: vec2f, // New line!\n      };\n\n      @group(0) @binding(0) var&lt;uniform&gt; grid: vec2f;\n      @group(0) @binding(1) var&lt;storage&gt; cellState: array&lt;u32&gt;; // New!          \n    \n      @vertex\n      fn vertexMain(@location(0) pos: vec2f,\n                    @builtin(instance_index) instance: u32) -&gt; VertexOutput {\n        let i = f32(instance);\n        let cell = vec2f(i % grid.x, floor(i / grid.x));\n        let state = f32(cellState[instance]); // New line!\n\n        let cellOffset = cell / grid * 2;\n        // New: Scale the position by the cell's active state.\n        let gridPos = (pos*state+1) / grid - 1 + cellOffset;\n\n        var output: VertexOutput;\n        output.pos = vec4f(gridPos, 0, 1);\n        output.cell = cell;\n        return output;\n      }\n\n      @fragment\n      fn fragmentMain(input: VertexOutput) -&gt; @location(0) vec4f {\n        let c = input.cell / grid;\n        return vec4f(c, 1-c.x, 1);\n      }\n    `,\n  });\n\n\n  // Create the compute shader that will process the simulation.\n  const WORKGROUP_SIZE = 8;\n  const simulationShaderModule = device.createShaderModule({\n    label: \"Life simulation shader\",\n    code: `\n      @group(0) @binding(0) var&lt;uniform&gt; grid: vec2f;\n\n      @group(0) @binding(1) var&lt;storage&gt; cellStateIn: array&lt;u32&gt;;\n      @group(0) @binding(2) var&lt;storage, read_write&gt; cellStateOut: array&lt;u32&gt;;\n\n      fn cellIndex(cell: vec2u) -&gt; u32 {\n        return (cell.y % u32(grid.y)) * u32(grid.x) +\n                (cell.x % u32(grid.x));\n      }\n\n      fn cellActive(x: u32, y: u32) -&gt; u32 {\n        return cellStateIn[cellIndex(vec2(x, y))];\n      }\n\n      @compute @workgroup_size(${WORKGROUP_SIZE}, ${WORKGROUP_SIZE})\n      fn computeMain(@builtin(global_invocation_id) cell: vec3u) {\n        // Determine how many active neighbors this cell has.\n        let activeNeighbors = cellActive(cell.x+1, cell.y+1) +\n                              cellActive(cell.x+1, cell.y) +\n                              cellActive(cell.x+1, cell.y-1) +\n                              cellActive(cell.x, cell.y-1) +\n                              cellActive(cell.x-1, cell.y-1) +\n                              cellActive(cell.x-1, cell.y) +\n                              cellActive(cell.x-1, cell.y+1) +\n                              cellActive(cell.x, cell.y+1);\n\n        let i = cellIndex(cell.xy);\n\n        // Conway's game of life rules:\n        switch activeNeighbors {\n          case 2: {\n            cellStateOut[i] = cellStateIn[i];\n          }\n          case 3: {\n            cellStateOut[i] = 1;\n          }\n          default: {\n            cellStateOut[i] = 0;\n          }\n        }\n      }\n    `\n  });\n  const GRID_SIZE = 32;\n  const uniformBuffer = createUniformBuffer(GRID_SIZE);\n\n  // Create an array representing the active state of each cell.\n  const cellStateArray = new Uint32Array(GRID_SIZE * GRID_SIZE);\n\n  // Create two storage buffers to hold the cell state.\n  const cellStateStorage = [\n    device.createBuffer({\n      label: \"Cell State A\",\n      size: cellStateArray.byteLength,\n      usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,\n    }),\n    device.createBuffer({\n      label: \"Cell State B\",\n      size: cellStateArray.byteLength,\n      usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,\n    })\n  ];\n\n  // Set each cell to a random state, then copy the JavaScript array \n  // into the storage buffer.\n  for (let i = 0; i &lt; cellStateArray.length; ++i) {\n    cellStateArray[i] = Math.random() &gt; 0.6 ? 1 : 0;\n  }\n  device.queue.writeBuffer(cellStateStorage[0], 0, cellStateArray);\n\n  // Mark every other cell of the second grid as active.\n  for (let i = 0; i &lt; cellStateArray.length; i++) {\n    cellStateArray[i] = i % 2;\n  }\n  device.queue.writeBuffer(cellStateStorage[1], 0, cellStateArray);\n\n\n  // Create the bind group layout and pipeline layout.\n  const bindGroupLayout = device.createBindGroupLayout({\n    label: \"Cell Bind Group Layout\",\n    entries: [{\n      binding: 0,\n      // Fragment stage included because the `grid` uniform is also read in fragmentMain.\n      visibility: GPUShaderStage.VERTEX | GPUShaderStage.FRAGMENT | GPUShaderStage.COMPUTE,\n      buffer: {} // Grid uniform buffer\n    }, {\n      binding: 1,\n      visibility: GPUShaderStage.VERTEX | GPUShaderStage.COMPUTE,\n      buffer: { type: \"read-only-storage\"} // Cell state input buffer\n    }, {\n      binding: 2,\n      visibility: GPUShaderStage.COMPUTE,\n      buffer: { type: \"storage\"} // Cell state output buffer\n    }]\n  });\n\n  // Create a bind group to pass the grid uniforms into the pipeline\n  const bindGroups = [\n    device.createBindGroup({\n      label: \"Cell renderer bind group A\",\n      layout: bindGroupLayout, // Updated Line\n      entries: [{\n        binding: 0,\n        resource: { buffer: uniformBuffer }\n      }, {\n        binding: 1,\n        resource: { buffer: cellStateStorage[0] }\n      }, {\n        binding: 2, // New Entry\n        resource: { buffer: cellStateStorage[1] }\n      }],\n    }),\n    device.createBindGroup({\n      label: \"Cell renderer bind group B\",\n      layout: bindGroupLayout, // Updated Line\n\n      entries: [{\n        binding: 0,\n        resource: { buffer: uniformBuffer }\n      }, {\n        binding: 1,\n        resource: { buffer: cellStateStorage[1] }\n      }, {\n        binding: 2, // New Entry\n        resource: { buffer: cellStateStorage[0] }\n      }],\n    }),\n  ];\n\n  const pipelineLayout = device.createPipelineLayout({\n    label: \"Cell Pipeline Layout\",\n    bindGroupLayouts: [ bindGroupLayout ],\n  });\n\n  const cellPipeline = device.createRenderPipeline({\n    label: \"Cell pipeline\",\n    layout: pipelineLayout,\n    vertex: {\n      module: cellShaderModule,\n      entryPoint: \"vertexMain\",\n      buffers: [cellVertexBufferLayout]\n    },\n    fragment: {\n      module: cellShaderModule,\n      entryPoint: \"fragmentMain\",\n      targets: [{ format: presentationFormat }]\n    }\n  });\n\n  // Create a compute pipeline that updates the game state.\n  const simulationPipeline = device.createComputePipeline({\n    label: \"Simulation pipeline\",\n    layout: pipelineLayout,\n    compute: {\n      module: simulationShaderModule,\n      entryPoint: \"computeMain\",\n    }\n  });\n\n  const canvas = createCanvas(512, 512);\n  const ctx = setupContext(canvas, device, presentationFormat);\n\n  const UPDATE_INTERVAL = 200; // Update every 200ms (5 times/sec)\n  let step = 0; // Track how many simulation steps have been run\n\n  // Move all of our rendering code into a function\n  function updateGrid() {\n    \n    // Start a render pass \n    const encoder = device.createCommandEncoder();\n\n    const computePass = encoder.beginComputePass();\n\n    computePass.setPipeline(simulationPipeline);\n    computePass.setBindGroup(0, bindGroups[step % 2]);\n\n    // New lines\n    const workgroupCount = Math.ceil(GRID_SIZE / WORKGROUP_SIZE);\n    computePass.dispatchWorkgroups(workgroupCount, workgroupCount);\n\n    computePass.end();\n\n    step++; // Increment the step count\n\n    const view = ctx.getCurrentTexture().createView();\n\n    const pass = encoder.beginRenderPass({\n      colorAttachments: [{\n        view: view,\n        loadOp: \"clear\",\n        storeOp: \"store\",\n        clearValue: { r: 0.02, g: 0.06, b: 0.15, a: 1 },\n      }],\n    });\n\n\n    // Draw the grid.\n    pass.setPipeline(cellPipeline);\n    pass.setBindGroup(0, bindGroups[step % 2]); // Updated!\n    pass.setVertexBuffer(0, vertexBuffer);\n    pass.draw(cellVertices.length / 2, GRID_SIZE * GRID_SIZE);\n\n    // End the render pass and submit the command buffer\n    pass.end();\n    device.queue.submit([encoder.finish()]);\n  }\n\n  // Schedule updateGrid() to run repeatedly\n  setInterval(updateGrid, UPDATE_INTERVAL);\n\n  return canvas;\n\n}",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Your First WebGPU App</span>"
    ]
  },
  {
    "objectID": "Courses/11_0_Designing-parallel.html",
    "href": "Courses/11_0_Designing-parallel.html",
    "title": "12  Designing parallel algorithms",
    "section": "",
    "text": "13 Designing parallel algorithms",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Designing parallel algorithms</span>"
    ]
  },
  {
    "objectID": "Courses/11_0_Designing-parallel.html#rowcolumn-wise-matrix-sum",
    "href": "Courses/11_0_Designing-parallel.html#rowcolumn-wise-matrix-sum",
    "title": "12  Designing parallel algorithms",
    "section": "13.1 Row/Column-wise matrix sum",
    "text": "13.1 Row/Column-wise matrix sum\n\nMatrix A = [a_{ij}]_{i=1:N}^{j=1:P} of dimension N \\times P\n\n\n\\begin{bmatrix}\n    \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n    \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n    \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & a_{ij} & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\\\\\\n    \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n    \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n  \\end{bmatrix}_{N \\times P}\n\n\nRow-wise sum: vector C = [c_{i}]_{i=1:N} of size N where c_{i} = \\sum_{j=1}^P a_{ij}\nColumn-wise sum: vector D = [d_{j}]_{j=1:P} of size P where d_{j} = \\sum_{i=1}^N a_{ij}",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Designing parallel algorithms</span>"
    ]
  },
  {
    "objectID": "Courses/11_0_Designing-parallel.html#row-wise-sum",
    "href": "Courses/11_0_Designing-parallel.html#row-wise-sum",
    "title": "12  Designing parallel algorithms",
    "section": "13.2 Row-wise sum",
    "text": "13.2 Row-wise sum\n\n\\begin{bmatrix}\n    \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n    \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n    \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & a_{ij} & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\\\\\\n    \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n    \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n  \\end{bmatrix}_{N \\times P}\\ \\ \\rightarrow \\ \\ \\begin{bmatrix}\n    \\vdots \\\\\n    \\vdots \\\\\n    \\sum_{j=1}^{P} a_{ij} \\\\\n    \\vdots\\\\\n    \\vdots\\\\\n  \\end{bmatrix}_{N \\times 1}",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Designing parallel algorithms</span>"
    ]
  },
  {
    "objectID": "Courses/11_0_Designing-parallel.html#column-wise-sum",
    "href": "Courses/11_0_Designing-parallel.html#column-wise-sum",
    "title": "12  Designing parallel algorithms",
    "section": "13.3 Column-wise sum",
    "text": "13.3 Column-wise sum\n\n\\begin{array}{c}\n\\begin{bmatrix}\n    \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n    \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n    \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & a_{ij} & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\\\\\\n    \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n    \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n  \\end{bmatrix}_{N \\times P}\\\\\n  \\downarrow \\ \\ \\ \\ \\ \\ \\\\\n\\begin{bmatrix}\n   \\ \\dots \\  & \\dots & \\sum_{i=1}^{N} a_{ij} & \\dots & \\ \\dots \\ \\\\\n  \\end{bmatrix}_{1\\times P}\\\\\n\\end{array}",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Designing parallel algorithms</span>"
    ]
  },
  {
    "objectID": "Courses/11_0_Designing-parallel.html#rowcolumn-wise-matrix-sum-algorithm",
    "href": "Courses/11_0_Designing-parallel.html#rowcolumn-wise-matrix-sum-algorithm",
    "title": "12  Designing parallel algorithms",
    "section": "13.4 Row/Column-wise matrix sum algorithm",
    "text": "13.4 Row/Column-wise matrix sum algorithm\n\n\nRow-wise sum:\n# input\nmatA = np.array(...).reshape(N,P)\n# output\nvecD = np.zeros(N)\n# algorithm\nfor i in range(N):\n  for j in range(P):\n    vecD[i] += matA[i,j]\n\nColumn-wise sum:\n# input\nmatA = np.array(...).reshape(N,P)\n# output\nvecD = np.zeros(P)\n# algorithm\nfor j in range(P):\n  for i in range(N):\n    vecD[j] += matA[i,j]\n\n\nExercise: parallel algorithm?\n\n\nSolution 1?\n# input\nmatA = np.array(...).reshape(N,P)\n# output\nvecD = np.zeros(N)\n# algorithm\n@parallel\nfor i in range(N):\n  for j in range(P):\n    vecD[i] += matA[i,j]\n\nSolution 2?\n# input\nmatA = np.array(...).reshape(N,P)\n# output\nvecD = np.zeros(P)\n# algorithm\n@parallel\nfor j in range(P):\n  for i in range(N):\n    vecD[i] += matA[i,j]\n\n\nExercise: any concurrent access to memory by the parallel tasks ? in input (reading) ? in output (writing) ?",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Designing parallel algorithms</span>"
    ]
  },
  {
    "objectID": "Courses/11_0_Designing-parallel.html#concurrent-access-to-memory",
    "href": "Courses/11_0_Designing-parallel.html#concurrent-access-to-memory",
    "title": "12  Designing parallel algorithms",
    "section": "13.5 Concurrent access to memory",
    "text": "13.5 Concurrent access to memory\nSolution 1:\n\nreading (input): no concurrent access\nwriting (output): no concurrent access\n\nSolution 2:\n\nreading (input): no concurrent access\nwriting (output): what happen if tasks j_1 and j_2 need to simultaneously update vecD[i]?\n\n\n\\rightarrow need for synchronization (with a time cost)\n\n\n\nSolution 3?\n# input\nmatA = np.array(...).reshape(N,P)\n# output\nvecD = np.zeros(P)\n# algorithm\nfor j in range(P):\n  @parallel\n  for i in range(N):\n    vecD[i] += matA[i,j]\n\nSolution 4?\n# input\nmatA = np.array(...).reshape(N,P)\n# output\nvecD = np.zeros(N)\n# algorithm\nfor i in range(N):\n  @parallel\n  for j in range(P):\n    vecD[i] += matA[i,j]\n(Concurrent access between tasks to update vecD[i])\n\n\nAny other issue ?",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Designing parallel algorithms</span>"
    ]
  },
  {
    "objectID": "Courses/11_0_Designing-parallel.html#cost-of-parallel-task-management",
    "href": "Courses/11_0_Designing-parallel.html#cost-of-parallel-task-management",
    "title": "12  Designing parallel algorithms",
    "section": "13.6 Cost of parallel task management",
    "text": "13.6 Cost of parallel task management\n\n\n@parallel\nfor i in range(N):\n  for j in range(P):\n    ...\n1 launch of N parallel tasks running each P operations\n\\rightarrow N “long” parallel tasks\nCost (in time) to launch parallel tasks \\sim O(N)\n\nfor j in range(P):\n  @parallel\n  for i in range(N):\n    ...\nP launches of N parallel tasks running each 1 operation\n\\rightarrow N \\times P “short” parallel tasks\nCost (in time) to launch parallel tasks \\sim O(NP)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Designing parallel algorithms</span>"
    ]
  },
  {
    "objectID": "Courses/11_0_Designing-parallel.html#parallel-column-wise-matrix-sum-algorithm",
    "href": "Courses/11_0_Designing-parallel.html#parallel-column-wise-matrix-sum-algorithm",
    "title": "12  Designing parallel algorithms",
    "section": "13.7 Parallel column-wise matrix sum algorithm",
    "text": "13.7 Parallel column-wise matrix sum algorithm\n\n\nSolution 1?\n# input\nmatA = np.array(...).reshape(N,P)\n# output\nvecD = np.zeros(P)\n# algorithm\n@parallel\nfor j in range(P):\n  for i in range(N):\n    vecD[j] += matA[i,j]\n\nSolution 2?\n# input\nmatA = np.array(...).reshape(N,P)\n# output\nvecD = np.zeros(P)\n# algorithm\n@parallel\nfor i in range(N):\n  for j in range(P):\n    vecD[j] += matA[i,j]\nConcurrent access between tasks to update vecD[j]",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Designing parallel algorithms</span>"
    ]
  },
  {
    "objectID": "Courses/11_0_Designing-parallel.html#illustration-columnrow-wise-matrix-sum-algorithm",
    "href": "Courses/11_0_Designing-parallel.html#illustration-columnrow-wise-matrix-sum-algorithm",
    "title": "12  Designing parallel algorithms",
    "section": "13.8 Illustration: column/row-wise matrix sum algorithm",
    "text": "13.8 Illustration: column/row-wise matrix sum algorithm\nParallel column-wise vs parallel row-wise matrix sum algorithms\n\n\n\nMatrix 10000 \\times 10000\nObjective: run 10000 tasks\nResources: 64 computing units\nNumber of workers: 1, 2, 4, 8, 16, 32\n\nExercise 1: why the performance degradation?\n\n{\\rightarrow overhead for memory access}\n\nExercise 2: why the performance difference?\n\n{\\rightarrow impact of array storage order}\n\n\n\n\n20 repetitions in each configurations",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Designing parallel algorithms</span>"
    ]
  },
  {
    "objectID": "Courses/11_0_Designing-parallel.html#array-storage-order",
    "href": "Courses/11_0_Designing-parallel.html#array-storage-order",
    "title": "12  Designing parallel algorithms",
    "section": "13.9 Array storage order",
    "text": "13.9 Array storage order\nMatrix in memory = a big array of contiguous rows or columns\n\n\n\n13.9.1 Row-major\nMemory: \n\\begin{array}{|c|c|c|}\n\\hline\na_{11} & a_{12} & a_{13} \\\\\n\\hline\n\\end{array}\\\n\\begin{array}{|c|c|c|}\n\\hline\na_{21} & a_{22} & a_{23} \\\\\n\\hline\n\\end{array}\\\n\\begin{array}{|c|c|c|}\n\\hline\na_{31} & a_{32} & a_{33} \\\\\n\\hline\n\\end{array}\n\n\n\n13.9.2 Column-major\nMemory: \n\\begin{array}{|c|c|c|}\n\\hline\na_{11} & a_{21} & a_{31} \\\\\n\\hline\n\\end{array}\\\n\\begin{array}{|c|c|c|}\n\\hline\na_{12} & a_{22} & a_{32} \\\\\n\\hline\n\\end{array}\\\n\\begin{array}{|c|c|c|}\n\\hline\na_{13} & a_{23} & a_{33} \\\\\n\\hline\n\\end{array}\n\n\n\n\n\nSource: wikimedia.org",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Designing parallel algorithms</span>"
    ]
  },
  {
    "objectID": "Courses/11_0_Designing-parallel.html#accessing-array-elements-in-memory",
    "href": "Courses/11_0_Designing-parallel.html#accessing-array-elements-in-memory",
    "title": "12  Designing parallel algorithms",
    "section": "13.10 Accessing array elements in memory",
    "text": "13.10 Accessing array elements in memory\nMemory access: read data from memory by block\n\n\nTo access a_{11}: load \\begin{array}{|c|c|c|} \\hline a_{11} & a_{12} & a_{13} \\\\ \\hline \\end{array} into cache\nTo access a_{11}: load \\begin{array}{|c|c|c|} \\hline a_{11} & a_{21} & a_{31} \\\\ \\hline \\end{array} into cache\n\n\n\nSource: wikimedia.org",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Designing parallel algorithms</span>"
    ]
  },
  {
    "objectID": "Courses/11_0_Designing-parallel.html#row-major-order-and-row-wise-sum",
    "href": "Courses/11_0_Designing-parallel.html#row-major-order-and-row-wise-sum",
    "title": "12  Designing parallel algorithms",
    "section": "13.11 Row-major order and row-wise sum",
    "text": "13.11 Row-major order and row-wise sum\nTo compute a_{11} + a_{12} + a_{13} ?\n\n\n\n\ninit res =0\nload \\begin{array}{|c|c|c|} \\hline a_{11} & a_{12} & a_{13} \\\\ \\hline \\end{array} into cache\ncompute res = res + a_{11}\ncompute res = res + a_{12}\ncompute res = res + a_{13}\n\n\n\n\n\nSource: wikimedia.org",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Designing parallel algorithms</span>"
    ]
  },
  {
    "objectID": "Courses/11_0_Designing-parallel.html#column-major-order-and-row-wise-sum",
    "href": "Courses/11_0_Designing-parallel.html#column-major-order-and-row-wise-sum",
    "title": "12  Designing parallel algorithms",
    "section": "13.12 Column-major order and row-wise sum",
    "text": "13.12 Column-major order and row-wise sum\nTo compute a_{11} + a_{12} + a_{13} ?\n\n\n\n\ninit res =0\nload \\begin{array}{|c|c|c|} \\hline a_{11} & a_{21} & a_{31} \\\\ \\hline \\end{array} into cache\ncompute res = res + a_{11}\nload \\begin{array}{|c|c|c|} \\hline a_{12} & a_{22} & a_{32} \\\\ \\hline \\end{array} into cache\ncompute res = res + a_{12}\nload \\begin{array}{|c|c|c|} \\hline a_{13} & a_{23} & a_{33} \\\\ \\hline \\end{array} into cache\ncompute res = res + a_{13} More memory accesses \\rightarrow time consuming\n\n\n\n\n\nSource: wikimedia.org",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Designing parallel algorithms</span>"
    ]
  },
  {
    "objectID": "Courses/11_0_Designing-parallel.html#memory-access-to-large-data",
    "href": "Courses/11_0_Designing-parallel.html#memory-access-to-large-data",
    "title": "12  Designing parallel algorithms",
    "section": "13.13 Memory access to large data",
    "text": "13.13 Memory access to large data\nExample: “big” matrix (4 \\times 6) \\tiny \\begin{array}{|c|c|c|c|c|c|c|}\\hline a_{11} & a_{12} & a_{13} & a_{14} & a_{15} & a_{16} \\\\ \\hline a_{21} & a_{22} & a_{23} & a_{24} & a_{25} & a_{26} \\\\ \\hline a_{31} & a_{32} & a_{33} & a_{34} & a_{35} & a_{36} \\\\ \\hline a_{41} & a_{42} & a_{43} & a_{44} & a_{45} & a_{46} \\\\ \\hline\\end{array}\nStorage in memory (row major):\n\\tiny\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\na_{11} & a_{12} & a_{13} & a_{14} & a_{15} & a_{16} \\\\\n\\hline\n\\end{array}\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\na_{21} & a_{22} & a_{23} & a_{24} & a_{25} & a_{26} \\\\\n\\hline\n\\end{array}\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\na_{31} & a_{32} & a_{33} & a_{34} & a_{35} & a_{36} \\\\\n\\hline\n\\end{array}\n\\begin{array}{|c|c|ccc}\n\\hline\na_{41} & a_{42} & \\cdot & \\cdot & \\cdot \\\\\n\\hline\n\\end{array}\n\nAccess by sub-blocks1 of data (e.g. sub-block of rows or columns)\n\nExample: load block \\begin{array}{|c|c|c|} \\hline a_{11} & a_{12} & a_{13} \\\\ \\hline \\end{array} into cache to access a_{11}",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Designing parallel algorithms</span>"
    ]
  },
  {
    "objectID": "Courses/11_0_Designing-parallel.html#impact-of-data-dimension",
    "href": "Courses/11_0_Designing-parallel.html#impact-of-data-dimension",
    "title": "12  Designing parallel algorithms",
    "section": "13.14 Impact of data dimension",
    "text": "13.14 Impact of data dimension\nSum of row 1 (row major):\n\n\n\naccess block \\begin{array}{|c|c|c|} \\hline a_{11} & a_{12} & a_{13} \\\\ \\hline \\end{array} res = res + a_{11} + a_{12} + a_{13}\n\n\n\naccess block \\begin{array}{|c|c|c|} \\hline a_{14} & a_{15} & a_{16} \\\\ \\hline \\end{array} res = res + a_{14} + a_{15} + a_{16}",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Designing parallel algorithms</span>"
    ]
  },
  {
    "objectID": "Courses/11_0_Designing-parallel.html#impact-of-data-dimension-ii",
    "href": "Courses/11_0_Designing-parallel.html#impact-of-data-dimension-ii",
    "title": "12  Designing parallel algorithms",
    "section": "13.15 Impact of data dimension, II",
    "text": "13.15 Impact of data dimension, II\nSum of row 1 (column major):\n\n\n\naccess block \\begin{array}{|c|c|c|} \\hline a_{11} & a_{21} & a_{31} \\\\ \\hline \\end{array} res = res + a_{11}\naccess block \\begin{array}{|c|c|c|} \\hline a_{12} & a_{22} & a_{32} \\\\ \\hline \\end{array} res = res + a_{12}\naccess block \\begin{array}{|c|c|c|} \\hline a_{13} & a_{23} & a_{33} \\\\ \\hline \\end{array} res = res + a_{13}\n\n\n\naccess block \\begin{array}{|c|c|c|} \\hline a_{14} & a_{24} & a_{34} \\\\ \\hline \\end{array} res = res + a_{14}\naccess block \\begin{array}{|c|c|c|} \\hline a_{15} & a_{25} & a_{35} \\\\ \\hline \\end{array} res = res + a_{14}\naccess block \\begin{array}{|c|c|c|} \\hline a_{16} & a_{26} & a_{36} \\\\ \\hline \\end{array} res = res + a_{16}",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Designing parallel algorithms</span>"
    ]
  },
  {
    "objectID": "Courses/11_0_Designing-parallel.html#matrix-product",
    "href": "Courses/11_0_Designing-parallel.html#matrix-product",
    "title": "12  Designing parallel algorithms",
    "section": "13.16 Matrix product",
    "text": "13.16 Matrix product\n\n\n\nMatrix A = [a_{ij}]_{i=1:N}^{j=1:P} of dimension N\\times{}P\nMatrix B = [b_{jk}]_{j=1:P}^{k=1:Q} of dimension N\\times{}P\nMatrix product: C = A \\times B = [c_{ik}]_{i=1:N}^{k=1:Q} of dimension N\\times{}Q where\n\nc_{ik} = \\sum_{j=1}^P a_{ij} \\times b_{jk}\n\n\n\nSource: wikimedia.org",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Designing parallel algorithms</span>"
    ]
  },
  {
    "objectID": "Courses/11_0_Designing-parallel.html#matrix-product-algorithm",
    "href": "Courses/11_0_Designing-parallel.html#matrix-product-algorithm",
    "title": "12  Designing parallel algorithms",
    "section": "13.17 Matrix product algorithm",
    "text": "13.17 Matrix product algorithm\n# input\nmatA = np.array(...).reshape(N,P)\nmatA = np.array(...).reshape(P,Q)\n# output\nmatC = np.zeros((N,Q))\n# algorithm\nfor i in range(N):\n  for k in range(Q):\n    for j in range(P):\n      matC[i,k] += matA[i,j] * matB[j,k]\nExercise: parallel algorithm?",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Designing parallel algorithms</span>"
    ]
  },
  {
    "objectID": "Courses/11_0_Designing-parallel.html#naive-parallel-matrix-product",
    "href": "Courses/11_0_Designing-parallel.html#naive-parallel-matrix-product",
    "title": "12  Designing parallel algorithms",
    "section": "13.18 (naive) Parallel matrix product",
    "text": "13.18 (naive) Parallel matrix product\n# input\nmatA = np.array(...).reshape(N,P)\nmatA = np.array(...).reshape(P,Q)\n# output\nmatC = np.zeros((N,Q))\n# algorithm\n@parallel\nfor i in range(N):\n  for k in range(Q):\n    for j in range(P):\n      matC[i,k] += matA[i,j] * matB[j,k]",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Designing parallel algorithms</span>"
    ]
  },
  {
    "objectID": "Courses/11_0_Designing-parallel.html#divide-and-conquer-procedure",
    "href": "Courses/11_0_Designing-parallel.html#divide-and-conquer-procedure",
    "title": "12  Designing parallel algorithms",
    "section": "13.19 Divide-and-conquer procedure",
    "text": "13.19 Divide-and-conquer procedure\n\nDivide output and input matrices into blocks:\n\n\nC = \\begin{bmatrix}\nC_{11} & C_{12} \\\\\nC_{21} & C_{22} \\\\\n\\end{bmatrix},\\,\nA = \\begin{bmatrix}\nA_{11} & A_{12} \\\\\nA_{21} & A_{22} \\\\\n\\end{bmatrix},\\,\nB = \\begin{bmatrix}\nB_{11} & B_{12} \\\\\nB_{21} & B_{22} \\\\\n\\end{bmatrix}\n\n\nCompute C = A \\times B by blocks:\n\n\n\\begin{darray}{rcl}\n\\begin{bmatrix}\nC_{11} & C_{12} \\\\\nC_{21} & C_{22} \\\\\n\\end{bmatrix}  & =\n& \\begin{bmatrix}\nA_{11} & A_{12} \\\\\nA_{21} & A_{22} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nB_{11} & B_{12} \\\\\nB_{21} & B_{22} \\\\\n\\end{bmatrix} \\\\\n& = & \\begin{bmatrix}\nA_{11} B_{11} + A_{12} B_{21} & A_{11} B_{12} + A_{12} B_{22}\\\\\nA_{21} B_{11} + A_{22} B_{21} & A_{21} B_{12} + A_{22} B_{22}\\\\\n\\end{bmatrix}\n\\end{darray}\n\n\nPossible parallelization over sub-block products A_{ik} \\times B_{kj} and then over result sums \\rightarrow see also “tiled implementation”",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Designing parallel algorithms</span>"
    ]
  },
  {
    "objectID": "Courses/11_0_Designing-parallel.html#fibonacci-sequence",
    "href": "Courses/11_0_Designing-parallel.html#fibonacci-sequence",
    "title": "12  Designing parallel algorithms",
    "section": "13.20 Fibonacci sequence",
    "text": "13.20 Fibonacci sequence\nInitialization: f_0 = 0, f_1 = 1\nIteration: f_i = f_{i-1} + f_{i-2} for any i \\geq 2\nSequence: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, …\n\n\nAlgorithm:\nn = 100\nfib = np.zeros(100)\nfib[0] = 0\nfib[1] = 1\nfor i in range(2,n):\n    res[i] = res[i-1] + res[i-2]\n\n\nParallel version? (NO!) (at least not directly)\n\\rightarrow result i depends of result from previous iterations (i-1 and i-2) \n\\rightarrow dependency between iterations",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Designing parallel algorithms</span>"
    ]
  },
  {
    "objectID": "Courses/11_0_Designing-parallel.html#markov-chain",
    "href": "Courses/11_0_Designing-parallel.html#markov-chain",
    "title": "12  Designing parallel algorithms",
    "section": "13.21 Markov chain",
    "text": "13.21 Markov chain\nMarkov chain: sequence of random variables (X_i)_{i&gt;1} such that \\mathbb{P}(X_i = x_i \\vert X_1 = x_1, X_2 = x_2, \\dots , X_{i-1} = x_{i-1}) = \\mathbb{P}(X_i = x_i \\vert X_{i-1} = x_{i-1})\nX_i\\in S where S is the state space\n\n\nExample:\n\ntwo states E and A, i.e S = \\{A, E\\}\ntransition probability matrix:\n\n\n\\begin{array}{cl}\n\\begin{array}{cc}\nA & E\n\\end{array} \\\\\n\\left(\\begin{array}{cc}\n0.6 & 0.4 \\\\\n0.3 & 0.7 \\\\\n\\end{array}\\right) &\n\\begin{array}{l}\nA \\\\\nE\n\\end{array}\n\\end{array}\n\n\n\n\n\n\nwikimedia.org",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Designing parallel algorithms</span>"
    ]
  },
  {
    "objectID": "Courses/11_0_Designing-parallel.html#markov-chain-simulation-algorithm",
    "href": "Courses/11_0_Designing-parallel.html#markov-chain-simulation-algorithm",
    "title": "12  Designing parallel algorithms",
    "section": "13.22 Markov chain simulation algorithm",
    "text": "13.22 Markov chain simulation algorithm\n\nPick an initial state X_0 = x with x\\in S\nFor in in 1,\\dots, N:\n\nSimulate X_i\\in S from the probability distribution given by state X_{i-1}\n\n\nFor the simulation:\n\nIf X_{i-1} = A then \\mathbb{P}(X_i = A) = 0.6 and \\mathbb{P}(X_i = E) = 0.4\nIf X_{i-1} = E then \\mathbb{P}(X_i = A) = 0.7 and \\mathbb{P}(X_i = E) = 0.3\n\nExercise: parallel algorithm?\n\nNO!",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Designing parallel algorithms</span>"
    ]
  },
  {
    "objectID": "Courses/11_0_Designing-parallel.html#take-home-message",
    "href": "Courses/11_0_Designing-parallel.html#take-home-message",
    "title": "12  Designing parallel algorithms",
    "section": "14.1 Take home message",
    "text": "14.1 Take home message\n\nParallel computing can be used to run computations faster (i.e. save time)\nRelationship between time gain and number of tasks run in parallel is not linear\nPotential bottlenecks leading to potential performance loss:\n\nmanagement of parallel tasks\noverhead for computing resource access\noverhead for memory access\nconcurrent memory access by parallel tasks",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Designing parallel algorithms</span>"
    ]
  },
  {
    "objectID": "Courses/11_0_Designing-parallel.html#footnotes",
    "href": "Courses/11_0_Designing-parallel.html#footnotes",
    "title": "12  Designing parallel algorithms",
    "section": "",
    "text": "the size of the block depends on the size of the cache↩︎",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Designing parallel algorithms</span>"
    ]
  },
  {
    "objectID": "Courses/11_1_WebGPU-matmul.html",
    "href": "Courses/11_1_WebGPU-matmul.html",
    "title": "13  Exploring WebGPU Matmul Kernels (OJS)",
    "section": "",
    "text": "13.1 WebGPU setup\nWe’ll build up a few compute shaders in WGSL and compare their throughput:\nYou can tweak M, K, N and the kernel, then measure latency and GFLOP/s. This runs entirely in the browser using WebGPU; make sure your browser/device supports it.\n// Import Plotly for advanced plotting\nPlotly = require(\"https://cdn.plot.ly/plotly-latest.min.js\")\ngpu = navigator.gpu\nadapter = gpu && (await navigator.gpu.requestAdapter())\n// Try to request a device with higher limits for large matrices\ndevice = adapter && (await adapter.requestDevice({\n  requiredFeatures: adapter.features.has(\"timestamp-query\") ? [\"timestamp-query\"] : [],\n  requiredLimits: {\n    maxStorageBufferBindingSize: Math.min(adapter.limits.maxStorageBufferBindingSize, 2 * 1024 * 1024 * 1024), // 2GB or adapter max\n    maxBufferSize: Math.min(adapter.limits.maxBufferSize, 2 * 1024 * 1024 * 1024), // 2GB or adapter max\n    maxStorageBuffersPerShaderStage: Math.max(adapter.limits.maxStorageBuffersPerShaderStage, 8),\n    maxComputeWorkgroupStorageSize: Math.max(adapter.limits.maxComputeWorkgroupStorageSize, 32768)\n\n  }\n}))\nqueue = device && device.queue\nwebgpu_ok = !!device\n// A small status banner\nhtml`&lt;div class=\"callout ${webgpu_ok ? 'callout-note' : 'callout-warning'}\"&gt;&lt;strong&gt;WebGPU&lt;/strong&gt;: ${webgpu_ok ? 'OK — device acquired' : 'Not available — try Chrome/Edge/Arc, enable WebGPU, or use a recent macOS'}&lt;/div&gt;`",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Exploring WebGPU Matmul Kernels (OJS)</span>"
    ]
  },
  {
    "objectID": "Courses/11_1_WebGPU-matmul.html#webgpu-setup",
    "href": "Courses/11_1_WebGPU-matmul.html#webgpu-setup",
    "title": "13  Exploring WebGPU Matmul Kernels (OJS)",
    "section": "",
    "text": "13.1.1 Helpers (buffers, shader, run)\n\n// Create a GPU buffer and optionally initialize with data\nfunction createBuffer(device, dataOrSize, usage) {\n  if (typeof dataOrSize === 'number') {\n    const n = dataOrSize\n    const size = Number.isFinite(n) && n &gt; 0 ? ((Math.ceil(n / 4) * 4) &gt;&gt;&gt; 0) : 0\n    if (!Number.isFinite(size) || size &lt;= 0) {\n      throw new Error(`Invalid buffer size: ${n}`)\n    }\n    return device.createBuffer({ size, usage })\n  }\n  const data = dataOrSize\n  const [byteLength, size] = [data.byteLength, ((data.byteLength + 3) & ~3)] // 4-byte align\n  const buffer = device.createBuffer({ size, usage, mappedAtCreation: true })\n  new Uint8Array(buffer.getMappedRange()).set(new Uint8Array(data.buffer, data.byteOffset, data.byteLength))\n  buffer.unmap()\n  return buffer\n}\n\n\n\n\n\n\n\n// Compile a WGSL compute pipeline\nfunction makePipeline(device, wgsl) {\n  const module = device.createShaderModule({ code: wgsl })\n  const pipeline = device.createComputePipeline({\n    layout: 'auto',\n    compute: { module, entryPoint: 'main' }\n  })\n  return pipeline\n}\n\n\n\n\n\n\n\n// Uniform struct needs 16-byte size; pad after M,K,N\nfunction makeDimsUniform(device, M, K, N) {\n  const u32 = new Uint32Array([M, K, N, 0]) // pad\n  return createBuffer(device, u32, GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST)\n}\n\n\n\n\n\n\n\n\nCode\n// CPU reference (for small sizes) to optionally check correctness\nfunction cpuMatmul(A, B, M, K, N) {\n  const C = new Float32Array(M * N)\n  for (let i = 0; i &lt; M; i++) {\n    for (let j = 0; j &lt; N; j++) {\n      let s = 0\n      for (let k = 0; k &lt; K; k++) s += A[i*K + k] * B[k*N + j]\n      C[i*N + j] = s\n    }\n  }\n  return C\n}\n\n\n\n\n\n\n\n\n\n13.1.2 WGSL kernels\nBelow are WGSL strings (paraphrased from the ideas in the article). All kernels expect the same bindings:\n\n@group(0) @binding(0) uniform Dimensions { M,K,N,pad }\n@group(0) @binding(1) storage read A (row-major M×K)\n@group(0) @binding(2) storage read B (row-major K×N)\n@group(0) @binding(3) storage read_write C (row-major M×N)\n\n\n\nCode\nwgsl_naive_1d = /* wgsl */ `\nstruct Dimensions { M: u32, K: u32, N: u32, _pad: u32 };\n@group(0) @binding(0) var&lt;uniform&gt; dims: Dimensions;\n@group(0) @binding(1) var&lt;storage, read&gt; A: array&lt;f32&gt;;\n@group(0) @binding(2) var&lt;storage, read&gt; B: array&lt;f32&gt;;\n@group(0) @binding(3) var&lt;storage, read_write&gt; C: array&lt;f32&gt;;\n\n@compute @workgroup_size(1)\nfn main(@builtin(global_invocation_id) gid: vec3&lt;u32&gt;) {\n  // Handle multi-dimensional dispatch properly\n  let idx = gid.x + gid.y * 65535u + gid.z * 65535u * 65535u;\n  let M = dims.M; let K = dims.K; let N = dims.N;\n  if (idx &gt;= M * N) { return; }\n  let row = idx / N;\n  let col = idx % N;\n  var sum: f32 = 0.0;\n  for (var kk: u32 = 0u; kk &lt; K; kk = kk + 1u) {\n    sum = sum + A[row * K + kk] * B[kk * N + col];\n  }\n  C[row * N + col] = sum;\n}\n`\n\n\n\n\n\n\n\n\n\nCode\nwgsl_1d_many_threads = /* wgsl */ `\nstruct Dimensions { M: u32, K: u32, N: u32, _pad: u32 };\n@group(0) @binding(0) var&lt;uniform&gt; dims: Dimensions;\n@group(0) @binding(1) var&lt;storage, read&gt; A: array&lt;f32&gt;;\n@group(0) @binding(2) var&lt;storage, read&gt; B: array&lt;f32&gt;;\n@group(0) @binding(3) var&lt;storage, read_write&gt; C: array&lt;f32&gt;;\n\n@compute @workgroup_size(256)\nfn main(@builtin(global_invocation_id) gid: vec3&lt;u32&gt;) {\n  // Handle multi-dimensional dispatch properly  \n  let idx = gid.x + gid.y * 65535u + gid.z * 65535u * 65535u;\n  let M = dims.M; let K = dims.K; let N = dims.N;\n  if (idx &gt;= M * N) { return; }\n  let row = idx / N;\n  let col = idx % N;\n  var sum: f32 = 0.0;\n  for (var kk: u32 = 0u; kk &lt; K; kk = kk + 1u) {\n    sum = sum + A[row * K + kk] * B[kk * N + col];\n  }\n  C[row * N + col] = sum;\n}\n`\n\n\n\n\n\n\n\n\n\nCode\nwgsl_2d_8x8 = /* wgsl */ `\nstruct Dimensions { M: u32, K: u32, N: u32, _pad: u32 };\n@group(0) @binding(0) var&lt;uniform&gt; dims: Dimensions;\n@group(0) @binding(1) var&lt;storage, read&gt; A: array&lt;f32&gt;;\n@group(0) @binding(2) var&lt;storage, read&gt; B: array&lt;f32&gt;;\n@group(0) @binding(3) var&lt;storage, read_write&gt; C: array&lt;f32&gt;;\n\n@compute @workgroup_size(8, 8)\nfn main(@builtin(global_invocation_id) gid: vec3&lt;u32&gt;) {\n  // Note: swap to favor coalesced access as discussed in the article\n  let row = gid.y; // M\n  let col = gid.x; // N\n  let M = dims.M; let K = dims.K; let N = dims.N;\n  if (row &gt;= M || col &gt;= N) { return; }\n  var sum: f32 = 0.0;\n  for (var kk: u32 = 0u; kk &lt; K; kk = kk + 1u) {\n    sum = sum + A[row * K + kk] * B[kk * N + col];\n  }\n  C[row * N + col] = sum;\n}\n`\n\n\n\n\n\n\n\n\n\nCode\nwgsl_tile_1x4 = /* wgsl */ `\nconst BLOCKSIZE: u32 = 16u; // 16x16 threads per workgroup\nconst TILE_N: u32 = 4u;     // 4 columns per thread\nstruct Dimensions { M: u32, K: u32, N: u32, _pad: u32 };\n@group(0) @binding(0) var&lt;uniform&gt; dims: Dimensions;\n@group(0) @binding(1) var&lt;storage, read&gt; A: array&lt;f32&gt;;\n@group(0) @binding(2) var&lt;storage, read&gt; B: array&lt;f32&gt;;\n@group(0) @binding(3) var&lt;storage, read_write&gt; C: array&lt;f32&gt;;\n\n@compute @workgroup_size(BLOCKSIZE, BLOCKSIZE)\nfn main(@builtin(global_invocation_id) gid: vec3&lt;u32&gt;) {\n  let M = dims.M; let K = dims.K; let N = dims.N;\n  let row = gid.y;\n  let col0 = gid.x * TILE_N;\n  if (row &gt;= M || col0 &gt;= N) { return; }\n\n  var s0: f32 = 0.0;\n  var s1: f32 = 0.0;\n  var s2: f32 = 0.0;\n  var s3: f32 = 0.0;\n  for (var kk: u32 = 0u; kk &lt; K; kk = kk + 1u) {\n    let a = A[row * K + kk];\n    let base = kk * N + col0;\n    s0 = s0 + a * B[base + 0u];\n    s1 = s1 + a * B[base + 1u];\n    s2 = s2 + a * B[base + 2u];\n    s3 = s3 + a * B[base + 3u];\n  }\n  C[row * N + col0 + 0u] = s0;\n  if (col0 + 1u &lt; N) { C[row * N + col0 + 1u] = s1; }\n  if (col0 + 2u &lt; N) { C[row * N + col0 + 2u] = s2; }\n  if (col0 + 3u &lt; N) { C[row * N + col0 + 3u] = s3; }\n}\n`\n\n\n\n\n\n\n\n\n\nCode\nwgsl_tile_4x4 = /* wgsl */ `\nconst BLOCKSIZE: u32 = 16u;\nconst TILE_M: u32 = 4u; // 4 rows per thread\nconst TILE_N: u32 = 4u; // 4 cols per thread\nstruct Dimensions { M: u32, K: u32, N: u32, _pad: u32 };\n@group(0) @binding(0) var&lt;uniform&gt; dims: Dimensions;\n@group(0) @binding(1) var&lt;storage, read&gt; A: array&lt;f32&gt;;\n@group(0) @binding(2) var&lt;storage, read&gt; B: array&lt;f32&gt;;\n@group(0) @binding(3) var&lt;storage, read_write&gt; C: array&lt;f32&gt;;\n\n@compute @workgroup_size(BLOCKSIZE, BLOCKSIZE)\nfn main(@builtin(global_invocation_id) gid: vec3&lt;u32&gt;) {\n  let M = dims.M; let K = dims.K; let N = dims.N;\n  let row0 = gid.y * TILE_M;\n  let col0 = gid.x * TILE_N;\n  if (row0 &gt;= M || col0 &gt;= N) { return; }\n\n  var s: array&lt;array&lt;f32, TILE_N&gt;, TILE_M&gt;;\n  // initialize\n  for (var ii: u32 = 0u; ii &lt; TILE_M; ii = ii + 1u) {\n    for (var jj: u32 = 0u; jj &lt; TILE_N; jj = jj + 1u) {\n      s[ii][jj] = 0.0;\n    }\n  }\n\n  // unrolled accumulation over K stays as a loop (K unknown at compile time)\n  for (var kk: u32 = 0u; kk &lt; K; kk = kk + 1u) {\n    for (var ii: u32 = 0u; ii &lt; TILE_M; ii = ii + 1u) {\n      let a = A[(row0 + ii) * K + kk];\n      let bbase = kk * N + col0;\n      for (var jj: u32 = 0u; jj &lt; TILE_N; jj = jj + 1u) {\n        s[ii][jj] = s[ii][jj] + a * B[bbase + jj];\n      }\n    }\n  }\n\n  for (var ii: u32 = 0u; ii &lt; TILE_M; ii = ii + 1u) {\n    let r = row0 + ii;\n    if (r &lt; M) {\n      for (var jj: u32 = 0u; jj &lt; TILE_N; jj = jj + 1u) {\n        let c = col0 + jj;\n        if (c &lt; N) { C[r * N + c] = s[ii][jj]; }\n      }\n    }\n  }\n}\n`\n\n\n\n\n\n\n\n\n\n13.1.3 Kernel registry and dispatch math\n\nkernels = [\n  { \n    key: 'naive1d', \n    label: 'Naive 1D (wg_size=1)', \n    wgsl: wgsl_naive_1d, \n    dispatch: ({M,N}) =&gt; {\n      const total = M*N\n      // Respect 65535 limit per dimension, use 2D dispatch if needed\n      if (total &lt;= 65535) return [total, 1, 1]\n      const x = Math.min(65535, Math.ceil(Math.sqrt(total)))\n      const y = Math.ceil(total / x)\n      return [x, Math.min(65535, y), Math.max(1, Math.ceil(y / 65535))]\n    }, \n    limits: { recommendMaxMN: 512 } \n  },\n  { \n    key: 'oneD256', \n    label: '1D many threads (256)', \n    wgsl: wgsl_1d_many_threads, \n    dispatch: ({M,N}) =&gt; {\n      const total = Math.ceil((M*N)/256)\n      if (total &lt;= 65535) return [total, 1, 1]\n      const x = Math.min(65535, Math.ceil(Math.sqrt(total)))\n      const y = Math.ceil(total / x)\n      return [x, Math.min(65535, y), Math.max(1, Math.ceil(y / 65535))]\n    }, \n    limits: { recommendMaxMN: 1024*1024 } \n  },\n  { key: 'twoD8x8', label: '2D 8×8', wgsl: wgsl_2d_8x8, dispatch: ({M,N}) =&gt; [Math.ceil(N/8), Math.ceil(M/8), 1], limits: { recommendMaxMN: 4096*4096 } },\n  { key: 'tile1x4', label: 'Tiled 1×4 (16×16 wg)', wgsl: wgsl_tile_1x4, dispatch: ({M,N}) =&gt; [Math.ceil(N/(16*4)), Math.ceil(M/16), 1], limits: { recommendMaxMN: 4096*4096 } },\n  { key: 'tile4x4', label: 'Tiled 4×4 (16×16 wg)', wgsl: wgsl_tile_4x4, dispatch: ({M,N}) =&gt; [Math.ceil(N/(16*4)), Math.ceil(M/(16*4)), 1], limits: { recommendMaxMN: 4096*4096 } }\n]\n\nnormalizeKernelKey = value =&gt; {\n  if (typeof value === 'string') return value\n  if (value && typeof value === 'object') {\n    if (typeof value.value === 'string') return value.value\n    if (typeof value.key === 'string') return value.key\n  }\n  return kernels[0]?.key ?? 'naive1d'\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Non-reactive input controls (mutable)\nmutable m_value = 512\n\n\n\n\n\n\n\nmutable k_value = 512\n\n\n\n\n\n\n\nmutable n_value = 512\n\n\n\n\n\n\n\nmutable kernel_value = 'naive1d'\n\n\n\n\n\n\n\nmutable verify_value = false\n\n\n\n\n\n\n\nmax_dim = 16384\n\nm_input = {\n  const input = Inputs.range([64, max_dim], {value: mutable m_value, step: 16, label: 'M (rows of A/C)'})\n  input.addEventListener('input', () =&gt; mutable m_value = input.value)\n  return input\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nk_input = {\n  const input = Inputs.range([64, max_dim], {value: mutable k_value, step: 16, label: 'K (cols of A / rows of B)'})\n  input.addEventListener('input', () =&gt; mutable k_value = input.value)\n  return input\n}\n\n\n\n\n\n\n\nn_input = {\n  const input = Inputs.range([64, max_dim], {value: mutable n_value, step: 16, label: 'N (cols of B/C)'})\n  input.addEventListener('input', () =&gt; mutable n_value = input.value)\n  return input\n}\n\n\n\n\n\n\n\nkernel_input = {\n  const input = Inputs.select(\n    kernels,\n    {\n      label: 'Kernel',\n      format: d =&gt; d.label,\n      value: mutable kernel_value,\n      reduce: d =&gt; d.key\n    }\n  )\n  input.addEventListener('input', () =&gt; mutable kernel_value = input.value)\n  return input\n}\n\n\n\n\n\n\n\nverify_input = {\n  const input = Inputs.toggle({label: 'Verify (CPU check if small)', value: mutable verify_value})\n  input.addEventListener('input', () =&gt; mutable verify_value = input.value)\n  return input\n}\n\n\n\n\n\n\n\n// Group all inputs visually\nhtml`&lt;div&gt;\n  ${m_input}\n  ${k_input}\n  ${n_input}\n  ${kernel_input}\n  ${verify_input}\n&lt;/div&gt;`\n\n\n\n\n\n\n\n// Control buttons\nhtml`&lt;div style=\"display: flex; gap: 0.5rem; margin-top: 1rem;\"&gt;\n  ${viewof run}\n  ${viewof sweep}\n  ${viewof download_results}\n  ${viewof clear_results}\n&lt;/div&gt;`\n\n\n\n\n\n\n\n// Internal cache to store history without creating reactive self-dependencies\nresults_history_cache = (() =&gt; {\n  const data = []\n  return {\n    get value() {\n      return data.slice()\n    },\n    push(item) {\n      data.push(item)\n      return data.slice()\n    },\n    clear() {\n      data.length = 0\n      return data.slice()\n    }\n  }\n})()\n\n\n\n\n\n\n\n// Mutable array to store all results\nmutable results_history = []\n\n\n\n\n\n\n\n// Only the submit button triggers computation - increment on each click\nviewof run = Inputs.button('Run matmul', { reduce: v =&gt; (v ?? 0) + 1 })\n\n\n\n\n\n\n\n// Sweep button for running all kernels with power-of-2 dimensions\nviewof sweep = Inputs.button('Sweep all kernels', { reduce: v =&gt; (v ?? 0) + 1 })\n\n\n\n\n\n\n\n// Download results button\nviewof download_results = Inputs.button('Download CSV', { reduce: v =&gt; (v ?? 0) + 1 })\n\n\n\n\n\n\n\n// Clear results button\nviewof clear_results = Inputs.button('Clear results', { reduce: v =&gt; (v ?? 0) + 1 })\n\n\n\n\n\n\n\n// Handle clear button clicks\nclear_handler = {\n  if (clear_results &gt; 0) {\n    const emptied = results_history_cache.clear()\n    mutable results_history = emptied\n  }\n  return clear_results\n}\n\n\n\n\n\n\n\n// Handle download button clicks\ndownload_handler = {\n  if (download_results &gt; 0) {\n    const history = results_history || []\n    if (history.length === 0) {\n      alert('No results to download. Run some tests first.')\n      return\n    }\n    \n    // Convert results to CSV format\n    const csvData = history.map(result =&gt; ({\n      'Run': result.runNumber ?? '',\n      'Kernel': result.kernel ?? '',\n      'M': result.M ?? '',\n      'K': result.K ?? '',\n      'N': result.N ?? '',\n      'Memory_MB': result.totalMB ? result.totalMB.toFixed(1) : '',\n      'Dispatch_X': result.dx ?? '',\n      'Dispatch_Y': result.dy ?? '',\n      'Dispatch_Z': result.dz ?? '',\n      'GPU_Time_ms': result.ms ? result.ms.toFixed(2) : '',\n      'Total_Time_ms': result.totalMs ? result.totalMs.toFixed(2) : '',\n      'GPU_Throughput_GFLOPS': result.gflops ? result.gflops.toFixed(2) : '',\n      'CPU_Time_ms': result.cpuTime ? result.cpuTime.toFixed(2) : '',\n      'CPU_Throughput_GFLOPS': result.cpuGflops ? result.cpuGflops.toFixed(2) : '',\n      'GPU_Speedup': (result.cpuTime && result.ms && result.ms &gt; 0) ? (result.cpuTime / result.ms).toFixed(1) : '',\n      'Max_Error_vs_CPU': result.check?.maxAbs ? result.check.maxAbs.toExponential(2) : ''\n    }))\n    \n    // Convert to CSV string\n    const headers = Object.keys(csvData[0])\n    const csvContent = [\n      headers.join(','),\n      ...csvData.map(row =&gt; headers.map(header =&gt; row[header]).join(','))\n    ].join('\\n')\n    \n    // Create and trigger download\n    const blob = new Blob([csvContent], { type: 'text/csv;charset=utf-8;' })\n    const link = document.createElement('a')\n    const url = URL.createObjectURL(blob)\n    link.setAttribute('href', url)\n    link.setAttribute('download', `webgpu-matmul-results-${new Date().toISOString().split('T')[0]}.csv`)\n    link.style.visibility = 'hidden'\n    document.body.appendChild(link)\n    link.click()\n    document.body.removeChild(link)\n    URL.revokeObjectURL(url)\n  }\n  return download_results\n}\n\n\n\n\n\n\n\n// Sweep runner - runs all kernels with power-of-2 dimensions\nsweep_results = {\n  if (!sweep) return null\n  \n  if (!webgpu_ok || !device) {\n    const error = 'WebGPU not available'\n    mutable compute_status = { state: 'error', message: error, error }\n    mutable status_trigger = mutable status_trigger + 1\n    return { error }\n  }\n\n  const dimensions = [512, 1024, 2048, 4096]\n  const allKernels = kernels.map(k =&gt; k.key)\n  const totalRuns = dimensions.length * allKernels.length\n  let completedRuns = 0\n  \n  mutable compute_status = {\n    state: 'running',\n    message: `Running sweep: 0/${totalRuns} completed`,\n    spinner: true\n  }\n  mutable status_trigger = mutable status_trigger + 1\n  \n  // Yield to browser for status update\n  await new Promise(requestAnimationFrame)\n  \n  try {\n    for (const kernelKey of allKernels) {\n      for (const dim of dimensions) {\n        const dims = { M: dim, K: dim, N: dim }\n        const totalMB = (dim * dim + dim * dim + dim * dim) * 4 / 1024 / 1024\n        \n        // Update status for current run\n        const kernelInfo = kernels.find(k =&gt; k.key === kernelKey)\n        const kernelLabel = kernelInfo?.label ?? kernelKey\n        \n        mutable compute_status = {\n          state: 'running',\n          message: `Running sweep: ${completedRuns}/${totalRuns} - ${kernelLabel} ${dim}×${dim}×${dim}`,\n          spinner: true\n        }\n        mutable status_trigger = mutable status_trigger + 1\n        \n        // Yield to browser for status update\n        await new Promise(requestAnimationFrame)\n        \n        const result = await runMatmul(dims, kernelKey, false) // Don't verify during sweep\n        \n        if (result && !result.error) {\n          const enriched = { \n            ...result, \n            totalMB,\n            runNumber: Date.now() + completedRuns // Use timestamp + counter for unique IDs\n          }\n          \n          // Append to history\n          const updatedHistory = results_history_cache.push(enriched)\n          mutable results_history = updatedHistory\n        }\n        \n        completedRuns++\n      }\n    }\n    \n    mutable compute_status = {\n      state: 'done',\n      message: `Sweep completed: ${completedRuns}/${totalRuns} runs finished`,\n      spinner: false\n    }\n    mutable status_trigger = mutable status_trigger + 1\n    \n    return { completed: completedRuns, total: totalRuns }\n  } catch (e) {\n    const error = e.message || e.toString()\n    mutable compute_status = { \n      state: 'error', \n      message: `Sweep failed: ${error} (completed ${completedRuns}/${totalRuns})`, \n      error \n    }\n    mutable status_trigger = mutable status_trigger + 1\n    return { error, completed: completedRuns, total: totalRuns }\n  }\n}\n\n\n\n\n\n\n\n\n13.1.4 Runner\n\n// Create typed arrays and buffers, dispatch, read back C if needed, and time it\nasync function runMatmul({M,K,N}, kernelKey, verify, onStatus = null) {\n  if (!device) return { error: 'No WebGPU device' }\n\n  const totalStart = performance.now()\n\n  const notify = (message, spinning = true) =&gt; {\n    if (onStatus) onStatus({ message, spinning })\n  }\n\n  notify('1/9 Initializing computation...')\n  await new Promise(requestAnimationFrame)\n\n  const key = (typeof kernelKey === 'string')\n    ? kernelKey\n    : (kernelKey && typeof kernelKey === 'object' && ('value' in kernelKey || 'key' in kernelKey))\n      ? (kernelKey.value ?? kernelKey.key)\n      : (kernelKey ?? 'twoD8x8')\n  const ker = kernels.find(k =&gt; k.key === key)\n  if (!ker) return { error: 'Unknown kernel' }\n\n  M = Math.max(1, Math.floor(+M || 0))\n  K = Math.max(1, Math.floor(+K || 0))\n  N = Math.max(1, Math.floor(+N || 0))\n\n  notify('2/9 Creating input matrices...')\n  await new Promise(requestAnimationFrame)\n\n  const elemA = M*K, elemB = K*N, elemC = M*N\n  const A = new Float32Array(elemA)\n  const B = new Float32Array(elemB)\n  for (let i = 0; i &lt; elemA; i++) A[i] = (Math.sin(i * 17.13) + 1) * 0.5\n  for (let i = 0; i &lt; elemB; i++) B[i] = (Math.cos(i * 9.97) + 1) * 0.5\n\n  notify('3/9 Creating GPU buffers...')\n  await new Promise(requestAnimationFrame)\n\n  const aBuf = createBuffer(device, A, GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST)\n  const bBuf = createBuffer(device, B, GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST)\n  const bytesC = elemC * 4\n  const sizeBytesC = Math.ceil(bytesC / 4) * 4\n  const cBuf = createBuffer(device, sizeBytesC, GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC | GPUBufferUsage.COPY_DST)\n  const uBuf = makeDimsUniform(device, M, K, N)\n\n  notify('4/9 Setting up GPU pipeline...')\n  await new Promise(requestAnimationFrame)\n\n  const pipeline = makePipeline(device, ker.wgsl)\n  const bind = device.createBindGroup({ layout: pipeline.getBindGroupLayout(0), entries: [\n    { binding: 0, resource: { buffer: uBuf } },\n    { binding: 1, resource: { buffer: aBuf } },\n    { binding: 2, resource: { buffer: bBuf } },\n    { binding: 3, resource: { buffer: cBuf } },\n  ]})\n\n  const encoder = device.createCommandEncoder()\n  const pass = encoder.beginComputePass()\n  pass.setPipeline(pipeline)\n  pass.setBindGroup(0, bind)\n  const [dx, dy, dz] = ker.dispatch({ M, N })\n  pass.dispatchWorkgroups(dx, dy, dz)\n  pass.end()\n\n  notify('5/9 Submitting GPU work...')\n  await new Promise(requestAnimationFrame)\n\n  const gpuStart = performance.now()\n  device.queue.submit([encoder.finish()])\n\n  notify('6/9 GPU computing...')\n  await device.queue.onSubmittedWorkDone()\n  const gpuEnd = performance.now()\n  const gpuMs = gpuEnd - gpuStart\n\n  notify('7/9 Reading back results...')\n  await new Promise(requestAnimationFrame)\n\n  const readBuf = createBuffer(device, sizeBytesC, GPUBufferUsage.COPY_DST | GPUBufferUsage.MAP_READ)\n  const enc2 = device.createCommandEncoder()\n  enc2.copyBufferToBuffer(cBuf, 0, readBuf, 0, sizeBytesC)\n  device.queue.submit([enc2.finish()])\n  await readBuf.mapAsync(GPUMapMode.READ)\n  const C = new Float32Array(readBuf.getMappedRange().slice(0))\n  readBuf.unmap()\n\n  let check = null\n  let cpuTime = null\n  if (verify && M * K &lt;= 1024 * 1024 && K * N &lt;= 1024 * 1024) {\n    notify('8/9 CPU verification...')\n    await new Promise(requestAnimationFrame)\n\n    const cpuStart = performance.now()\n    const ref = cpuMatmul(A, B, M, K, N)\n    const cpuEnd = performance.now()\n    cpuTime = cpuEnd - cpuStart\n\n    let maxAbs = 0\n    for (let i = 0; i &lt; elemC; i++) maxAbs = Math.max(maxAbs, Math.abs(ref[i] - C[i]))\n    check = { maxAbs, cpuTime }\n  } else if (verify) {\n    notify('8/9 Skipping CPU verification (too large)...')\n    await new Promise(requestAnimationFrame)\n  } else {\n    notify('8/9 CPU verification disabled')\n    await new Promise(requestAnimationFrame)\n  }\n\n  const totalEnd = performance.now()\n  const totalMs = totalEnd - totalStart\n\n  notify('9/9 Complete!', false)\n\n  const flops = 2 * M * K * N\n  const gflops = flops / (gpuMs / 1000) / 1e9\n  const cpuGflops = cpuTime ? flops / (cpuTime / 1000) / 1e9 : null\n\n  return {\n    ms: gpuMs,\n    totalMs,\n    gflops,\n    M,\n    K,\n    N,\n    kernel: ker.label,\n    kernelKey: ker.key,\n    dx,\n    dy,\n    dz,\n    check,\n    cpuTime,\n    cpuGflops\n  }\n}\n\n\n\n\n\n\n\n// Wrap runMatmul to surface exceptions as error objects so UI can display them\nasync function runMatmulSafe(dims, kernel, verify) {\n  try {\n    return await runMatmul(dims, kernel, verify)\n  } catch (e) {\n    return { error: (e && e.message) ? e.message : String(e) }\n  }\n}\n\n\n\n\n\n\n\nmutable compute_status = ({\n  state: 'idle',\n  message: 'Idle — ready'\n})\n\n\n\n\n\n\n\nmutable status_trigger = 0\n\n\n\n\n\n\n\ncompute_status_value = {\n  status_trigger; // Force dependency on trigger\n  return mutable compute_status;\n}\n\n\n\n\n\n\n\n// WebGPU Matrix Multiplication Runner - only executes on button click\nresults = {\n  if (!run) {\n    mutable compute_status = { state: 'idle', message: 'Idle — ready' }\n    mutable status_trigger = mutable status_trigger + 1\n    return null\n  }\n\n  // Capture current values at button click time\n  const currentDims = {\n    M: Math.max(1, Math.floor(Number(mutable m_value) || 0)),\n    K: Math.max(1, Math.floor(Number(mutable k_value) || 0)),\n    N: Math.max(1, Math.floor(Number(mutable n_value) || 0))\n  }\n  const currentKernelValue = mutable kernel_value\n  const currentKernel = normalizeKernelKey(currentKernelValue)\n  const currentVerify = !!mutable verify_value\n\n  const kernelInfo = kernels.find(k =&gt; k.key === currentKernel)\n  const kernelLabel = kernelInfo?.label ?? String(currentKernel)\n  \n  if (!webgpu_ok) {\n    const error = 'WebGPU not available'\n    mutable compute_status = { state: 'error', message: error, error, kernelLabel, dims: currentDims }\n    mutable status_trigger = mutable status_trigger + 1\n    return { error }\n  }\n  \n  if (!device) {\n    const error = 'No WebGPU device'\n    mutable compute_status = { state: 'error', message: error, error, kernelLabel, dims: currentDims }\n    mutable status_trigger = mutable status_trigger + 1\n    return { error }\n  }\n\n  mutable compute_status = {\n    state: 'running',\n    message: `Running ${kernelLabel} — ${currentDims.M}×${currentDims.K}×${currentDims.N}`,\n    spinner: true,\n    kernelLabel,\n    dims: currentDims\n  }\n  mutable status_trigger = mutable status_trigger + 1\n\n  // Yield to the browser so the status panel can refresh before the GPU work finishes.\n  await new Promise(requestAnimationFrame)\n  \n  try {\n    const { M, K, N } = currentDims\n    const totalMB = (M * K + K * N + M * N) * 4 / 1024 / 1024\n    \n    // Status callback to update phase / spinner state\n    const updateStatus = (statusUpdate) =&gt; {\n      const { message, spinning = true } = (typeof statusUpdate === 'string')\n        ? { message: statusUpdate, spinning: true }\n        : statusUpdate || {}\n      mutable compute_status = {\n        state: 'running',\n        message: `${kernelLabel} — ${message}`,\n        spinner: !!spinning,\n        kernelLabel,\n        dims: currentDims\n      }\n      mutable status_trigger = mutable status_trigger + 1\n    }\n    \n  // Call the computation with captured values and status callback\n    const result = await runMatmul(currentDims, currentKernel, currentVerify, updateStatus)\n    \n    if (!result) {\n      const error = 'runMatmul returned null/undefined'\n      mutable compute_status = { state: 'error', message: error, error, kernelLabel, dims: currentDims }\n      mutable status_trigger = mutable status_trigger + 1\n      return { error }\n    }\n    \n    if (result.error) {\n      const error = result.error\n      mutable compute_status = { state: 'error', message: error, error, kernelLabel, dims: currentDims }\n      mutable status_trigger = mutable status_trigger + 1\n      return { error }\n    }\n    \n    // Add metadata for display and append to history\n    const enriched = { \n      ...result, \n      totalMB, \n      runNumber: run \n    }\n\n  // Append to history via cache to avoid self-dependency loops\n  const updatedHistory = results_history_cache.push(enriched)\n  mutable results_history = updatedHistory\n\n    mutable compute_status = {\n      state: 'done',\n      message: `Completed ${kernelLabel} in ${enriched.ms.toFixed(2)} ms GPU (${enriched.totalMs.toFixed(2)} ms total)`,\n      kernelLabel,\n      dims: currentDims,\n      gpuTimeMs: enriched.ms,\n      totalTimeMs: enriched.totalMs,\n      gpuGflops: enriched.gflops,\n      spinner: false\n    }\n    mutable status_trigger = mutable status_trigger + 1\n\n    return enriched\n  } catch (e) {\n    const error = e.message || e.toString()\n    mutable compute_status = { state: 'error', message: error, error, kernelLabel, dims: currentDims }\n    mutable status_trigger = mutable status_trigger + 1\n    return { error }\n  }\n}\n\n\n\n\n\n\n\nstatus_panel = {\n  const status = compute_status_value || { state: 'idle', message: 'Idle — ready' }\n  const container = html`&lt;div class=\"callout status-panel\"&gt;&lt;/div&gt;`\n  container.classList.add(status.state === 'error' ? 'callout-warning' : 'callout-note')\n\n  if (status.state === 'running' && status.spinner) {\n    if (!document.getElementById('status-spinner-style')) {\n      const style = document.createElement('style')\n      style.id = 'status-spinner-style'\n      style.textContent = `\n        .status-spinner {\n          width: 1.25rem;\n          height: 1.25rem;\n          border: 2px solid rgba(0,0,0,0.1);\n          border-top-color: rgba(0,0,0,0.6);\n          border-radius: 9999px;\n          animation: status-spin 0.8s linear infinite;\n          margin-bottom: 0.5rem;\n        }\n        @keyframes status-spin {\n          to { transform: rotate(360deg); }\n        }\n      `\n      document.head.append(style)\n    }\n    const spinner = document.createElement('div')\n    spinner.className = 'status-spinner'\n    container.append(spinner)\n  }\n\n  const heading = document.createElement('div')\n  heading.style.fontWeight = '600'\n  heading.textContent = status.message ?? 'Idle — ready'\n  container.append(heading)\n\n  if (status.state === 'done' && Number.isFinite(status.gpuGflops)) {\n    const detail = document.createElement('div')\n    detail.textContent = `${status.kernelLabel ?? ''} — ${status.gpuGflops.toFixed(2)} GFLOP/s`\n    container.append(detail)\n  } else if (status.state === 'error' && status.error) {\n    const detail = document.createElement('div')\n    detail.textContent = status.error\n    container.append(detail)\n  } else if (status.dims) {\n    const dims = status.dims\n    const detail = document.createElement('div')\n    detail.textContent = `Dims: ${dims.M}×${dims.K}×${dims.N}`\n    container.append(detail)\n  }\n\n  return container\n}\n\n\n\n\n\n\n\nhtml`${status_panel}`\n\n\n\n\n\n\n\n// Display results - shows all results in history\nresults_table = {\n  clear_handler; // React to clear button\n  \n  const history = results_history || []\n  // console.log('[results_table] history length:', history.length, history)\n  \n  if (history.length === 0) {\n    return html`&lt;em&gt;Click Run matmul to execute the selected kernel. Results will be added to the table below.&lt;/em&gt;`\n  }\n  \n  // Convert all results to table rows\n  const tableRows = history.map(result =&gt; {\n    const tableRow = {\n      runNumber: result.runNumber,\n      kernel: result.kernel,\n      shape: { M: result.M, K: result.K, N: result.N },\n      memoryMB: result.totalMB,\n      dispatch: [result.dx, result.dy, result.dz],\n      gpuTimeMs: result.ms,\n      totalTimeMs: result.totalMs,\n      gpuGflops: result.gflops,\n      cpuTimeMs: result.cpuTime,\n      cpuGflops: result.cpuGflops,\n      speedup: (Number.isFinite(result.cpuTime) && Number.isFinite(result.ms) && result.ms &gt; 0)\n        ? result.cpuTime / result.ms\n        : null,\n      maxAbs: result.check?.maxAbs ?? null\n    }\n\n    const dash = '—'\n    return {\n      'Run': tableRow.runNumber,\n      'Kernel': tableRow.kernel,\n      'Dispatch': `[${tableRow.dispatch[0]}, ${tableRow.dispatch[1]}, ${tableRow.dispatch[2]}]`,\n      'GPU Throughput (GFLOP/s)': Number.isFinite(tableRow.gpuGflops) ? tableRow.gpuGflops : dash,\n      'Shape': `(${tableRow.shape.M}×${tableRow.shape.K}) · (${tableRow.shape.K}×${tableRow.shape.N}) → (${tableRow.shape.M}×${tableRow.shape.N})`,\n      'Memory (MB)': Number.isFinite(tableRow.memoryMB) ? tableRow.memoryMB : dash,\n      'GPU Time (ms)': Number.isFinite(tableRow.gpuTimeMs) ? tableRow.gpuTimeMs : dash,\n      'Total Time (ms)': Number.isFinite(tableRow.totalTimeMs) ? tableRow.totalTimeMs : dash,\n      'CPU Time (ms)': Number.isFinite(tableRow.cpuTimeMs) ? tableRow.cpuTimeMs : dash,\n      'CPU Throughput (GFLOP/s)': Number.isFinite(tableRow.cpuGflops) ? tableRow.cpuGflops : dash,\n      'GPU Speedup': Number.isFinite(tableRow.speedup) ? tableRow.speedup : dash,\n      'Max |Δ| vs CPU': Number.isFinite(tableRow.maxAbs) ? tableRow.maxAbs.toExponential(2) : dash\n    }\n  })\n\n  return Inputs.table(tableRows, {\n    sort: \"Run\",\n    reverse: true, // Show newest results first\n    // compare: {\n    //   \"Run\": compareNumeric,\n    //   \"Memory\": compareNumeric,\n    //   \"GPU Time\": compareNumeric,\n    //   \"Total Time\": compareNumeric,\n    //   \"GPU Throughput\": compareNumeric,\n    //   \"CPU Time\": compareNumeric,\n    //   \"CPU Throughput\": compareNumeric,\n    //   \"GPU Speedup\": compareNumeric,\n    //   \"Max |Δ| vs CPU\": compareNumeric\n    // }\n  })\n}\n\n\n\n\n\n\n\nhtml`${results_table}`\n\n\n\n\n\n\n\nresults_plot_data = {\n  clear_handler;\n  const history = results_history || []\n  return history.map((entry, index) =&gt; ({\n    runNumber: entry.runNumber ?? (index + 1),\n    kernel: entry.kernel,\n    kernelKey: entry.kernelKey ?? entry.kernel,\n    totalMs: entry.totalMs ?? entry.ms ?? null,\n    gpuMs: entry.ms ?? null,\n    gpuGflops: entry.gflops ?? null,\n    cpuGflops: entry.cpuGflops ?? null,\n    memoryMB: entry.totalMB ?? null,\n    dims: entry.dims ?? { M: entry.M, K: entry.K, N: entry.N }\n  }))\n}\n\n\n\n\n\n\n\nplot_kernel_colors = {\n  const palette = [\n    \"#1f77b4\",\n    \"#ff7f0e\",\n    \"#2ca02c\",\n    \"#d62728\",\n    \"#9467bd\",\n    \"#8c564b\",\n    \"#e377c2\",\n    \"#7f7f7f\",\n    \"#bcbd22\",\n    \"#17becf\"\n  ]\n  const domain = kernels.map(k =&gt; k.label)\n  const range = domain.map((_, i) =&gt; palette[i % palette.length])\n  return { domain, range }\n}\n\n\n\n\n\n\n\nresults_plots = {\n  clear_handler;\n  const data = results_plot_data ?? []\n  if (!data.length) {\n    return html`&lt;em&gt;Run matmul to populate timing and throughput charts.&lt;/em&gt;`\n  }\n\n  const { domain, range } = plot_kernel_colors\n\n  const throughputData = data.filter(d =&gt; Number.isFinite(d.gpuGflops))\n  const totalTimeData = data.filter(d =&gt; Number.isFinite(d.totalMs))\n\n  if (!throughputData.length && !totalTimeData.length) {\n    return html`&lt;em&gt;No results with throughput or total time available yet.&lt;/em&gt;`\n  }\n\n  // Calculate dynamic ticks based on data range\n  const memoryData = data.filter(d =&gt; Number.isFinite(d.memoryMB))\n  let dynamicTicks = [8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]\n  \n  if (memoryData.length &gt; 0) {\n    const minMemory = Math.min(...memoryData.map(d =&gt; d.memoryMB))\n    const maxMemory = Math.max(...memoryData.map(d =&gt; d.memoryMB))\n    \n    // If there's only one unique value or very close range, show fewer ticks\n    if (memoryData.length === 1 || (maxMemory / minMemory) &lt; 2) {\n      // Find the closest tick to the actual value\n      const targetMemory = memoryData[0].memoryMB\n      const closestTick = dynamicTicks.reduce((prev, curr) =&gt; \n        Math.abs(curr - targetMemory) &lt; Math.abs(prev - targetMemory) ? curr : prev\n      )\n      dynamicTicks = [closestTick]\n    } else {\n      // Filter ticks to only show relevant range\n      dynamicTicks = dynamicTicks.filter(tick =&gt; tick &gt;= minMemory * 0.5 && tick &lt;= maxMemory * 2)\n    }\n  }\n\n  // Prepare Plotly data\n  const traces = []\n  \n  // Create color mapping for kernels\n  const colorMap = {}\n  domain.forEach((kernel, i) =&gt; {\n    colorMap[kernel] = range[i]\n  })\n\n  // Group data by kernel for throughput traces\n  const kernelGroups = {}\n  throughputData.forEach(d =&gt; {\n    if (!kernelGroups[d.kernel]) {\n      kernelGroups[d.kernel] = { throughput: [], time: [] }\n    }\n    kernelGroups[d.kernel].throughput.push(d)\n  })\n\n  // Group time data by kernel\n  totalTimeData.forEach(d =&gt; {\n    if (!kernelGroups[d.kernel]) {\n      kernelGroups[d.kernel] = { throughput: [], time: [] }\n    }\n    kernelGroups[d.kernel].time.push(d)\n  })\n\n  // Create throughput traces (solid lines)\n  Object.entries(kernelGroups).forEach(([kernel, kernelData]) =&gt; {\n    if (kernelData.throughput.length &gt; 0) {\n      const sortedData = kernelData.throughput.sort((a, b) =&gt; a.memoryMB - b.memoryMB)\n      traces.push({\n        x: sortedData.map(d =&gt; d.memoryMB),\n        y: sortedData.map(d =&gt; d.gpuGflops),\n        mode: 'lines+markers',\n        type: 'scatter',\n        name: kernel,\n        line: { color: colorMap[kernel], width: 2 },\n        marker: { color: colorMap[kernel], size: 6 },\n        hovertemplate: sortedData.map(d =&gt; {\n          const dims = d.dims || { M: '?', K: '?', N: '?' }\n          return `&lt;b&gt;${d.kernel}&lt;/b&gt;&lt;br&gt;` +\n                 `Run: #${d.runNumber}&lt;br&gt;` +\n                 `Dims: ${dims.M}×${dims.K}×${dims.N}&lt;br&gt;` +\n                 `Memory: ${d.memoryMB.toFixed(1)} MB&lt;br&gt;` +\n                 `GPU Throughput: ${d.gpuGflops.toFixed(2)} GFLOP/s&lt;extra&gt;&lt;/extra&gt;`\n        }),\n        yaxis: 'y1'\n      })\n    }\n  })\n\n  // Calculate scaling for secondary y-axis\n  const baseThroughputMax = throughputData.reduce((acc, d) =&gt; Math.max(acc, d.gpuGflops), 0)\n  const baseTimeMax = totalTimeData.reduce((acc, d) =&gt; Math.max(acc, d.totalMs), 0)\n  const yMax = Math.max(baseThroughputMax, 1)\n  const timeScaleFactor = baseTimeMax &gt; 0 ? yMax / baseTimeMax : 1\n\n  // Create time traces (dashed lines) on secondary y-axis\n  Object.entries(kernelGroups).forEach(([kernel, kernelData]) =&gt; {\n    if (kernelData.time.length &gt; 0) {\n      const sortedData = kernelData.time.sort((a, b) =&gt; a.memoryMB - b.memoryMB)\n      traces.push({\n        x: sortedData.map(d =&gt; d.memoryMB),\n        y: sortedData.map(d =&gt; d.totalMs),\n        mode: 'lines+markers',\n        type: 'scatter',\n        name: `${kernel} (time)`,\n        line: { color: colorMap[kernel], width: 2, dash: 'dash' },\n        marker: { \n          color: 'white', \n          size: 8, \n          line: { color: colorMap[kernel], width: 2 },\n          symbol: 'cross'\n        },\n        hovertemplate: sortedData.map(d =&gt; {\n          const dims = d.dims || { M: '?', K: '?', N: '?' }\n          return `&lt;b&gt;${d.kernel}&lt;/b&gt;&lt;br&gt;` +\n                 `Run: #${d.runNumber}&lt;br&gt;` +\n                 `Dims: ${dims.M}×${dims.K}×${dims.N}&lt;br&gt;` +\n                 `Memory: ${d.memoryMB.toFixed(1)} MB&lt;br&gt;` +\n                 `Total Time: ${d.totalMs.toFixed(2)} ms&lt;extra&gt;&lt;/extra&gt;`\n        }),\n        yaxis: 'y2',\n        showlegend: false\n      })\n    }\n  })\n\n  // Add dummy traces for series style legend\n  traces.push({\n    x: [null],\n    y: [null],\n    mode: 'lines+markers',\n    type: 'scatter',\n    name: '— Throughput (GFLOP/s)',\n    line: { color: '#666', width: 2 },\n    marker: { color: '#666', size: 6 },\n    showlegend: true,\n    yaxis: 'y1'\n  })\n  \n  traces.push({\n    x: [null],\n    y: [null],\n    mode: 'lines+markers',\n    type: 'scatter',\n    name: '- - Time (ms)',\n    line: { color: '#666', width: 2, dash: 'dash' },\n    marker: { \n      color: 'white', \n      size: 8, \n      line: { color: '#666', width: 2 },\n      symbol: 'cross'\n    },\n    showlegend: true,\n    yaxis: 'y2'\n  })\n\n  // Create the plot container\n  const container = html`&lt;div style=\"width: 100%; height: 450px;\"&gt;&lt;/div&gt;`\n  \n  // Plotly layout\n  const layout = {\n    title: {\n      text: 'WebGPU Matrix Multiplication Performance',\n      // x: 0.5,\n      // y: 1.5,\n      // xanchor: 'center',\n      font: { size: 16 }\n    },\n    xaxis: {\n      type: 'log',\n      title: 'Memory (MB)',\n      tickvals: dynamicTicks,\n      ticktext: dynamicTicks.map(d =&gt; d &gt;= 1 ? `${d.toFixed(0)}` : `${d.toFixed(1)}`)\n    },\n    yaxis: {\n      title: 'GPU Throughput (GFLOP/s)',\n      side: 'left'\n    },\n    yaxis2: {\n      title: 'Total Time (ms)',\n      side: 'right',\n      overlaying: 'y'\n    },\n    legend: {\n      x: 1.3,\n      y: 0.8,\n      xanchor: 'left',\n      yanchor: 'top',\n      bgcolor: 'rgba(255,255,255,0.8)',\n      bordercolor: '#ddd',\n      borderwidth: 1,\n      font: { size: 11 }\n    },\n    margin: { l: 80, r: 150, t: 80, b: 80 },\n    hovermode: 'closest'\n  }\n\n  // Create the plot\n  Plotly.newPlot(container, traces, layout, {\n    responsive: true,\n    displayModeBar: false,\n    modeBarButtonsToRemove: ['pan2d', 'lasso2d', 'select2d']\n  })\n\n  return container\n}\n\n\n\n\n\n\n\nhtml`${results_plots}`",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Exploring WebGPU Matmul Kernels (OJS)</span>"
    ]
  },
  {
    "objectID": "Courses/11_1_WebGPU-matmul.html#notes-and-tips",
    "href": "Courses/11_1_WebGPU-matmul.html#notes-and-tips",
    "title": "13  Exploring WebGPU Matmul Kernels (OJS)",
    "section": "13.2 Notes and tips",
    "text": "13.2 Notes and tips\n\nThis page uses Observable JS cells; code runs reactively in the browser.\nPerformance varies a lot across browsers/GPUs. Numbers shown include submission/synchronization overhead; GPU timestamp queries can refine timing when supported.\nThe kernels here don’t use workgroup-shared memory or subgroups; those can yield further gains on some hardware.\nCredit: Kernel ideas and the optimization path are inspired by Zach Nussbaum’s article referenced above and by Simon Böhm’s CUDA matmul write-up.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Exploring WebGPU Matmul Kernels (OJS)</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "14  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Fowler, M. 2022. Python Concurrency with Asyncio. Manning. https://www.manning.com/books/python-concurrency-with-asyncio.\n\n\nInoue, Hiroshi. 2016. “How SIMD Width Affects Energy Efficiency: A\nCase Study on Sorting.” In 2016 IEEE Symposium in Low-Power\nand High-Speed Chips (COOL CHIPS XIX), 1–3. IEEE.\n\n\nRobey, R., and Y. Zamora. 2021. Parallel and High Performance\nComputing. Manning. https://www.manning.com/books/parallel-and-high-performance-computing.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.\n“Attention Is All You Need.” Advances in Neural\nInformation Processing Systems 30.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "Projects/list.html",
    "href": "Projects/list.html",
    "title": "Appendix A — List of projects",
    "section": "",
    "text": "A.1 General Indications\nThis is a list of project ideas, some are of different difficulty levels, and some are cross-over projects that combine multiple subjects of the course.\nThose projects should be done alone or in pairs. The deliverables are also indicative, you can propose something else if you think it is more appropriate.\nAs one say “It’s not about the destination, it’s about the journey”, so the goal is to learn, and to showcase what you have learned.\nYou need to demonstrate intellectual curiosity, to dig into the subject, search for related work, and to understand what is already done in the field of the project you choose. Sometimes there is already prior work, so you may need to build on top of it if it is relevant.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>List of projects</span>"
    ]
  },
  {
    "objectID": "Projects/list.html#general-indications",
    "href": "Projects/list.html#general-indications",
    "title": "Appendix A — List of projects",
    "section": "",
    "text": "Important\n\n\n\nDon’t hesitate to propose your own project idea if you have something else in mind that is not listed here!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>List of projects</span>"
    ]
  },
  {
    "objectID": "Projects/list.html#terminology",
    "href": "Projects/list.html#terminology",
    "title": "Appendix A — List of projects",
    "section": "A.2 Terminology",
    "text": "A.2 Terminology\n\nA.2.1 ETL\nExtract, transform, load\n\n\n\n\n\n\n\nA.2.2 HPO\nHyperparameter optimization\n\n\n\n\n\n\n\nA.2.3 Deep Research\n\nDeep Research is composed of an LLM (which can be selected from the current list of LLMs provided by OpenAI, 4o, o1, o3, etc) and an internal “agentic framework” which guide the LLM to use tools like web search and organize its actions in steps.\n\nHere, we may broaden the “Deep Research” domain of application to any type of search like codebase extraction (instead of web search).\nSee Building Deep Research from scratch\n\n\n\n\n\n\n\nA.2.4 Work-Stealing\nWork-stealing is a scheduling strategy for parallel computing where idle processors dynamically “steal” tasks from busy processors to balance the workload and improve efficiency.\n\n\n\n\n\n\n\nA.2.5 OLAP (Online Analytical Processing)\nOnline Analytical Processing is a category of software technology that enables analysts, managers, and executives to gain insight into data through fast, consistent, interactive access in a variety of ways.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>List of projects</span>"
    ]
  },
  {
    "objectID": "Projects/list.html#code-assistants",
    "href": "Projects/list.html#code-assistants",
    "title": "Appendix A — List of projects",
    "section": "B.1 Code Assistants",
    "text": "B.1 Code Assistants\n\n(1) Deep research assistant for creating a README for a codebase (pipeline version)\n\n\n\n\nDescription\n\nBuild an assistant that analyzes a codebase, infers project structure, and generates a high‑quality README with a summary, usage and architecture details.\n\nDeliverables\n\nCLI/Notebook that ingests a repo and outputs a README.md file.\n\n\n\n\n\n\n\n\n\n\n(2) Deep research assistant for creating a README for a codebase (agentic/mcp version in continue.dev)\n\n\n\n\nDescription\n\nBuild an agentic/mcp assistant that analyzes a codebase, infers project structure, and generates a high‑quality README with a summary, usage and architecture details.\n\nDeliverables\n\na set of LLM setup with mcp servers and promopts in vscode continue extension that can generate a README.md file for a codebase.\n\n\n\n\n\n\n\n\n\n\n(3) Generating docstrings for Python code in a codebase\n\n\n\n\nDescription\n\nAuto-generate docstrings for functions and classes, then implement automated checks and tests that validate docstring correctness against function behavior.\n\nDeliverables\n\ntools that annotates Python code with docstrings, and a test suite that checks docstring claims (examples/expected behavior)\n\n\n\n\n\n\n\n\n\n\n(4) Code summarization/explanation and modularization suggestions for long Python scripts\n\n\n\n\nDescription\n\nCreate a tool that parses very long Python scripts and outputs modularization suggestions (module boundaries, function extraction) and a summary with refactor hints.\n\nDeliverables\n\nTool that takes a long Python script and outputs a summary and modularization suggestions (mcp server)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples of long scripts\n\n\n\n\nhttps://github.com/yt-dlp/yt-dlp/blob/master/yt_dlp/YoutubeDL.py\nhttps://github.com/aws/aws-cli/blob/develop/awscli/clidriver.py\nhttps://github.com/sqlalchemy/sqlalchemy/blob/main/lib/sqlalchemy/orm/session.py\nhttps://github.com/pallets/jinja/blob/main/src/jinja2/environment.py\nhttps://github.com/ipython/ipython/blob/main/IPython/core/interactiveshell.py\n\n\n\n\n\n(5) Pipeline for generating D2lang diagrams with a vision LLM\n\n\n\n\nDescription\n\nPipeline that takes a prompt, translates/templates it for a RAG query over the D2lang documentation, and generates a D2 diagram, compiles it to SVG/PNG, analyzes it for correctness and completeness, and iterates to improve it.\n\nDeliverables\n\nGennerate tool that takes a prompt and outputs a D2lang diagram over multiple iterations.\n\n\n\n\n\n\n\n\nD2 Documentation\n\n\n\n\n\n\nTip\n\n\n\nmistral is a vision LLM.\n\n\n\n\nB.1.1 Cross-over : Code Assistants + Numba or Dask\n\n(6) Refactoring bottleneck functions in a codebase to use Numba both with benchmarks, profiling, and tests.\n\n\n\n\nDescription\n\nIdentify hotspots, refactor to Numba, and quantify speedup/accuracy tradeoffs with tests and profiles.\n\nDeliverables\n\nOriginal vs optimized implementations, benchmark script, profiling report, tests verifying numerical parity. Clear reporting of reproducible use of LLMS/Deep Research for codebase understanding and refactoring and reproducibility.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples of codebases\n\n\n\n\nscikit-learn/scikit-learn — sklearn/manifold/_locally_linear.py — _locally_linear_embedding()\npandas-dev/pandas — pandas/core/window/common.py — flex_binary_moment()\nultralytics/yolov5 — train.py — train()\nPaddlePaddle/PaddleOCR\nultralytics/yolov5 — utils/plots.py, utils/segment/plots.py\nmdobook/resources - exercises/tenbartruss/truss.py\n\n\n\n\n\n\n\n\n\nExamples of numba uses\n\n\n\npandas/core/_numba/kernels/* — sliding_mean/var/sum/min_max\n\n\n\n\n(7) Translating PySpark pipelines to Dask — practical migration guide + AI Code Assistant.\n\n\n\n\nDescription\n\nPort small-to-medium PySpark ETL pipelines to Dask, preserve semantics, benchmark PySpark vs Dask, and provide an AI assistant that suggests rewrites, generates patches/tests, and explains tuning/semantic differences.\n\nDeliverables\n\nPySpark examples, Dask reimplementations, benchmark notebook, migration checklist, assistant (mapping rules + prompt templates + patch/test generator), and evaluation report.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>List of projects</span>"
    ]
  },
  {
    "objectID": "Projects/list.html#parallel-computing",
    "href": "Projects/list.html#parallel-computing",
    "title": "Appendix A — List of projects",
    "section": "B.2 Parallel Computing",
    "text": "B.2 Parallel Computing\n\n(8) Locking Strategy Lab: coarse vs fine vs optimistic vs RW locks\n\n\n\n\nDescription\n\nImplement and compare multiple locking strategies on a shared structure (e.g., graph or ledger).\n\nDeliverables\n\nImplementations, microbench harness, plots comparing throughput vs contention, write-up with recommendations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStrategies to consider\n\n\n\nCoarse grain: unique global Lock protecting whole structure Fine grain: sharded locks or per-node locks Reader‑Writer lock: shared readers, exclusive writers Optimistic read (versioning / retry): read w/o lock, validate version/cas (Optionnel) Lock‑free / CAS implementations in lower level language\n\n\n\n\n\n\n\n\nLinks\n\n\n\n\nGeneral background on locks and trade‑offs.\nTheory and common variants.\nClassical OCC discussion.\nPython threading locks.\nPython atomicts\n\n\n\n\n\n(9) Multi‑stage Image Pipeline with Backpressure & Flow Control\n\n\n\n\nDescription\n\nImplement a 3+ stage image processing pipeline with bounded queues, backpressure propagation, and dynamic scaling heuristics.\n\nDeliverables\n\nPipeline code, scaling heuristics, benchmark, short UI or logging dashboard.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTypical pipelines\n\n\n\n\nGeneric supervised CV pipeline (classification/recognition)\nOCR pipeline (detection + recognition)\n\n\n\n\n\n(10) Adaptive Concurrent Downloader with Politeness & Rate‑Limits (e.g., Google Scholar scraping or LLM API application)\n\n\n\n\nDescription\n\nImplement polite, adaptive downloader that enforces per-domain politeness, adapts concurrency to observed latency and failures, supports cancellation/priorities. Application to recursive scraping in google scholar or LLM API calls with rate limits.\n\nDeliverables\n\nCLI downloader, simulator of remote servers, policies and tests with real-world scraping or API calls example.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExemple of polite scraper\n\n\n\nhttps://github.com/dmi3kno/polite\n\n\n\n\n(11) Filesystem Work‑Stealing Crawler (toy distributed scheduler)\n\n\n\n\nDescription\n\nParallel filesystem walker and worker pool using work-stealing deques to deal with skewed directories.\n\nDeliverables\n\nCrawler implementation, skewed tree generator, benchmark and analysis.\n\n\n\n\n\n\n\n\n\n\nB.2.1 Dask/Ray based\n\n(12) Ray Tune: distributed HPO with comparison of strategies\n\n\n\n\nDescription\n\nImplement reproducible HPO experiments comparing grid/random/ASHA on a model and present best practices for local vs distributed runs.\n\nDeliverables\n\nExperiment suite, plots, reproducible configs with\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nRay Tune\n\n\n\n\n(13) ETL dask/ray pipeline + Front‑end (Streamlit/Panel/React)\n\n\n\n\nDescription\n\nBuild an ETL that scales locally with Dask or Ray and expose controls/visualization through a simple front-end.\n\nDeliverables\n\nETL code + front-end, README, sample dataset and demo video.\n\n\n\n\n\n\n\n\n\n\n(14) Synthetic Dask DAGs generator for evaluating dask task fusion and scheduling strategies.\n\n\n\n\nDescription\n\nCreate a synthetic DAG generator that can vary task sizes, dependencies, and fusion parameters to evaluate scheduler behavior.\n\nDeliverables\n\nGenerator, benchmark harness, plots and scheduler tuning recommendations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nOfficial Dask benchmarks\nDask Graphs\nDask Transformations\n\n\n\n\n\n(15) Streaming source + sliding window aggregation + dashboard (air quality / financial / river monitoring)\n\n\n\n\nDescription\n\nBuild a streaming pipeline using a local source (or Kafka emulator) performing sliding-window aggregations with a Streamlit/Dash dashboard. Apply to air quality, financial data, river monitoring, etc.\n\nDeliverables\n\nStreaming app, dashboard, reproducible synthetic generator, evaluation of latency/accuracy.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>List of projects</span>"
    ]
  },
  {
    "objectID": "Projects/list.html#webgpu",
    "href": "Projects/list.html#webgpu",
    "title": "Appendix A — List of projects",
    "section": "B.3 WebGPU",
    "text": "B.3 WebGPU\n\n(16) Proof of concept for using WebGPU for Olap queries in the browser\n\n\n\n\nDescription\n\nDemonstrate how WebGPU can accelerate in‑browser OLAP operations (filters/group-bys/aggregates) on columnar data.\n\nDeliverables\n\nIn-browser demo, short perf report.\n\n\n\n\n\n\n\n\n\n\n(17) WebGPU app for counting people in a video stream (browser)\n\n\n\n\nDescription\n\nImplement a browser-based (WebGPU-accelerated) pipeline that performs simple frame-level processing to count people (using a small model or heuristic).\n\nDeliverables\n\nWeb demo\n\n\n\n\n\n\n\n\n\n\n(18) WebGPU path-tracing\n\n\n\n\nDescription\n\nhttps://github.com/ferminLR/webgpu-path-tracing \\Rightarrow Implement one or more of the TODO items in the referenced path-tracing repo: advanced shader/task mapping and performance tuning etc.\n\nDeliverables\n\nPR to the repo with one or more completed TODO items.\n\n\n\n\n\n\n\n\n\n\nB.3.1 Crossover : Webgpu + LLMs\n\n(19) WebGPU/WebLLM RAG app for document/website Q&A in browser\n\n\n\n\nDescription\n\nImplement Retrieval-Augmented-Generation entirely in the browser: use WebGPU for vector similarity (ANN) and a small local WebLLM. Application to document or website Q&A.\n\nDeliverables\n\nWeb demo with interface to upload a document or URL and ask questions.\n\n\n\n\n\n\n\n\n\n\n\nB.3.2 Crossover : WebGPU + Dask\n\n(20) ETL + WebGPU visualization dashboard with Dask and Streamlit\n\n\n\n\nDescription\n\nRun ETL at scale locally with Dask and visualize large aggregated results in-browser using WebGPU for rendering (or via Streamlit embedding).\n\nDeliverables\n\nDask ETL pipeline, frontend WebGPU visualizer, fallback renderer, performance report.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>List of projects</span>"
    ]
  },
  {
    "objectID": "applications.html",
    "href": "applications.html",
    "title": "Appendix B — Applications",
    "section": "",
    "text": "Original (In Percent Format)\nOnline Html (Corrected)\nNotebook (Corrected)\n\n\n\n\nPrompt engineering exercises\nSolution\nNotebook\n\n\nAI Agent\nSolution\nNotebook\n\n\nNumpy Workout\nSolution\nNotebook\n\n\nDecorators Tutorial\nSolution\nNotebook\n\n\nA tutorial on Python generators\nSolution\nNotebook\n\n\nMultiprocessing in Python 3\nSolution\nNotebook\n\n\nScaling App with multiprocessing\nSolution\nNotebook\n\n\nAn asyncio application\nSolution\nNotebook\n\n\nIPC and Locking\nSolution\nNotebook\n\n\nLocking with multiprocessing.Value\nSolution\nNotebook\n\n\nDistributed models examples\nSolution\nNotebook\n\n\nDask delayed App\nSolution\nNotebook\n\n\nNumba Introduction\nSolution\nNotebook\n\n\nNumba first steps\nSolution\nNotebook\n\n\n\nNo matching items",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Applications</span>"
    ]
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Appendix C — Slides in reveal.js",
    "section": "",
    "text": "Code Assistants\nNumpy Workout\nIntroduction to parallel computing\nAsynchronous Programming with Python\nIPC and locking\nDistributed Computing models\nDask and Ray\nDask delayed\nHardware Vectorization with SIMD\nGPU computing, CPU cache, Computing cluster\nYour First WebGPU App\nDesigning parallel algorithms\n\nNo matching items",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Slides in reveal.js</span>"
    ]
  }
]