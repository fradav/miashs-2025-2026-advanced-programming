{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI Agent\n",
        "\n",
        "Taking the bull by the horns\n",
        "\n",
        "François-David Collin (CNRS, IMAG, Paul-Valéry Montpellier 3\n",
        "University)  \n",
        "Wednesday, August 27, 2025\n",
        "\n",
        "For this practical work, you need the following python packages:\n",
        "\n",
        "-   `openai`\n",
        "-   `python-dotenv`\n",
        "-   `faiss-cpu`\n",
        "-   `numpy`\n",
        "\n",
        "and also a nicely format function for markdown output of LLM:"
      ],
      "id": "c104bd36-bb09-4c67-8584-858da6dc3f33"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "def format_markdown_output(response):\n",
        "    # Create a boxed effect using HTML and CSS\n",
        "    boxed_content = f\"\"\"\n",
        "<div style=\"border: 2px solid #f15006ff; padding: 10px; border-radius: 5px; background-color: #493f3fff;\">\n",
        "    {response}\n",
        "</div>\n",
        "\"\"\"\n",
        "    return display(Markdown(boxed_content))"
      ],
      "id": "704dc8d1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hello World\n",
        "\n",
        "Make work the example of the course.\n",
        "\n",
        "``` python\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "chat_response = client.chat.completions.create(\n",
        "    model= \"gpt-4o\",\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is the best French cheese?\",\n",
        "        },\n",
        "    ]\n",
        ")\n",
        "print(chat_response.choices[0].message.content)\n",
        "```\n",
        "\n",
        "Look at the documentation of the `OpenAI()` constructor in order to take\n",
        "your own model. Modify the model name accordingly.\n",
        "\n",
        "> **Important**\n",
        ">\n",
        "> **Never, ever** put your API key in the code. Use environment\n",
        "> variables instead. For example, use python `dotenv` to load the API\n",
        "> key from a `.env` file.\n",
        "\n",
        "> **Tip**\n",
        ">\n",
        "> The openai compatible endpoint for mistral.ai is\n",
        "> `https://api.mistral.ai/v1`. `mistral-small-latest` as the model\n",
        "> should be sufficient.\n",
        "\n",
        "> **Tip**\n",
        ">\n",
        "> If you have locally installed llm with ollama/lmstudio for example,\n",
        "> don’t hesitate to adapt the code to use your local model.\n",
        "\n",
        "# Build a RAG Agent\n",
        "\n",
        "## Split the document in chunks\n",
        "\n",
        "Get the\n",
        "[alice.txt](https://fradav.github.io/miashs-2025-2026-advanced-programming/docs-resources/Notebooks/materials/alice.txt)\n",
        "and split it in 2048 characters.\n",
        "\n",
        "## Encode the chunks\n",
        "\n",
        "A simple function to get an embedding is:"
      ],
      "id": "a1546913-e60b-447c-8bb5-9348b3a5042a"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from time import sleep\n",
        "\n",
        "def get_text_embedding(input):\n",
        "    sleep(0.5) # Rate-limiting\n",
        "    embeddings_response = client.embeddings.create(\n",
        "          model=\"mistral-embed\",\n",
        "          input=input\n",
        "      )\n",
        "    return embeddings_response.data[0].embedding"
      ],
      "id": "6f985750"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It does return a 1024 list of float (the embedding of the input).\n",
        "\n",
        "Make a `numpy` array of all chunk embeddings from the text. Save the\n",
        "array to a file to avoid recomputing it.\n",
        "\n",
        "(Should take one and half minute)\n",
        "\n",
        "> **Tip**\n",
        ">\n",
        "> An optimal version of this should use the [batch\n",
        "> feature](https://platform.openai.com/docs/guides/batch) of OpenAI APi\n",
        "\n",
        "## Store embeddings in vector database"
      ],
      "id": "4652d949-161a-4bc0-868a-bf57a94f379e"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import faiss\n",
        "\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)"
      ],
      "id": "e4ae1fe6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Query\n",
        "\n",
        "## Make an example query\n",
        "\n",
        "Make an embedding for a question like “À quels obstacles est confrontée\n",
        "Alice?”\n",
        "\n",
        "## Search for the most semantically similar chunks"
      ],
      "id": "43e0490f-afff-4168-a595-865bb6a72a13"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "D, I = index.search(question_embeddings, k=2) # distance, index"
      ],
      "id": "2eb0e5d4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieve the chunks\n",
        "\n",
        "## RAG prompt\n",
        "\n",
        "Make the RAG query with the following prompt"
      ],
      "id": "27ab740b-a93d-4a39-95fd-41bfb37eb4c6"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\n",
        "Context information is below.\n",
        "---------------------\n",
        "{retrieved_chunk}\n",
        "---------------------\n",
        "Given the context information and not prior knowledge, answer the query.\n",
        "Query: {question}\n",
        "Answer:\n",
        "\"\"\""
      ],
      "id": "894902b5"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "output-location": "column"
      },
      "outputs": [],
      "source": [
        "def run_mistral(user_message, model=\"mistral-small-latest\"):\n",
        "    sleep(1) # Rate-limit\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\", \"content\": user_message\n",
        "        }\n",
        "    ]\n",
        "    chat_response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "    return (chat_response.choices[0].message.content)\n",
        "\n",
        "format_markdown_output(run_mistral(prompt))"
      ],
      "id": "863eb447"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Put it together\n",
        "\n",
        "Make a function for any question about the book.\n",
        "\n",
        "## Areas for improvement\n",
        "\n",
        "-   Chunking strategies: chunk sizes, overlap, metadata\n",
        "-   LLM parameters: context size, temperature, top_p, etc.\n",
        "-   reranking\n",
        "-   hybrid retrieval\n",
        "\n",
        "# MCP server\n",
        "\n",
        "## How to test/install a python MCP server\n",
        "\n",
        "As simple mcp server requires to be just a complete one-liner script to\n",
        "run, simply create a new python file (e.g. `mcp_server.py`) and add the\n",
        "following code:\n",
        "\n",
        "``` python\n",
        "from mcp.server.fastmcp import FastMCP\n",
        "\n",
        "mcp = FastMCP(\"MyMCPServer\")\n",
        "\n",
        "@mcp.tool()\n",
        "def my_tool():\n",
        "    \"\"\"\n",
        "    This is my tool, it does nothing.\n",
        "\n",
        "    Args:\n",
        "        None\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with the status of the tool.\n",
        "    \"\"\"\n",
        "    return {\"message\": \"Hello from my_tool!\"}\n",
        "```\n",
        "\n",
        "``` bash\n",
        "uv run --with fastmcp mcp run path/to/mcp_server.py\n",
        "```\n",
        "\n",
        "in a yaml/json file for mcp server:\n",
        "\n",
        "## YAML\n",
        "\n",
        "``` yaml\n",
        "    command: uv\n",
        "    args:\n",
        "      - run\n",
        "      - --with\n",
        "      - fastmcp\n",
        "      - mcp\n",
        "      - run\n",
        "      - mcp_server.py\n",
        "```\n",
        "\n",
        "## JSON\n",
        "\n",
        "``` json\n",
        "{\n",
        "  \"command\": \"uv\",\n",
        "  \"args\": [\n",
        "    \"run\",\n",
        "    \"--with\",\n",
        "    \"fastmcp\",\n",
        "    \"mcp\",\n",
        "    \"run\",\n",
        "    \"mcp_server.py\"\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "## First draft\n",
        "\n",
        "We want to make an MCP server which logs useful stuff in standard\n",
        "logging format to a file specified by environment variable\n",
        "`MCP_LOGGING_FILE`.\n",
        "\n",
        "Use [FastMCP](https://gofastmcp.com/getting-started/welcome) as MCP SDK.\n",
        "\n",
        "We should use standard `logging` python package.\n",
        "\n",
        "## Refining\n",
        "\n",
        "We want to let the LLM choose the logging level\n",
        "\n",
        "## Playing with it\n",
        "\n",
        "Make a rule to use the tool at each LLM request, test it. Pay attention\n",
        "to the workflow, especially with devstral.\n",
        "\n",
        "<span class=\"proof-title\">*Solution*. </span>\n",
        "\n",
        "``` markdown\n",
        "# Logging each answer\n",
        "\n",
        "Each time your answer:\n",
        "- Choose a concise summary of your answer\n",
        "- Choose a level of importance `INFO`, `WARNING`, `ERROR`\n",
        "- log that\n",
        "- after logging, if successful, just say \"Logged.\"\n",
        "\n",
        "DO NOT FORGET TO LOG EACH TIME\n",
        "```"
      ],
      "id": "e3fc6e0e-a483-40ba-9e5c-87ad1d819a25"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/Users/fradav/Documents/Dev/Python/Cours-programmation-MIASHS-2025/.venv/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  }
}