{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI Agent\n",
        "\n",
        "Taking the bull by the horns\n",
        "\n",
        "François-David Collin (CNRS, IMAG, Paul-Valéry Montpellier 3\n",
        "University)  \n",
        "Wednesday, August 27, 2025\n",
        "\n",
        "For this practical work, you need the following python packages:\n",
        "\n",
        "-   `openai`\n",
        "-   `python-dotenv`\n",
        "-   `faiss-cpu`\n",
        "-   `numpy`\n",
        "\n",
        "# Hello World\n",
        "\n",
        "Make work the example of the course.\n",
        "\n",
        "``` python\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "chat_response = client.chat.completions.create(\n",
        "    model= \"gpt-4o\",\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is the best French cheese?\",\n",
        "        },\n",
        "    ]\n",
        ")\n",
        "print(chat_response.choices[0].message.content)\n",
        "```\n",
        "\n",
        "Look at the documentation of the `OpenAI()` constructor in order to take\n",
        "your own model. Modify the model name accordingly.\n",
        "\n",
        "> **Important**\n",
        ">\n",
        "> **Never, ever** put your API key in the code. Use environment\n",
        "> variables instead. For example, use python `dotenv` to load the API\n",
        "> key from a `.env` file.\n",
        "\n",
        "> **Tip**\n",
        ">\n",
        "> The openai compatible endpoint for mistral.ai is\n",
        "> `https://api.mistral.ai/v1`. `mistral-small-latest` as the model\n",
        "> should be sufficient.\n",
        "\n",
        "> **Tip**\n",
        ">\n",
        "> If you have locally installed llm with ollama/lmstudio for example,\n",
        "> don’t hesitate to adapt the code to use your local model.\n",
        "\n",
        "# Build a RAG Agent\n",
        "\n",
        "## Split the document in chunks\n",
        "\n",
        "Get the\n",
        "[alice.txt](https://fradav.github.io/miashs-2025-2026-advanced-programming/docs-resources/Notebooks/materials/alice.txt)\n",
        "and split it in 2048 characters.\n",
        "\n",
        "## Encode the chunks\n",
        "\n",
        "A simple function to get an embedding is:"
      ],
      "id": "b54391fb-865f-4231-8794-472557f327eb"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from time import sleep\n",
        "\n",
        "def get_text_embedding(input):\n",
        "    sleep(0.5) # Rate-limiting\n",
        "    embeddings_response = client.embeddings.create(\n",
        "          model=\"mistral-embed\",\n",
        "          input=input\n",
        "      )\n",
        "    return embeddings_response.data[0].embedding"
      ],
      "id": "bece7179"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It does return a 1024 list of float (the embedding of the input).\n",
        "\n",
        "Make a `numpy` array of all chunk embeddings from the text. Save the\n",
        "array to a file to avoid recomputing it.\n",
        "\n",
        "(Should take one and half minute)\n",
        "\n",
        "> **Tip**\n",
        ">\n",
        "> An optimal version of this should use the [batch\n",
        "> feature](https://platform.openai.com/docs/guides/batch) of OpenAI APi\n",
        "\n",
        "## Store embeddings in vector database"
      ],
      "id": "27f371d6-81a3-4731-878a-ceadb38ba7c7"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import faiss\n",
        "\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)"
      ],
      "id": "f405c59e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Query\n",
        "\n",
        "## Make an example query\n",
        "\n",
        "Make an embedding for a question like “À quels obstacles est confrontée\n",
        "Alice?”\n",
        "\n",
        "## Search for the most semantically similar chunks"
      ],
      "id": "5edfb73e-7102-49b7-94c4-9dd9b1845983"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "D, I = index.search(question_embeddings, k=2) # distance, index"
      ],
      "id": "be333706"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieve the chunks"
      ],
      "id": "b8854409-b653-4e97-a67a-df6fbaaa600a"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "#! tags: [solution]\n",
        "retrieved_chunk = [chunks[i] for i in I.tolist()[0]]"
      ],
      "id": "b3eb2423"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAG prompt\n",
        "\n",
        "Make the RAG query with the following prompt"
      ],
      "id": "42b821a5-79d6-40ee-95e2-acedc9b418c9"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\n",
        "Context information is below.\n",
        "---------------------\n",
        "{retrieved_chunk}\n",
        "---------------------\n",
        "Given the context information and not prior knowledge, answer the query.\n",
        "Query: {question}\n",
        "Answer:\n",
        "\"\"\""
      ],
      "id": "a75f0b8f"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_mistral(user_message, model=\"mistral-small-latest\"):\n",
        "    sleep(1) # Rate-limit\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\", \"content\": user_message\n",
        "        }\n",
        "    ]\n",
        "    chat_response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "    return (chat_response.choices[0].message.content)\n",
        "\n",
        "display(Markdown(run_mistral(prompt)))"
      ],
      "id": "40f0d9eb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Put it together\n",
        "\n",
        "Make a function for any question about the book.\n",
        "\n",
        "## Areas for improvement\n",
        "\n",
        "-   Chunking strategies: chunk sizes, overlap, metadata\n",
        "-   LLM parameters: context size, temperature, top_p, etc.\n",
        "-   reranking\n",
        "-   hybrid retrieval\n",
        "\n",
        "# MCP server\n",
        "\n",
        "## First draft\n",
        "\n",
        "We want to make an MCP server which logs useful stuff in standard\n",
        "logging format to a file specified by environment variable\n",
        "`MCP_LOGGING_FILE`.\n",
        "\n",
        "Use [FastMCP](https://gofastmcp.com/getting-started/welcome) as MCP SDK.\n",
        "\n",
        "We should use standard `logging` python package.\n",
        "\n",
        "## Refining\n",
        "\n",
        "We want to let the LLM choose the logging level\n",
        "\n",
        "## Playing with it\n",
        "\n",
        "Make a rule to use the tool at each LLM request, test it. Pay attention\n",
        "to the workflow, especially with devstral.\n",
        "\n",
        "``` markdown\n",
        "<!-- | tags: [solution] -->\n",
        "# Logging each answer\n",
        "\n",
        "Each time your answer:\n",
        "- Choose a concise summary of your answer\n",
        "- Choose a level of importance `INFO`, `WARNING`, `ERROR`\n",
        "- log that\n",
        "- after logging, if successful, just say \"Logged.\"\n",
        "\n",
        "DO NOT FORGET TO LOG EACH TIME\n",
        "```"
      ],
      "id": "9dcfc286-1a12-43f3-b3f7-c58997f1c50d"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/Users/fradav/Documents/Dev/Python/Cours-programmation-MIASHS-2025/.venv/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  }
}