{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI Agent\n",
        "\n",
        "Taking the bull by the horns\n",
        "\n",
        "François-David Collin (CNRS, IMAG, Paul-Valéry Montpellier 3\n",
        "University)  \n",
        "Wednesday, August 27, 2025\n",
        "\n",
        "For this practical work, you need the following python packages:\n",
        "\n",
        "-   `openai`\n",
        "-   `python-dotenv`\n",
        "-   `faiss-cpu`\n",
        "-   `numpy`\n",
        "\n",
        "# Hello World\n",
        "\n",
        "Make work the example of the course.\n",
        "\n",
        "``` python\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "chat_response = client.chat.completions.create(\n",
        "    model= \"gpt-4o\",\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is the best French cheese?\",\n",
        "        },\n",
        "    ]\n",
        ")\n",
        "print(chat_response.choices[0].message.content)\n",
        "```\n",
        "\n",
        "Look at the documentation of the `OpenAI()` constructor in order to take\n",
        "your own model. Modify the model name accordingly.\n",
        "\n",
        "> **Important**\n",
        ">\n",
        "> **Never, ever** put your API key in the code. Use environment\n",
        "> variables instead. For example, use python `dotenv` to load the API\n",
        "> key from a `.env` file.\n",
        "\n",
        "> **Tip**\n",
        ">\n",
        "> The openai compatible endpoint for mistral.ai is\n",
        "> `https://api.mistral.ai/v1`. `mistral-small-latest` as the model\n",
        "> should be sufficient.\n",
        "\n",
        "> **Tip**\n",
        ">\n",
        "> If you have locally installed llm with ollama/lmstudio for example,\n",
        "> don’t hesitate to adapt the code to use your local model."
      ],
      "id": "bd52f83b-fc67-45e5-a1e4-90d7c4fa9c2f"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": [
          "solution"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The \"best\" French cheese is highly subjective and depends on personal taste, but here are some of the most celebrated and iconic French cheeses that are often considered the finest:\n",
            "\n",
            "### **Top Contenders for the Best French Cheese:**\n",
            "1. **Camembert de Normandie (AOP)** – A creamy, buttery, and earthy cow’s milk cheese with a bloomy rind. The best comes from Normandy and has a rich, mushroomy flavor.\n",
            "2. **Roquefort (AOP)** – A bold, tangy blue cheese made from sheep’s milk, aged in natural caves. It’s salty, crumbly, and intensely flavorful.\n",
            "3. **Comté (AOP)** – A nutty, caramelized cow’s milk cheese with a firm texture. Aged versions (18+ months) develop deep, complex flavors.\n",
            "4. **Brie de Meaux (AOP)** – The king of Brie, with a velvety texture and a rich, slightly funky aroma. Best enjoyed at peak ripeness.\n",
            "5. **Reblochon (AOP)** – A soft, creamy cheese from Savoie, known for its mild, slightly tangy flavor. Perfect for fondue.\n",
            "6. **Chèvre (Goat Cheese)** – Varieties like **Sainte-Maure de Touraine** or **Crottin de Chavignol** offer fresh, tangy, or aged flavors.\n",
            "7. **Munster (AOP)** – A pungent, washed-rind cheese from Alsace, with a strong aroma and creamy texture.\n",
            "8. **Beaufort (AOP)** – A smooth, buttery Alpine cheese with a nutty, slightly sweet taste.\n",
            "\n",
            "### **Best for Different Preferences:**\n",
            "- **Creamy & Mild:** Brie, Camembert\n",
            "- **Strong & Funky:** Munster, Époisses\n",
            "- **Blue Cheese:** Roquefort, Fourme d’Ambert\n",
            "- **Aged & Nutty:** Comté, Beaufort\n",
            "- **Goat Cheese:** Chèvre (Sainte-Maure, Crottin)\n",
            "\n",
            "### **Final Verdict:**\n",
            "If you had to pick **one** as the absolute best, **Roquefort** (for bold lovers) or **Comté** (for a balanced, crowd-pleasing choice) are often top contenders. But the best cheese is ultimately the one you enjoy most!\n",
            "\n",
            "Would you like recommendations based on a specific dish or pairing?"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "import dotenv\n",
        "import os\n",
        "\n",
        "dotenv.load_dotenv(\"/Users/fradav/.continue/.env\",verbose=True)\n",
        "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
        "base_url = \"https://api.mistral.ai/v1\"\n",
        "model = \"mistral-small-latest\"\n",
        "\n",
        "client = OpenAI(api_key=api_key, base_url=base_url)\n",
        "\n",
        "chat_response = client.chat.completions.create(\n",
        "    model= model,\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is the best French cheese?\",\n",
        "        },\n",
        "    ]\n",
        ")\n",
        "print(chat_response.choices[0].message.content)"
      ],
      "id": "d4068030"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Build a RAG Agent\n",
        "\n",
        "## Split the document in chunks\n",
        "\n",
        "Get the alice.txt and split it in 2048 characters."
      ],
      "id": "e3abe1d8-4702-4f16-8011-5309683c0f6c"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": [
          "solution"
        ]
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "84"
            ]
          }
        }
      ],
      "source": [
        "with open(\"../../materials/alice.txt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "chunk_size = 2048\n",
        "chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "len(chunks)"
      ],
      "id": "cf667ea9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encode the chunks\n",
        "\n",
        "A simple function to get an embedding is:"
      ],
      "id": "a42680bb-6b56-4a6e-8200-1474baa63756"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from time import sleep\n",
        "\n",
        "def get_text_embedding(input):\n",
        "    sleep(1) # Rate-limiting\n",
        "    embeddings_batch_response = client.embeddings.create(\n",
        "          model=\"mistral-embed\",\n",
        "          input=input\n",
        "      )\n",
        "    return embeddings_batch_response.data[0].embedding"
      ],
      "id": "e13a48e6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It does return a 1024 list of float (the embedding of the input).\n",
        "\n",
        "make a numpy array of all chunk embeddings from the text."
      ],
      "id": "5390f900-1d67-4f2e-9b44-4985fab29f8d"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "tags": [
          "solution"
        ]
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])"
      ],
      "id": "f363953b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(Should take one and half minute)\n",
        "\n",
        "## Store embeddings in vector database"
      ],
      "id": "7e5bcc6c-c8e3-4b22-9aa6-56d87acee466"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import faiss\n",
        "\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)"
      ],
      "id": "e6483bdf"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Query\n",
        "\n",
        "## Make an example query\n",
        "\n",
        "Make an embedding for a question like “À quels obstacles est confrontée\n",
        "Alice?”"
      ],
      "id": "8eecf462-e80e-4b27-a97c-42b87c08fd94"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "tags": [
          "solution"
        ]
      },
      "outputs": [],
      "source": [
        "question = \"À quels obstacles est confrontée Alice?\"\n",
        "question_embeddings = np.array([get_text_embedding(question)])"
      ],
      "id": "58acb493"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Search for the most similar chunks"
      ],
      "id": "83758782-77f1-4abc-9745-fbcb251b2eba"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "D, I = index.search(question_embeddings, k=2) # distance, index"
      ],
      "id": "40d47bd9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieve the chunks"
      ],
      "id": "912c2104-d19b-4b6c-96a1-38e5c406e423"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#! tags: [solution]\n",
        "retrieved_chunk = [chunks[i] for i in I.tolist()[0]]"
      ],
      "id": "39311003"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAG prompt\n",
        "\n",
        "Make the RAG query with the following prompt"
      ],
      "id": "c2c1511a-aa79-45ef-ab0d-c40bab95fd9f"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\n",
        "Context information is below.\n",
        "---------------------\n",
        "{retrieved_chunk}\n",
        "---------------------\n",
        "Given the context information and not prior knowledge, answer the query.\n",
        "Query: {question}\n",
        "Answer:\n",
        "\"\"\""
      ],
      "id": "fd054036"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "\"D'après le texte fourni, Alice est confrontée à plusieurs obstacles lors de la partie de croquet bizarre dans *Alice au pays des merveilles* :\\n\\n1. **Un terrain difficile** :\\n   - Le terrain est rempli de **creux et de bosses**, ce qui rend le jeu imprévisible et compliqué.\\n\\n2. **Des accessoires vivants et imprévisibles** :\\n   - **Les boules sont des hérissons vivants** qui se déroulent et s’éloignent, ce qui les rend difficiles à frapper.\\n   - **Les maillets sont des flamants vivants** :\\n     - Le flamant se retourne pour regarder Alice avec un air intrigué, la faisant rire et la distraire.\\n     - Il est difficile à manipuler (elle doit le tenir sous son bras, et il ne coopère pas).\\n   - **Les arceaux sont des soldats** qui se redressent et bougent constamment, empêchant de viser correctement.\\n\\n3. **Un jeu chaotique et sans règles claires** :\\n   - **Tout le monde joue en même temps** sans attendre son tour, ce qui crée de la confusion.\\n   - Les joueurs **se disputent et s’arrachent les hérissons**, rendant le jeu agressif et désorganisé.\\n   - La **Reine** crie régulièrement *« Qu’on lui coupe la tête ! »*, ce qui crée une atmosphère menaçante et stressante pour Alice.\\n\\n4. **Une pression sociale et des attentes étranges** :\\n   - Plus tard, lors de son récit devant la Simili-Tortue et le Griffon, Alice est **intimidée** par leurs regards et leurs demandes (comme réciter des poèmes déformés).\\n   - Elle se sent **jugée** et doit improviser des réponses ou des récits sous pression, ce qui déforme ses paroles (ex. : le poème du homard au lieu du flemmard).\\n\\n---\\n**En résumé** :\\nAlice affronte des **difficultés physiques** (terrain, animaux non coopératifs), **sociales** (règles absurdes, menace de la Reine, attentes des créatures) et **psychologiques** (stress, confusion, sentiment d’être en classe ou en danger). Le monde du Pays des Merveilles semble conçu pour la déstabiliser à chaque étape.\""
            ]
          }
        }
      ],
      "source": [
        "def run_mistral(user_message, model=\"mistral-large-latest\"):\n",
        "    sleep(1) # Rate-limit\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\", \"content\": user_message\n",
        "        }\n",
        "    ]\n",
        "    chat_response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "    return (chat_response.choices[0].message.content)\n",
        "\n",
        "run_mistral(prompt)"
      ],
      "id": "c9f37715"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final\n",
        "\n",
        "Make a function for any question about the book."
      ],
      "id": "40d10cc2-c934-4926-9a99-83597a1f1313"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "tags": [
          "solution"
        ]
      },
      "outputs": [],
      "source": [
        "def ask_book(question):\n",
        "    question_embeddings = np.array([get_text_embedding(question)])\n",
        "    D, I = index.search(question_embeddings, k=2) # distance, index\n",
        "    retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
        "    prompt = f\"\"\"\n",
        "Context information is below.\n",
        "---------------------\n",
        "{retrieved_chunk}\n",
        "---------------------\n",
        "Given the context information and not prior knowledge, answer the query.\n",
        "Query: {question}\n",
        "Answer:\n",
        "\"\"\"\n",
        "    return run_mistral(prompt)"
      ],
      "id": "fc3f57f8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##"
      ],
      "id": "1beab95f-17a0-4b57-ace1-19a4225d224f"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/Users/fradav/Documents/Dev/Python/Cours-programmation-MIASHS-2025/.venv/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  }
}