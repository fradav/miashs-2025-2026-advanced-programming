{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI Agent\n",
        "\n",
        "Taking the bull by the horns\n",
        "\n",
        "François-David Collin (CNRS, IMAG, Paul-Valéry Montpellier 3\n",
        "University)  \n",
        "Wednesday, August 27, 2025\n",
        "\n",
        "For this practical work, you need the following python packages:\n",
        "\n",
        "-   `openai`\n",
        "-   `python-dotenv`\n",
        "-   `faiss-cpu`\n",
        "-   `numpy`\n",
        "\n",
        "and also a nicely format function for markdown output of LLM:"
      ],
      "id": "963087c5-7e66-41f8-b5a3-698fa8ff8060"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "def format_markdown_output(response):\n",
        "    # Create a boxed effect using HTML and CSS\n",
        "    boxed_content = f\"\"\"\n",
        "<div style=\"border: 2px solid #f15006ff; padding: 10px; border-radius: 5px; background-color: #493f3fff;\">\n",
        "    {response}\n",
        "</div>\n",
        "\"\"\"\n",
        "    return display(Markdown(boxed_content))"
      ],
      "id": "242cf04f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hello World\n",
        "\n",
        "Make work the example of the course.\n",
        "\n",
        "``` python\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "chat_response = client.chat.completions.create(\n",
        "    model= \"gpt-4o\",\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is the best French cheese?\",\n",
        "        },\n",
        "    ]\n",
        ")\n",
        "print(chat_response.choices[0].message.content)\n",
        "```\n",
        "\n",
        "Look at the documentation of the `OpenAI()` constructor in order to take\n",
        "your own model. Modify the model name accordingly.\n",
        "\n",
        "> **Important**\n",
        ">\n",
        "> **Never, ever** put your API key in the code. Use environment\n",
        "> variables instead. For example, use python `dotenv` to load the API\n",
        "> key from a `.env` file.\n",
        "\n",
        "> **Tip**\n",
        ">\n",
        "> The openai compatible endpoint for mistral.ai is\n",
        "> `https://api.mistral.ai/v1`. `mistral-small-latest` as the model\n",
        "> should be sufficient.\n",
        "\n",
        "> **Tip**\n",
        ">\n",
        "> If you have locally installed llm with ollama/lmstudio for example,\n",
        "> don’t hesitate to adapt the code to use your local model."
      ],
      "id": "b53176ff-75ec-4081-9303-cbbe8a403329"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "output-location": "column",
        "tags": [
          "solution"
        ]
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import dotenv\n",
        "import os\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "dotenv.load_dotenv(\"/Users/fradav/.continue/.env\",verbose=True)\n",
        "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
        "base_url = \"https://api.mistral.ai/v1\"\n",
        "model = \"mistral-small-latest\"\n",
        "\n",
        "client = OpenAI(api_key=api_key, base_url=base_url)\n",
        "\n",
        "chat_response = client.chat.completions.create(\n",
        "    model= model,\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is the best French cheese?\",\n",
        "        },\n",
        "    ]\n",
        ")\n",
        "format_markdown_output(chat_response.choices[0].message.content)"
      ],
      "id": "145aac7a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Build a RAG Agent\n",
        "\n",
        "## Split the document in chunks\n",
        "\n",
        "Get the\n",
        "[alice.txt](https://fradav.github.io/miashs-2025-2026-advanced-programming/docs-resources/Notebooks/materials/alice.txt)\n",
        "and split it in 2048 characters."
      ],
      "id": "12ccc1ad-0b43-40d0-a341-dc477ae9a237"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": [
          "solution"
        ]
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "84"
            ]
          }
        }
      ],
      "source": [
        "with open(\"../../materials/alice.txt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "chunk_size = 2048\n",
        "chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "len(chunks)"
      ],
      "id": "203e06e9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encode the chunks\n",
        "\n",
        "A simple function to get an embedding is:"
      ],
      "id": "4dd83963-71e3-4e3f-b739-9de9710f8392"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from time import sleep\n",
        "\n",
        "def get_text_embedding(input):\n",
        "    sleep(0.5) # Rate-limiting\n",
        "    embeddings_response = client.embeddings.create(\n",
        "          model=\"mistral-embed\",\n",
        "          input=input\n",
        "      )\n",
        "    return embeddings_response.data[0].embedding"
      ],
      "id": "81cdaab1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It does return a 1024 list of float (the embedding of the input).\n",
        "\n",
        "Make a `numpy` array of all chunk embeddings from the text. Save the\n",
        "array to a file to avoid recomputing it."
      ],
      "id": "f6504639-7ec0-4d78-a7ac-479f995723a8"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "tags": [
          "solution"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings loaded from embeddings.npy"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "if not os.path.exists(\"embeddings.npy\"):\n",
        "    embeddings = np.array([get_text_embedding(chunk) for chunk in tqdm(chunks)])\n",
        "    np.save(\"embeddings.npy\", embeddings)\n",
        "    print(\"Embeddings saved to embeddings.npy\")\n",
        "else:\n",
        "    embeddings = np.load(\"embeddings.npy\")\n",
        "    print(\"Embeddings loaded from embeddings.npy\")"
      ],
      "id": "877d9e42"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(Should take one and half minute)\n",
        "\n",
        "> **Tip**\n",
        ">\n",
        "> An optimal version of this should use the [batch\n",
        "> feature](https://platform.openai.com/docs/guides/batch) of OpenAI APi\n",
        "\n",
        "## Store embeddings in vector database"
      ],
      "id": "ff0b3428-0861-4372-87ee-498f7e0c13d2"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import faiss\n",
        "\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)"
      ],
      "id": "dfe9938d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Query\n",
        "\n",
        "## Make an example query\n",
        "\n",
        "Make an embedding for a question like “À quels obstacles est confrontée\n",
        "Alice?”"
      ],
      "id": "628261b5-9f2b-450e-9b83-d121be3bd181"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "tags": [
          "solution"
        ]
      },
      "outputs": [],
      "source": [
        "question = \"À quels obstacles est confrontée Alice?\"\n",
        "question_embeddings = np.array([get_text_embedding(question)])"
      ],
      "id": "a8b9e337"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Search for the most semantically similar chunks"
      ],
      "id": "d1385578-e1cd-4b31-9da4-c566a72b5070"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "D, I = index.search(question_embeddings, k=2) # distance, index"
      ],
      "id": "883c03c9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieve the chunks"
      ],
      "id": "09829117-6842-40e8-a528-7b34a5398836"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "tags": [
          "solution"
        ]
      },
      "outputs": [],
      "source": [
        "retrieved_chunk = [chunks[i] for i in I.tolist()[0]]"
      ],
      "id": "e0a371eb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAG prompt\n",
        "\n",
        "Make the RAG query with the following prompt"
      ],
      "id": "aaa563f6-6176-427c-a287-1221c5517939"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\n",
        "Context information is below.\n",
        "---------------------\n",
        "{retrieved_chunk}\n",
        "---------------------\n",
        "Given the context information and not prior knowledge, answer the query.\n",
        "Query: {question}\n",
        "Answer:\n",
        "\"\"\""
      ],
      "id": "6503b7a0"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "output-location": "column"
      },
      "outputs": [],
      "source": [
        "def run_mistral(user_message, model=\"mistral-small-latest\"):\n",
        "    sleep(1) # Rate-limit\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\", \"content\": user_message\n",
        "        }\n",
        "    ]\n",
        "    chat_response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "    return (chat_response.choices[0].message.content)\n",
        "\n",
        "format_markdown_output(run_mistral(prompt))"
      ],
      "id": "11ad6799"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Put it together\n",
        "\n",
        "Make a function for any question about the book."
      ],
      "id": "ca9a54f3-17a9-4bae-804a-4a06ce3e1c0b"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "tags": [
          "solution"
        ]
      },
      "outputs": [],
      "source": [
        "def ask_book(question):\n",
        "    question_embeddings = np.array([get_text_embedding(question)])\n",
        "    D, I = index.search(question_embeddings, k=2) # distance, index\n",
        "    retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
        "    prompt = f\"\"\"\n",
        "Context information is below.\n",
        "---------------------\n",
        "{retrieved_chunk}\n",
        "---------------------\n",
        "Given the context information and not prior knowledge, answer the query.\n",
        "Query: {question}\n",
        "Answer:\n",
        "\"\"\"\n",
        "    return run_mistral(prompt)"
      ],
      "id": "4225bd5f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Areas for improvement\n",
        "\n",
        "-   Chunking strategies: chunk sizes, overlap, metadata\n",
        "-   LLM parameters: context size, temperature, top_p, etc.\n",
        "-   reranking\n",
        "-   hybrid retrieval\n",
        "\n",
        "# MCP server\n",
        "\n",
        "## How to test/install a python MCP server\n",
        "\n",
        "As simple mcp server requires to be just a complete one-liner script to\n",
        "run, simply create a new python file (e.g. `mcp_server.py`) and add the\n",
        "following code:\n",
        "\n",
        "``` python\n",
        "from mcp.server.fastmcp import FastMCP\n",
        "\n",
        "mcp = FastMCP(\"MyMCPServer\")\n",
        "\n",
        "@mcp.tool()\n",
        "def my_tool():\n",
        "    \"\"\"\n",
        "    This is my tool, it does nothing.\n",
        "\n",
        "    Args:\n",
        "        None\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with the status of the tool.\n",
        "    \"\"\"\n",
        "    return {\"message\": \"Hello from my_tool!\"}\n",
        "```\n",
        "\n",
        "``` bash\n",
        "uv run --with fastmcp mcp run path/to/mcp_server.py\n",
        "```\n",
        "\n",
        "in a yaml/json file for mcp server:\n",
        "\n",
        "## YAML\n",
        "\n",
        "``` yaml\n",
        "    command: uv\n",
        "    args:\n",
        "      - run\n",
        "      - --with\n",
        "      - fastmcp\n",
        "      - mcp\n",
        "      - run\n",
        "      - mcp_server.py\n",
        "```\n",
        "\n",
        "## JSON\n",
        "\n",
        "``` json\n",
        "{\n",
        "  \"command\": \"uv\",\n",
        "  \"args\": [\n",
        "    \"run\",\n",
        "    \"--with\",\n",
        "    \"fastmcp\",\n",
        "    \"mcp\",\n",
        "    \"run\",\n",
        "    \"mcp_server.py\"\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "## First draft\n",
        "\n",
        "We want to make an MCP server which logs useful stuff in standard\n",
        "logging format to a file specified by environment variable\n",
        "`MCP_LOGGING_FILE`.\n",
        "\n",
        "Use [FastMCP](https://gofastmcp.com/getting-started/welcome) as MCP SDK.\n",
        "\n",
        "We should use standard `logging` python package."
      ],
      "id": "9665f231-23f7-4561-92dc-674833ba0b29"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "tags": [
          "solution"
        ]
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "from mcp.server.fastmcp import FastMCP\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    filename=os.getenv('MCP_LOGGING_FILE', 'default.log'),\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# Create an MCP server\n",
        "mcp = FastMCP(\"MCPLogging\")\n",
        "\n",
        "@mcp.tool()\n",
        "def log_message(message: str):\n",
        "    \"\"\"\n",
        "    Logs a message to the specified log file.\n",
        "\n",
        "    Args:\n",
        "        message (str): The message to log.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with the status of the logging operation.\n",
        "    \"\"\"\n",
        "    logging.info(message)\n",
        "    return {\"status\": \"Message logged\"}"
      ],
      "id": "3bb2b25c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Refining\n",
        "\n",
        "We want to let the LLM choose the logging level"
      ],
      "id": "6b633e6b-475e-4eb2-b25c-b4db7d5351b3"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "tags": [
          "solution"
        ]
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "from mcp.server.fastmcp import FastMCP\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    filename=os.getenv('MCP_LOGGING_FILE', 'default.log'),\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# Create an MCP server\n",
        "mcp = FastMCP(\"MCPLogging\")\n",
        "\n",
        "@mcp.tool()\n",
        "def log_message(message: str, level: str = \"INFO\"):\n",
        "    \"\"\"\n",
        "    Logs a message to the specified log file at the specified level.\n",
        "\n",
        "    Args:\n",
        "        message (str): The message to log.\n",
        "        level (str): The logging level (e.g., \"INFO\", \"WARNING\", \"ERROR\"). Defaults to \"INFO\".\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with the status of the logging operation.\n",
        "    \"\"\"\n",
        "    level = level.upper()\n",
        "    if level == \"INFO\":\n",
        "        logging.info(message)\n",
        "    elif level == \"WARNING\":\n",
        "        logging.warning(message)\n",
        "    elif level == \"ERROR\":\n",
        "        logging.error(message)\n",
        "    else:\n",
        "        return {\"status\": \"Invalid logging level\"}\n",
        "    return {\"status\": \"Message logged\"}"
      ],
      "id": "8ed8f315"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Playing with it\n",
        "\n",
        "Make a rule to use the tool at each LLM request, test it. Pay attention\n",
        "to the workflow, especially with devstral.\n",
        "\n",
        "<span class=\"proof-title\">*Solution*. </span>\n",
        "\n",
        "``` markdown\n",
        "# Logging each answer\n",
        "\n",
        "Each time your answer:\n",
        "- Choose a concise summary of your answer\n",
        "- Choose a level of importance `INFO`, `WARNING`, `ERROR`\n",
        "- log that\n",
        "- after logging, if successful, just say \"Logged.\"\n",
        "\n",
        "DO NOT FORGET TO LOG EACH TIME\n",
        "```"
      ],
      "id": "78e5a48f-ffdb-4cf9-8b93-ca0b5a9dddf6"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/Users/fradav/Documents/Dev/Python/Cours-programmation-MIASHS-2025/.venv/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  }
}