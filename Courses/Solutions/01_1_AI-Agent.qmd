---
title: AI Agent
subtitle: Taking the bull by the horns
---

For this practical work, you need the following python packages:

- `openai`
- `python-dotenv`
- `faiss-cpu`
- `numpy`

# Hello World 

Make work the example of the course.

```python
from openai import OpenAI
client = OpenAI()

chat_response = client.chat.completions.create(
    model= "gpt-4o",
    messages = [
        {
            "role": "user",
            "content": "What is the best French cheese?",
        },
    ]
)
print(chat_response.choices[0].message.content)
```

Look at the documentation of the `OpenAI()` constructor in order to take your own model. Modify the model name accordingly.

::: {.callout-important}
**Never, ever** put your API key in the code. Use environment variables instead. For example, use python `dotenv` to load the API key from a `.env` file.
:::

::: {.callout-tip}
The openai compatible endpoint for mistral.ai is `https://api.mistral.ai/v1`. `mistral-small-latest` as the model should be sufficient.
:::

::: {.callout-tip}
If you have locally installed llm with ollama/lmstudio for example, don’t hesitate to adapt the code to use your local model.
:::

```{python}
#| tags: [solution]
from openai import OpenAI
import dotenv
import os

dotenv.load_dotenv("/Users/fradav/.continue/.env",verbose=True)
api_key = os.getenv("MISTRAL_API_KEY")
base_url = "https://api.mistral.ai/v1"
model = "mistral-small-latest"

client = OpenAI(api_key=api_key, base_url=base_url)

chat_response = client.chat.completions.create(
    model= model,
    messages = [
        {
            "role": "user",
            "content": "What is the best French cheese?",
        },
    ]
)
print(chat_response.choices[0].message.content)
```

# Build a RAG Agent

## Split the document in chunks

Get the alice.txt and split it in 2048 characters.

```{python}
#| tags: [solution]
with open("../../materials/alice.txt", "r") as f:
    text = f.read()
chunk_size = 2048
chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
len(chunks)
```

## Encode the chunks

A simple function to get an embedding is:

```{python}
from time import sleep

def get_text_embedding(input):
    sleep(1) # Rate-limiting
    embeddings_batch_response = client.embeddings.create(
          model="mistral-embed",
          input=input
      )
    return embeddings_batch_response.data[0].embedding
```

It does return a 1024 list of float (the embedding of the input).

make a numpy array of all chunk embeddings from the text.

```{python}
#| tags: [solution]

import numpy as np

embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])
```

(Should take one and half minute)

## Store embeddings in vector database

```{python}
import faiss

index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(embeddings)
```

# Query

## Make an example query

Make an embedding for a question like "À quels obstacles est confrontée Alice?"

```{python}
#| tags: [solution]

question = "À quels obstacles est confrontée Alice?"
question_embeddings = np.array([get_text_embedding(question)])
```

## Search for the most similar chunks

```{python}
D, I = index.search(question_embeddings, k=2) # distance, index
```

## Retrieve the chunks

```{python}
#! tags: [solution]
retrieved_chunk = [chunks[i] for i in I.tolist()[0]]
```

## RAG prompt

Make the RAG query with the following prompt
```{python}
prompt = f"""
Context information is below.
---------------------
{retrieved_chunk}
---------------------
Given the context information and not prior knowledge, answer the query.
Query: {question}
Answer:
"""
```

```{python}
def run_mistral(user_message, model="mistral-large-latest"):
    sleep(1) # Rate-limit
    messages = [
        {
            "role": "user", "content": user_message
        }
    ]
    chat_response = client.chat.completions.create(
        model=model,
        messages=messages
    )
    return (chat_response.choices[0].message.content)

run_mistral(prompt)
```

# Final

Make a function for any question about the book.

```{python}
#| tags: [solution]

def ask_book(question):
    question_embeddings = np.array([get_text_embedding(question)])
    D, I = index.search(question_embeddings, k=2) # distance, index
    retrieved_chunk = [chunks[i] for i in I.tolist()[0]]
    prompt = f"""
Context information is below.
---------------------
{retrieved_chunk}
---------------------
Given the context information and not prior knowledge, answer the query.
Query: {question}
Answer:
"""
    return run_mistral(prompt)
```

## 