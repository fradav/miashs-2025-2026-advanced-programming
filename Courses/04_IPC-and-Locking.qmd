---
title: IPC and locking
subtitle: Interprocess communication and how to not shoot yourself in the foot
nocite: |
    @aubanel2016elements
---

# Inter-Process Communication

## Remainder on Process-level parallelization

:::{layout-ncol="2"}

![Multi-Processing](tikz-figures/multiprocessing_model.svg)

![Multi-Threading](tikz-figures/multithreading_model.svg)

:::

## Inter-process is easy...

::::::: incremental

- But if my algorithm is **not** "embarrassingly parallel", what if we want to **share** data between processes ? 
- let's go for _Shared Memory_

:::::::

## Shared Memory Model

```{.asciiflow}
┌─────────────────────────────┐    ┌─────────────────────────────┐
│                             │    │                             │
│ ┌──────────┐   ┌──────────┐ │    │ ┌──────────┐   ┌──────────┐ │
│ │          │   │          │ │    │ │          │   │          │ │
│ │  CORE 1  │   │  CORE 2  │ │    │ │  CORE 3  │   │  CORE 4  │ │
│ │          │   │          │ │    │ │          │   │          │ │
│ └─┬──┬─────┘   └────┬─────┘ │    │ └┬─────────┘   └──────┬───┘ │
│   │  │              │       │    │  │                    │     │
│   │  │              │       │    │  │                    │     │
│   │  │  CPU 1       │       │    │  │      CPU 2         │     │
│   │  │              │       │    │  │                    │     │
└───┼──┼──────────────┼───────┘    └──┼────────────────────┼─────┘
    │  │              │               │                    │
    │  │              └────────────┐  │                    │
    │  │                           │  │                    │
    │  └─────────────────────────┐ │  │                    │
    │                            │ │  │  ┌─────────────────┘
    └──────────────────────────┐ │ │  │  │
                               │ │ │  │  │
┌──────────────────────────────┼─┼─┼──┼──┼──────────────────────┐
│                              │ │ │  │  │                      │
│ ┌─────┐  ┌─────┐  ┌─────┐  ┌─▼─▼─▼──▼──▼─┐                    │
│ │     │  │     │  │     │  │Shared Memory│                    │
│ └─────┘  └─────┘  └─────┘  └─────────────┘                    │
│                                      Main Memory              │
└───────────────────────────────────────────────────────────────┘
```

## Aside : memory models

::: {layout="[10,-15,10]" layout-valign="top"}

![UMA](figs/uma.svg){height="300px"}

![NUMA](figs/numa.svg){height="500px"}

There are differents models
::: 

## Shared FIFOs : Queues

An ubiquitous tool in multiprocessing (and distributed computing) is shared memory `FIFO` list, aka **Queues**.

## Shared FIFOs : Queues, (2)

A FIFO is a :

- Linked list 
- with FIFO (*First In First Out*) semantics, with `enqueue(x)` et `dequeue()` function (or `push(x)`/`pop()`)

![](figs/fifo.svg){ height=50% fig-align="center"}

## Shared FIFOs : Queues, (3)

In the context of multi-processing (or multi-threading) :

Shared Memory + FIFO list = **Queue**

## Shared FIFOs : Queues, (4)

Queues are the basis of the *consumer/producer* model, which is widely used in concurrent and distributed applications.

## When to use queues?

An algorithm with two computations $A$ and $B$ where :

- $B$ depends on the result of $A$
- $A$ is independent of $B$

. . .

$A$ could be a *producer* for $B$, and $B$ a *consumer* for $A$. 

## How to use queues?

```{.asciiflow}
┌───────────┐
│           │
│ Producer  │
│           │ Process A
│           │
└─────┬─────┘
      │
 ┌────┼───────────────────────────────────────────────────────────────────┐
 │    │                         Queue                                     │
 │    │        ┌─────┬─────┬─────┬─────┬─────┬─────┬─────┐                │
 │    │        │     │     │     │     │     │     │     │                │
 │    └───────►│     │     │     │     │     │     │     ├──────────┐     │
 │             │     │     │     │     │     │     │     │          │     │
 │             └─────┴─────┴─────┴─────┴─────┴─────┴─────┘          │     │
 │                                                                  │     │
 │        Shared Memory                                             │     │
 └──────────────────────────────────────────────────────────────────┼─────┘
                                                                    │
                                                                    ▼
                                                              ┌───────────┐
                                                              │           │
                                                   Process B  │ Consumer  │
                                                              │           │
                                                              │           │
                                                              └───────────┘
```

## Producer/consumer, Examples

- $A$ finds primes in a list of number, $B$ formats and prints them every 10 numbers found.
- $A$ fetches a bunch of images on the web, $B$ downloads them and saves them to disk.
- $A$ takes the orders in the restaurant, $B$ cooks them.

. . .

![](figs/overcooked.gif){ .noinvert height=30% fig-align="center" }

# More on locking

## The main gotcha

what if several processes want to write/read the same shared memory portions at the same time?

. . .

Enter the realm of the dreaded **__race condition__** 

![](figs/planning-execution.png){ .noinvert width=25% fig-align="center" }

## Simple example

Printing from several processes a string with 10 times the same char.

::::: {.columns}

:::: {.column width="50%"}
```python
from multiprocessing.pool import Pool
from itertools import repeat
# print "AAAAAAAAA", "BBBBBBBBBBB" etc.
def repeat10Cap(c): 
    print("".join(repeat(chr(c+65),10))) 
with Pool(8) as pool:
    pool.map(repeat10Cap, range(10))
```
::::

:::: {.column width="50%"}

::: {.fragment}
Output:
```
AAAAAAAAAACCCCCCCCCCBBBBBBBBBBDDDDDDDDDDEEEEEEEEEE


FFFFFFFFFFGGGGGGGGGG
IIIIIIIIII

HHHHHHHHHH
JJJJJJJJJJ

```
:::
::::
:::::

# The answer : critical section 

:::::::::::::: {.columns}
::: {.column width="60%"}

A critical section is :

- a multiprocessing (and also multithreading) primitive which *decorates* a portion of code.
- guaranteed to be run by only ONE process at a time.

:::
::: {.column width="40%"}
```{.asciiflow}
┌─────────────┐
│             │
│    Normal   │
│     Code    │      Parallelized
│             │
└──────┬──────┘
       │
┌──────▼──────┐
│             │
│   Critical  │      Not parallelized
│    Section  │
│             │
└──────┬──────┘
       │
┌──────▼──────┐
│             │
│    Normal   │      Parallelized
│     Code    │
│             │
└─────────────┘
```
:::
::::::::::::::

## Critical section workflow

:::{.content-hidden when-format="revealjs"}
![Three processes with critical section](tikz-figures/critical-section.svg){width=100%}
:::

:::{.content-visible when-format="revealjs"}

[]{.fragment .fade-in-then-out}
[]{.fragment .fade-in-then-out}
[]{.fragment .fade-in-then-out}
[]{.fragment .fade-in-then-out}
[]{.fragment .fade-in-then-out}

```yaml { .animate style="width: 700px;" src="tikz-figures/critical-section.svg"}
setup:
  - element: "[id^=t], [id^=zone]"
    modifier: "opacity"
    parameters: [ 0 ]
animation:
- []
- 
  - element: "#t1"
    modifier: "opacity"
    parameters: [ 1 ]
  - element: "#zone1"
    modifier: "opacity"
    parameters: [ 1 ]
  - element: "#t2"
    modifier: "opacity"
    parameters: [ 1 ]
-
  - element: "#zone2"
    modifier: "opacity"
    parameters: [ 1 ]
  - element: "#t3"
    modifier: "opacity"
    parameters: [ 1 ]
-
  - element: "#t4"
    modifier: "opacity"
    parameters: [ 1 ]
-
  - element: "#zone3"
    modifier: "opacity"
    parameters: [ 1 ]
  - element: "#t5"
    modifier: "opacity"
    parameters: [ 1 ]
-
  - element: "#t6"
    modifier: "opacity"
    parameters: [ 1 ]
```
:::

## A simple implementation in Python : Lock { .smaller }

::::: {.columns}

:::: {.column width="50%"}
```python
from multiprocessing.pool import Pool
from multiprocessing import Lock
from itertools import repeat
lock = Lock()
def safe_repeat10Cap(c):
    with lock: 
        # Beginning of critical section
        print("".join(repeat(chr(c+65),10)))
        # End of critical section
with Pool(8) as pool:
    pool.map(safe_repeat10Cap, range(10))
```
::::

:::: {.column width="50%"}

::: {.fragment}
Output:
```
AAAAAAAAAA
BBBBBBBBBB
CCCCCCCCCC
DDDDDDDDDD
EEEEEEEEEE
FFFFFFFFFF
GGGGGGGGGG
HHHHHHHHHH
IIIIIIIIII
JJJJJJJJJJ
```
:::
::::
:::::


# When to use locks ?

::::::::: incremental

- Concurrent access to shared data structures
- Structural consistency not guaranteed.

:::::::::

## Consistency problems with FIFO example I

Process $A$ (resp. $B$) wants to `push` $x$ (resp. $y$) on the list.

![$\Longrightarrow$ Consistency problem if they both create a new linked node to node $3$.](tikz-figures/racecond1.svg)

## Consistency problems with FIFO example 2

Process $A$ and $B$ both want to `pop` the list. 

![$\Longrightarrow$ Consistency problem if they both pop the same node.](tikz-figures/racecond2.svg)


## (No) Consistency problems with FIFO example 3

![No problem there.](tikz-figures/racecond3.svg)

. . .

:::{.callout-warning}
⚠ ⚠ *As long the list is not empty* ⚠ ⚠
:::

# Locking, refined { transition="none" }

Beware of putting locks everywhere...
Beware... 

. . .

![](figs/deadlock-coming.png){ .notransparent .noinvert height=50% fig-align="center" }

## Deadlock example

![](figs/deadlock-job.png){ .notransparent .noinvert height=50% fig-align="center" }

## Deadlock (serious) example

:::{.content-hidden when-format="revealjs"}
![Deadlock illustration](tikz-figures/deadlock.svg){width=100%}
:::

::::{ style="min-height: 1.5em;"}
[Process $A$ acquires lock $L1$.]{.fragment .fade-in-then-out .absolute}
[Process $B$ acquires lock $L2$.]{.fragment .fade-in-then-out .absolute}
[Process $A$ tries to acquire lock $L2$, but it is already held by $B$.]{.fragment .fade-in-then-out .absolute}
[Process $B$ tries to acquire lock $L1$, but it is already held by $A$.]{.fragment .fade-in-then-out .absolute}
[Both processes are blocked.]{.fragment .fade-in-then-out .absolute}
::::

:::{.content-visible when-format="revealjs"}

```yaml { .animate src="tikz-figures/deadlock.svg"}
setup:
  - element: "#AL1, #BL2, #AL2, #BL1"
    modifier: "opacity"
    parameters: [ 0 ]
animation:
- []
- 
  - element: "#AL1"
    modifier: "opacity"
    parameters: [ 1 ]
-
  - element: "#BL2"
    modifier: "opacity"
    parameters: [ 1 ]
-
  - element: "#AL2"
    modifier: "opacity"
    parameters: [ 1 ]
  - element: "#A text"
    modifier: "fill"
    parameters: [ "#0F0" ]
-
  - element: "#BL1"
    modifier: "opacity"
    parameters: [ 1 ]
  - element: "#B text"
    modifier: "fill"
    parameters: [ "#0F0" ]
```
:::

## Avoiding Deadlocks

There is several ways to avoid deadlocks. One of them is the [Dijkstra](https://en.wikipedia.org/wiki/Edsger_W._Dijkstra)'s [Resource Hiearchy Solution](https://en.wikipedia.org/wiki/Dining_philosophers_problem#Resource_hierarchy_solution).

. . .

In the previous example, processes should try the lowest numbered locks first. Instead of $B$ acquiring $L2$ first, it should tries to acquire $L1$ instead and $L2$ after.

. . .

This solution isn't universal but is pretty usable in general case.

# Conclusion

Diving (a little) deeper into parallelism, when computations are NOT independent of each other (no *embarrasingly parallel* approach), we need a way to decouple processing of data, while still keeping the dependancies intact.

. . .

$\Longrightarrow$ Shared Memory and Queues to the rescue

. . .

With the concurrent use of ressources, there are two pitfalls to be aware of:

::::::: incremental

- **Race Conditions**, solution : locking
- **Deadlocks**, solution : careful and consistent ordering of locks.

::::::: 

# References {.allowframebreaks}