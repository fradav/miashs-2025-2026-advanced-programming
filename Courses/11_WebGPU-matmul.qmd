---
title: "Exploring WebGPU Matmul Kernels (OJS)"
description: "Interactive Observable JS page to experiment with WebGPU matrix multiplication kernels and measure performance in your browser."
format:
  html:
    toc: true
    # code-fold: show
    # code-tools: true

execute:
  echo: false
categories: [webgpu, wgsl, gpu, matmul, ojs]
---

> This interactive page re-creates (in spirit) several matrix-multiply (matmul) kernels discussed by Zach Nussbaum in “Optimizing a WebGPU Matmul Kernel for 1TFLOP+ Performance” and lets you run them live with WebGPU + Observable JS. The write-up here is paraphrased; see the original article for the full narrative:
> https://www.nuss-and-bolts.com/p/optimizing-a-webgpu-matmul-kernel

We’ll build up a few compute shaders in WGSL and compare their throughput:

- Naive 1D kernel (one thread per output element)
- 1D with many threads per workgroup
- 2D workgroups (8×8)
- Tiled/unrolled kernels (1×4 and 4×4 per thread)

You can tweak M, K, N and the kernel, then measure latency and GFLOP/s. This runs entirely in the browser using WebGPU; make sure your browser/device supports it.

## WebGPU setup

```{ojs}
gpu = navigator.gpu
```

```{ojs}
adapter = gpu && (await navigator.gpu.requestAdapter())
```

```{ojs}
// Try to request a device with higher limits for large matrices
device = adapter && (await adapter.requestDevice({
  requiredFeatures: adapter.features.has("timestamp-query") ? ["timestamp-query"] : [],
  requiredLimits: {
    maxStorageBufferBindingSize: Math.min(adapter.limits.maxStorageBufferBindingSize, 2 * 1024 * 1024 * 1024), // 2GB or adapter max
    maxBufferSize: Math.min(adapter.limits.maxBufferSize, 2 * 1024 * 1024 * 1024), // 2GB or adapter max
    maxStorageBuffersPerShaderStage: Math.max(adapter.limits.maxStorageBuffersPerShaderStage, 8),
    maxComputeWorkgroupStorageSize: Math.max(adapter.limits.maxComputeWorkgroupStorageSize, 32768)
  }
}))
```

```{ojs}
queue = device && device.queue
```

```{ojs}
webgpu_ok = !!device
```

```{ojs}
// A small status banner
html`<div class="callout ${webgpu_ok ? 'callout-note' : 'callout-warning'}"><strong>WebGPU</strong>: ${webgpu_ok ? 'OK — device acquired' : 'Not available — try Chrome/Edge/Arc, enable WebGPU, or use a recent macOS'}</div>`
```

### Helpers (buffers, shader, run)

```{ojs}
// Create a GPU buffer and optionally initialize with data
function createBuffer(device, dataOrSize, usage) {
  if (typeof dataOrSize === 'number') {
    const n = dataOrSize
    const size = Number.isFinite(n) && n > 0 ? ((Math.ceil(n / 4) * 4) >>> 0) : 0
    if (!Number.isFinite(size) || size <= 0) {
      throw new Error(`Invalid buffer size: ${n}`)
    }
    return device.createBuffer({ size, usage })
  }
  const data = dataOrSize
  const [byteLength, size] = [data.byteLength, ((data.byteLength + 3) & ~3)] // 4-byte align
  const buffer = device.createBuffer({ size, usage, mappedAtCreation: true })
  new Uint8Array(buffer.getMappedRange()).set(new Uint8Array(data.buffer, data.byteOffset, data.byteLength))
  buffer.unmap()
  return buffer
}
```

```{ojs}
// Compile a WGSL compute pipeline
function makePipeline(device, wgsl) {
  const module = device.createShaderModule({ code: wgsl })
  const pipeline = device.createComputePipeline({
    layout: 'auto',
    compute: { module, entryPoint: 'main' }
  })
  return pipeline
}
```

```{ojs}
// Uniform struct needs 16-byte size; pad after M,K,N
function makeDimsUniform(device, M, K, N) {
  const u32 = new Uint32Array([M, K, N, 0]) // pad
  return createBuffer(device, u32, GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST)
}
```

```{ojs}
// CPU reference (for small sizes) to optionally check correctness
function cpuMatmul(A, B, M, K, N) {
  const C = new Float32Array(M * N)
  for (let i = 0; i < M; i++) {
    for (let j = 0; j < N; j++) {
      let s = 0
      for (let k = 0; k < K; k++) s += A[i*K + k] * B[k*N + j]
      C[i*N + j] = s
    }
  }
  return C
}
```

### WGSL kernels

Below are WGSL strings (paraphrased from the ideas in the article). All kernels
expect the same bindings:

1. `@group(0) @binding(0)` uniform Dimensions { M,K,N,pad }
2. `@group(0) @binding(1)` storage read A (row-major M×K)
3. `@group(0) @binding(2)` storage read B (row-major K×N)
4. `@group(0) @binding(3)` storage read_write C (row-major M×N)

```{ojs}
wgsl_naive_1d = /* wgsl */ `
struct Dimensions { M: u32, K: u32, N: u32, _pad: u32 };
@group(0) @binding(0) var<uniform> dims: Dimensions;
@group(0) @binding(1) var<storage, read> A: array<f32>;
@group(0) @binding(2) var<storage, read> B: array<f32>;
@group(0) @binding(3) var<storage, read_write> C: array<f32>;

@compute @workgroup_size(1)
fn main(@builtin(global_invocation_id) gid: vec3<u32>) {
  // Handle multi-dimensional dispatch properly
  let idx = gid.x + gid.y * 65535u + gid.z * 65535u * 65535u;
  let M = dims.M; let K = dims.K; let N = dims.N;
  if (idx >= M * N) { return; }
  let row = idx / N;
  let col = idx % N;
  var sum: f32 = 0.0;
  for (var kk: u32 = 0u; kk < K; kk = kk + 1u) {
    sum = sum + A[row * K + kk] * B[kk * N + col];
  }
  C[row * N + col] = sum;
}
`
```

```{ojs}
wgsl_1d_many_threads = /* wgsl */ `
struct Dimensions { M: u32, K: u32, N: u32, _pad: u32 };
@group(0) @binding(0) var<uniform> dims: Dimensions;
@group(0) @binding(1) var<storage, read> A: array<f32>;
@group(0) @binding(2) var<storage, read> B: array<f32>;
@group(0) @binding(3) var<storage, read_write> C: array<f32>;

@compute @workgroup_size(256)
fn main(@builtin(global_invocation_id) gid: vec3<u32>) {
  // Handle multi-dimensional dispatch properly  
  let idx = gid.x + gid.y * 65535u + gid.z * 65535u * 65535u;
  let M = dims.M; let K = dims.K; let N = dims.N;
  if (idx >= M * N) { return; }
  let row = idx / N;
  let col = idx % N;
  var sum: f32 = 0.0;
  for (var kk: u32 = 0u; kk < K; kk = kk + 1u) {
    sum = sum + A[row * K + kk] * B[kk * N + col];
  }
  C[row * N + col] = sum;
}
`
```

```{ojs}
wgsl_2d_8x8 = /* wgsl */ `
struct Dimensions { M: u32, K: u32, N: u32, _pad: u32 };
@group(0) @binding(0) var<uniform> dims: Dimensions;
@group(0) @binding(1) var<storage, read> A: array<f32>;
@group(0) @binding(2) var<storage, read> B: array<f32>;
@group(0) @binding(3) var<storage, read_write> C: array<f32>;

@compute @workgroup_size(8, 8)
fn main(@builtin(global_invocation_id) gid: vec3<u32>) {
  // Note: swap to favor coalesced access as discussed in the article
  let row = gid.y; // M
  let col = gid.x; // N
  let M = dims.M; let K = dims.K; let N = dims.N;
  if (row >= M || col >= N) { return; }
  var sum: f32 = 0.0;
  for (var kk: u32 = 0u; kk < K; kk = kk + 1u) {
    sum = sum + A[row * K + kk] * B[kk * N + col];
  }
  C[row * N + col] = sum;
}
`
```

```{ojs}
wgsl_tile_1x4 = /* wgsl */ `
const BLOCKSIZE: u32 = 16u; // 16x16 threads per workgroup
const TILE_N: u32 = 4u;     // 4 columns per thread
struct Dimensions { M: u32, K: u32, N: u32, _pad: u32 };
@group(0) @binding(0) var<uniform> dims: Dimensions;
@group(0) @binding(1) var<storage, read> A: array<f32>;
@group(0) @binding(2) var<storage, read> B: array<f32>;
@group(0) @binding(3) var<storage, read_write> C: array<f32>;

@compute @workgroup_size(BLOCKSIZE, BLOCKSIZE)
fn main(@builtin(global_invocation_id) gid: vec3<u32>) {
  let M = dims.M; let K = dims.K; let N = dims.N;
  let row = gid.y;
  let col0 = gid.x * TILE_N;
  if (row >= M || col0 >= N) { return; }

  var s0: f32 = 0.0;
  var s1: f32 = 0.0;
  var s2: f32 = 0.0;
  var s3: f32 = 0.0;
  for (var kk: u32 = 0u; kk < K; kk = kk + 1u) {
    let a = A[row * K + kk];
    let base = kk * N + col0;
    s0 = s0 + a * B[base + 0u];
    s1 = s1 + a * B[base + 1u];
    s2 = s2 + a * B[base + 2u];
    s3 = s3 + a * B[base + 3u];
  }
  C[row * N + col0 + 0u] = s0;
  if (col0 + 1u < N) { C[row * N + col0 + 1u] = s1; }
  if (col0 + 2u < N) { C[row * N + col0 + 2u] = s2; }
  if (col0 + 3u < N) { C[row * N + col0 + 3u] = s3; }
}
`
```

```{ojs}
wgsl_tile_4x4 = /* wgsl */ `
const BLOCKSIZE: u32 = 16u;
const TILE_M: u32 = 4u; // 4 rows per thread
const TILE_N: u32 = 4u; // 4 cols per thread
struct Dimensions { M: u32, K: u32, N: u32, _pad: u32 };
@group(0) @binding(0) var<uniform> dims: Dimensions;
@group(0) @binding(1) var<storage, read> A: array<f32>;
@group(0) @binding(2) var<storage, read> B: array<f32>;
@group(0) @binding(3) var<storage, read_write> C: array<f32>;

@compute @workgroup_size(BLOCKSIZE, BLOCKSIZE)
fn main(@builtin(global_invocation_id) gid: vec3<u32>) {
  let M = dims.M; let K = dims.K; let N = dims.N;
  let row0 = gid.y * TILE_M;
  let col0 = gid.x * TILE_N;
  if (row0 >= M || col0 >= N) { return; }

  var s: array<array<f32, TILE_N>, TILE_M>;
  // initialize
  for (var ii: u32 = 0u; ii < TILE_M; ii = ii + 1u) {
    for (var jj: u32 = 0u; jj < TILE_N; jj = jj + 1u) {
      s[ii][jj] = 0.0;
    }
  }

  // unrolled accumulation over K stays as a loop (K unknown at compile time)
  for (var kk: u32 = 0u; kk < K; kk = kk + 1u) {
    for (var ii: u32 = 0u; ii < TILE_M; ii = ii + 1u) {
      let a = A[(row0 + ii) * K + kk];
      let bbase = kk * N + col0;
      for (var jj: u32 = 0u; jj < TILE_N; jj = jj + 1u) {
        s[ii][jj] = s[ii][jj] + a * B[bbase + jj];
      }
    }
  }

  for (var ii: u32 = 0u; ii < TILE_M; ii = ii + 1u) {
    let r = row0 + ii;
    if (r < M) {
      for (var jj: u32 = 0u; jj < TILE_N; jj = jj + 1u) {
        let c = col0 + jj;
        if (c < N) { C[r * N + c] = s[ii][jj]; }
      }
    }
  }
}
`
```

### Kernel registry and dispatch math

```{ojs}
kernels = [
  { 
    key: 'naive1d', 
    label: 'Naive 1D (wg_size=1)', 
    wgsl: wgsl_naive_1d, 
    dispatch: ({M,N}) => {
      const total = M*N
      // Respect 65535 limit per dimension, use 2D dispatch if needed
      if (total <= 65535) return [total, 1, 1]
      const x = Math.min(65535, Math.ceil(Math.sqrt(total)))
      const y = Math.ceil(total / x)
      return [x, Math.min(65535, y), Math.max(1, Math.ceil(y / 65535))]
    }, 
    limits: { recommendMaxMN: 512 } 
  },
  { 
    key: 'oneD256', 
    label: '1D many threads (256)', 
    wgsl: wgsl_1d_many_threads, 
    dispatch: ({M,N}) => {
      const total = Math.ceil((M*N)/256)
      if (total <= 65535) return [total, 1, 1]
      const x = Math.min(65535, Math.ceil(Math.sqrt(total)))
      const y = Math.ceil(total / x)
      return [x, Math.min(65535, y), Math.max(1, Math.ceil(y / 65535))]
    }, 
    limits: { recommendMaxMN: 1024*1024 } 
  },
  { key: 'twoD8x8', label: '2D 8×8', wgsl: wgsl_2d_8x8, dispatch: ({M,N}) => [Math.ceil(N/8), Math.ceil(M/8), 1], limits: { recommendMaxMN: 4096*4096 } },
  { key: 'tile1x4', label: 'Tiled 1×4 (16×16 wg)', wgsl: wgsl_tile_1x4, dispatch: ({M,N}) => [Math.ceil(N/(16*4)), Math.ceil(M/16), 1], limits: { recommendMaxMN: 4096*4096 } },
  { key: 'tile4x4', label: 'Tiled 4×4 (16×16 wg)', wgsl: wgsl_tile_4x4, dispatch: ({M,N}) => [Math.ceil(N/(16*4)), Math.ceil(M/(16*4)), 1], limits: { recommendMaxMN: 4096*4096 } }
]
```

```{ojs}
// Non-reactive input controls (mutable)
mutable m_value = 512
```

```{ojs}
mutable k_value = 512
```

```{ojs}
mutable n_value = 512
```

```{ojs}
mutable kernel_value = 'naive1d'
```

```{ojs}
mutable verify_value = false
```

```{ojs}
// UI controls that update mutable values but don't trigger reactivity elsewhere
max_dim = 16384

m_input = {
  const input = Inputs.range([64, max_dim], {value: mutable m_value, step: 16, label: 'M (rows of A/C)'})
  input.addEventListener('input', () => mutable m_value = input.value)
  return input
}
```

```{ojs}
k_input = {
  const input = Inputs.range([64, max_dim], {value: mutable k_value, step: 16, label: 'K (cols of A / rows of B)'})
  input.addEventListener('input', () => mutable k_value = input.value)
  return input
}
```

```{ojs}
n_input = {
  const input = Inputs.range([64, max_dim], {value: mutable n_value, step: 16, label: 'N (cols of B/C)'})
  input.addEventListener('input', () => mutable n_value = input.value)
  return input
}
```

```{ojs}
kernel_input = {
  const input = Inputs.select(
    kernels,
    {
      label: 'Kernel',
      format: d => d.label,
      value: mutable kernel_value,
      reduce: d => d.key
    }
  )
  input.addEventListener('input', () => mutable kernel_value = input.value)
  return input
}
```

```{ojs}
verify_input = {
  const input = Inputs.toggle({label: 'Verify (CPU check if small)', value: mutable verify_value})
  input.addEventListener('input', () => mutable verify_value = input.value)
  return input
}
```

```{ojs}
// Group all inputs visually
html`<div>
  ${m_input}
  ${k_input}
  ${n_input}
  ${kernel_input}
  ${verify_input}
</div>`
```

```{ojs}
// Only the submit button triggers computation - increment on each click
viewof run = Inputs.button('Run matmul', { reduce: v => (v ?? 0) + 1 })
```

### Runner

```{ojs}
// Create typed arrays and buffers, dispatch, read back C if needed, and time it
async function runMatmul({M,K,N}, kernelKey, verify) {
  if (!device) return { error: 'No WebGPU device' }
  // Normalize kernel key: Inputs.select may yield a string or an option object
  const key = (typeof kernelKey === 'string')
    ? kernelKey
    : (kernelKey && typeof kernelKey === 'object' && ('value' in kernelKey || 'key' in kernelKey))
      ? (kernelKey.value ?? kernelKey.key)
      : (kernelKey ?? 'twoD8x8')
  const ker = kernels.find(k => k.key === key)
  if (!ker) return { error: 'Unknown kernel' }

  // Coerce and validate dims
  M = Math.max(1, Math.floor(+M || 0))
  K = Math.max(1, Math.floor(+K || 0))
  N = Math.max(1, Math.floor(+N || 0))

  const elemA = M*K, elemB = K*N, elemC = M*N
  const A = new Float32Array(elemA)
  const B = new Float32Array(elemB)
  // Fill with deterministic pseudo-randoms for repeatability
  for (let i=0;i<elemA;i++) A[i] = (Math.sin(i*17.13)+1)*0.5
  for (let i=0;i<elemB;i++) B[i] = (Math.cos(i*9.97)+1)*0.5

  const aBuf = createBuffer(device, A, GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST)
  const bBuf = createBuffer(device, B, GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST)
  const bytesC = elemC * 4
  const sizeBytesC = Math.ceil(bytesC / 4) * 4
  const cBuf = createBuffer(device, sizeBytesC, GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC | GPUBufferUsage.COPY_DST)
  const uBuf = makeDimsUniform(device, M, K, N)

  const pipeline = makePipeline(device, ker.wgsl)
  const bind = device.createBindGroup({ layout: pipeline.getBindGroupLayout(0), entries: [
    { binding: 0, resource: { buffer: uBuf } },
    { binding: 1, resource: { buffer: aBuf } },
    { binding: 2, resource: { buffer: bBuf } },
    { binding: 3, resource: { buffer: cBuf } },
  ]})

  const encoder = device.createCommandEncoder()
  const pass = encoder.beginComputePass()
  pass.setPipeline(pipeline)
  pass.setBindGroup(0, bind)
  const [dx, dy, dz] = ker.dispatch({M,N})
  pass.dispatchWorkgroups(dx, dy, dz)
  pass.end()

  const t0 = performance.now()
  device.queue.submit([encoder.finish()])
  await device.queue.onSubmittedWorkDone()
  const t1 = performance.now()
  const ms = t1 - t0

  // Optionally read back for correctness (or always, to avoid reuse of old data)
  const readBuf = createBuffer(device, sizeBytesC, GPUBufferUsage.COPY_DST | GPUBufferUsage.MAP_READ)
  const enc2 = device.createCommandEncoder()
  enc2.copyBufferToBuffer(cBuf, 0, readBuf, 0, sizeBytesC)
  device.queue.submit([enc2.finish()])
  await readBuf.mapAsync(GPUMapMode.READ)
  const C = new Float32Array(readBuf.getMappedRange().slice(0))
  readBuf.unmap()

  let check = null
  let cpuTime = null
  if (verify && M*K <= 1024*1024 && K*N <= 1024*1024) {
    // Measure CPU time for comparison
    const cpuStart = performance.now()
    const ref = cpuMatmul(A, B, M, K, N)
    const cpuEnd = performance.now()
    cpuTime = cpuEnd - cpuStart
    
    // Check accuracy
    let maxAbs = 0
    for (let i=0;i<elemC;i++) maxAbs = Math.max(maxAbs, Math.abs(ref[i]-C[i]))
    check = { maxAbs, cpuTime }
  }

  const flops = 2 * M * K * N // multiply-add per output element
  const gflops = flops / (ms/1000) / 1e9
  const cpuGflops = cpuTime ? flops / (cpuTime/1000) / 1e9 : null

  return { ms, gflops, M, K, N, kernel: ker.label, kernelKey: ker.key, dx, dy, dz, check, cpuTime, cpuGflops }
}
```

```{ojs}
// Wrap runMatmul to surface exceptions as error objects so UI can display them
async function runMatmulSafe(dims, kernel, verify) {
  try {
    return await runMatmul(dims, kernel, verify)
  } catch (e) {
    return { error: (e && e.message) ? e.message : String(e) }
  }
}
```

```{ojs}
mutable compute_status = ({
  state: 'idle',
  message: 'Idle — ready'
})
```

```{ojs}
mutable status_trigger = 0
```

```{ojs}
compute_status_value = {
  status_trigger; // Force dependency on trigger
  return mutable compute_status;
}
```

```{ojs}
// WebGPU Matrix Multiplication Runner - only executes on button click
results = {
  if (!run) {
    mutable compute_status = { state: 'idle', message: 'Idle — ready' }
    mutable status_trigger = mutable status_trigger + 1
    return null
  }

  // Capture current values at button click time
  const currentDims = {
    M: Math.max(1, Math.floor(Number(mutable m_value) || 0)),
    K: Math.max(1, Math.floor(Number(mutable k_value) || 0)),
    N: Math.max(1, Math.floor(Number(mutable n_value) || 0))
  }
  const currentKernel = mutable kernel_value
  const currentVerify = !!mutable verify_value

  const kernelInfo = kernels.find(k => k.key === currentKernel)
  const kernelLabel = kernelInfo?.label ?? String(currentKernel)
  
  if (!webgpu_ok) {
    const error = 'WebGPU not available'
    mutable compute_status = { state: 'error', message: error, error, kernelLabel, dims: currentDims }
    mutable status_trigger = mutable status_trigger + 1
    return { error }
  }
  
  if (!device) {
    const error = 'No WebGPU device'
    mutable compute_status = { state: 'error', message: error, error, kernelLabel, dims: currentDims }
    mutable status_trigger = mutable status_trigger + 1
    return { error }
  }

  mutable compute_status = {
    state: 'running',
    message: `Running ${kernelLabel} — ${currentDims.M}×${currentDims.K}×${currentDims.N}`,
    progress: null,
    kernelLabel,
    dims: currentDims
  }
  mutable status_trigger = mutable status_trigger + 1

  // Yield to the browser so the status panel can refresh before the GPU work finishes.
  await new Promise(requestAnimationFrame)
  
  try {
    const { M, K, N } = currentDims
    const totalMB = (M * K + K * N + M * N) * 4 / 1024 / 1024
    
    // Call the computation with captured values
    const result = await runMatmul(currentDims, currentKernel, currentVerify)
    
    if (!result) {
      const error = 'runMatmul returned null/undefined'
      mutable compute_status = { state: 'error', message: error, error, kernelLabel, dims: currentDims }
      mutable status_trigger = mutable status_trigger + 1
      return { error }
    }
    
    if (result.error) {
      const error = result.error
      mutable compute_status = { state: 'error', message: error, error, kernelLabel, dims: currentDims }
      mutable status_trigger = mutable status_trigger + 1
      return { error }
    }
    
    // Add metadata for display
    const enriched = { 
      ...result, 
      totalMB, 
      runNumber: run 
    }

    mutable compute_status = {
      state: 'done',
      message: `Completed ${kernelLabel} in ${enriched.ms.toFixed(2)} ms`,
      kernelLabel,
      dims: currentDims,
      gpuTimeMs: enriched.ms,
      gpuGflops: enriched.gflops,
      progress: 1
    }
    mutable status_trigger = mutable status_trigger + 1

    return enriched
  } catch (e) {
    const error = e.message || e.toString()
    mutable compute_status = { state: 'error', message: error, error, kernelLabel, dims: currentDims }
    mutable status_trigger = mutable status_trigger + 1
    return { error }
  }
}
```

```{ojs}
status_panel = {
  const status = compute_status_value || { state: 'idle', message: 'Idle — ready' }
  const container = html`<div class="callout status-panel"></div>`
  container.classList.add(status.state === 'error' ? 'callout-warning' : 'callout-note')

  const progressEl = document.createElement('progress')
  progressEl.max = 1
  progressEl.style.width = '100%'
  progressEl.style.height = '1rem'

  if (status.state === 'running') {
    if (Number.isFinite(status.progress)) {
      progressEl.value = Math.min(1, Math.max(0, status.progress))
    } else {
      progressEl.removeAttribute('value')
    }
  } else if (status.state === 'done') {
    progressEl.value = 1
  } else {
    progressEl.value = 0
  }

  container.append(progressEl)

  const heading = document.createElement('div')
  heading.style.fontWeight = '600'
  heading.textContent = status.message ?? 'Idle — ready'
  container.append(heading)

  if (status.state === 'done' && Number.isFinite(status.gpuGflops)) {
    const detail = document.createElement('div')
    detail.textContent = `${status.kernelLabel ?? ''} — ${status.gpuGflops.toFixed(2)} GFLOP/s`
    container.append(detail)
  } else if (status.state === 'error' && status.error) {
    const detail = document.createElement('div')
    detail.textContent = status.error
    container.append(detail)
  } else if (status.dims) {
    const dims = status.dims
    const detail = document.createElement('div')
    detail.textContent = `Dims: ${dims.M}×${dims.K}×${dims.N}`
    container.append(detail)
  }

  return container
}
```

```{ojs}
html`${status_panel}`
```

```{ojs}
// Display results - this cell only updates when results change
{
  if (!run) {
    return html`<em>Click Run matmul to execute the selected kernel</em>`
  }
  
  // Handle async results
  if (results instanceof Promise) {
    return html`<div>Computing... Please wait...</div>`
  }
  
  if (results.error) {
    return html`<div class="callout callout-warning"><strong>Error:</strong> ${results.error}</div>`
  }
  
  const tableRow = {
    runNumber: results.runNumber,
    kernel: results.kernel,
    shape: { M: results.M, K: results.K, N: results.N },
    memoryMB: results.totalMB,
    dispatch: [results.dx, results.dy, results.dz],
    gpuTimeMs: results.ms,
    gpuGflops: results.gflops,
    cpuTimeMs: results.cpuTime,
    cpuGflops: results.cpuGflops,
    speedup: (Number.isFinite(results.cpuTime) && Number.isFinite(results.ms) && results.ms > 0)
      ? results.cpuTime / results.ms
      : null,
    maxAbs: results.check?.maxAbs ?? null
  }

  const dash = '—'
  const formattedRow = {
    'Run': `#${tableRow.runNumber}`,
    'Kernel': tableRow.kernel,
    'Shape': `(${tableRow.shape.M}×${tableRow.shape.K}) · (${tableRow.shape.K}×${tableRow.shape.N}) → (${tableRow.shape.M}×${tableRow.shape.N})`,
    'Memory': Number.isFinite(tableRow.memoryMB) ? `${tableRow.memoryMB.toFixed(1)} MB` : dash,
    'Dispatch': `[${tableRow.dispatch[0]}, ${tableRow.dispatch[1]}, ${tableRow.dispatch[2]}]`,
    'GPU Time': Number.isFinite(tableRow.gpuTimeMs) ? `${tableRow.gpuTimeMs.toFixed(2)} ms` : dash,
    'GPU Throughput': Number.isFinite(tableRow.gpuGflops) ? `${tableRow.gpuGflops.toFixed(2)} GFLOP/s` : dash,
    'CPU Time': Number.isFinite(tableRow.cpuTimeMs) ? `${tableRow.cpuTimeMs.toFixed(2)} ms` : dash,
    'CPU Throughput': Number.isFinite(tableRow.cpuGflops) ? `${tableRow.cpuGflops.toFixed(2)} GFLOP/s` : dash,
    'GPU Speedup': Number.isFinite(tableRow.speedup) ? `${tableRow.speedup.toFixed(1)}×` : dash,
    'Max |Δ| vs CPU': Number.isFinite(tableRow.maxAbs) ? tableRow.maxAbs.toExponential(2) : dash
  }

  return Inputs.table([formattedRow])
}
```

## Notes and tips

- This page uses Observable JS cells; code runs reactively in the browser.
- Performance varies a lot across browsers/GPUs. Numbers shown include submission/synchronization overhead; GPU timestamp queries can refine timing when supported.
- The kernels here don’t use workgroup-shared memory or subgroups; those can yield further gains on some hardware.
- Credit: Kernel ideas and the optimization path are inspired by Zach Nussbaum’s article referenced above and by Simon Böhm’s CUDA matmul write-up.
